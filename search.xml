<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>算法-树</title>
    <url>/2021/03/04/%E7%AE%97%E6%B3%95-%E6%A0%91/</url>
    <content><![CDATA[<h1>Tree</h1>

<h3>剑指offer 26. 树的子结构</h3>

<blockquote>
<p>输入两颗二叉树A和B，判断B是不是A的子结构。（约定空树不是任意一个树的子结构）</p>
<p>B是A的子结构，即A中有出现和B相同的结构和节点值。</p>
</blockquote>
<p>考虑用递归和先序遍历：</p>
<p>若树B是树A的子结构，则子结构的跟节点可能为树A的任意一个节点。因此，判断树B是否是树A的子结构，需完成以下两步工作：</p>
<ol>
<li>先序遍历树A中的每个节点(这个由大函数isSubStructure完成)</li>
<li>判断树A中以该节点为根节点的子树是否包含树B(这个由函数recur完成)</li>
</ol>
<p>recur(A, B) 函数：</p>
<p>终止条件：</p>
<p>当节点 B 为空：说明树 B 已匹配完成（越过叶子节点），因此返回 ttrue ；</p>
<p>当节点 A 为空：说明已经越过树 A 叶子节点，即匹配失败，返回 false ；</p>
<p>当节点 A 和 B 的值不同：说明匹配失败，返回 false ；</p>
<p>返回值：</p>
<p>判断 A 和 B 的左子节点是否相等，即 recur(A.left, B.left) ；</p>
<p>判断 A 和 B 的右子节点是否相等，即 recur(A.right, B.right) ；</p>
<p>isSubStructure(A, B) 函数：</p>
<p>特例处理： 当 树 A 为空 或 树 B 为空 时，直接返回 false ；</p>
<p>返回值： 若树 B 是树 A 的子结构，则必满足以下三种情况之一，因此用或 || 连接；</p>
<p>以 节点 A 为根节点的子树 包含树 B ，对应 recur(A, B)；</p>
<p>树 B 是 树 A 左子树 的子结构，对应 isSubStructure(A.left, B)；</p>
<p>树 B 是 树 A 右子树 的子结构，对应 isSubStructure(A.right, B)；</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isSubStructure</span><span class="params">(self, A: TreeNode, B: TreeNode)</span> -&gt; bool:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">recur</span><span class="params">(A, B)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> B: <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> A <span class="keyword">or</span> A.val != B.val: <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            <span class="keyword">return</span> recur(A.left, B.left) <span class="keyword">and</span> recur(A.right, B.right)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> bool(A <span class="keyword">and</span> B) <span class="keyword">and</span> (recur(A, B) <span class="keyword">or</span> self.isSubStructure(A.left, B) <span class="keyword">or</span> self.isSubStructure(A.right, B))</span><br></pre></td></tr></table></figure>
<h3>剑指offer 32-1. 从上到下打印二叉树</h3>

<blockquote>
<p>从上到下打印出二叉树的每个节点，同一层的节点按照从左到右的顺序打印。</p>
</blockquote>
<p>这道题就是层次遍历BFS二叉树。BFS通常借助队列的先入先出特性来实现。</p>
<p>一个精品回答采用了python的一个数据结构库<code>collections</code>中的<code>deque()</code>方法，一个双端队列。代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">levelOrder</span><span class="params">(self, root: TreeNode)</span> -&gt; List[int]:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> root: <span class="keyword">return</span> []</span><br><span class="line">    res, queue = [], collections.deque()</span><br><span class="line">    queue.append(root)</span><br><span class="line">    <span class="keyword">while</span> queue:</span><br><span class="line">      node = queue.popleft()</span><br><span class="line">      res.append(node.val)</span><br><span class="line">      <span class="keyword">if</span> node.left: queue.append(node.left)</span><br><span class="line">      <span class="keyword">if</span> node.right: queue.append(node.right)</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>也有不使用这个库的方式：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">levelOrder</span><span class="params">(self, root: TreeNode)</span> -&gt; List[int]:</span></span><br><span class="line">    temp = [root]</span><br><span class="line">    ans = []</span><br><span class="line">    <span class="keyword">while</span> len(temp) &gt; <span class="number">0</span>:</span><br><span class="line">      <span class="keyword">if</span> temp[<span class="number">0</span>]:</span><br><span class="line">        ans.append(temp[<span class="number">0</span>].val)</span><br><span class="line">        temp.append(temp[<span class="number">0</span>].left)</span><br><span class="line">        temp.append(temp[<span class="number">0</span>].right)</span><br><span class="line">      temp.pop(o)</span><br><span class="line">    <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
<h3>剑指offer 33. 二叉搜索树的后序遍历序列</h3>

<blockquote>
<p>输入一个整数数组，判断该数组是不是某二叉搜索树的后序遍历结果。如果是则返回 true，否则返回 false。假设输入的数组的任意两个数字都互不相同。</p>
</blockquote>
<p><em>后序遍历定义：[左子树｜右子树｜根节点]，即遍历顺序为“左、右、根”</em></p>
<p><em>二叉搜索树定义：左子树中所有节点的值&lt;根节点的值；右子树中所有节点的值&gt;根节点的值；其左右子树也分别是二叉搜索树。</em></p>
<p><b>常见的方式是递归分治：</b>根据二叉搜索树的定义，可以通过递归，判断所有子树的正确性（即其后序遍历是否满足二叉搜索树的定义），若所有子树都正确，则此序列为二叉搜索树的后序遍历。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">verifyPostorder</span><span class="params">(self, posterder: [int])</span> -&gt; bool:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">recur</span><span class="params">(i, j)</span>:</span></span><br><span class="line">      <span class="keyword">if</span> i &gt;= j: <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">      p = i</span><br><span class="line">      <span class="keyword">while</span> posterder[p] &lt; posterder[j]: p += <span class="number">1</span></span><br><span class="line">      m = p</span><br><span class="line">      <span class="keyword">while</span> posterder[p] &gt; posterder[j]: p += <span class="number">1</span></span><br><span class="line">      <span class="keyword">return</span> p == j <span class="keyword">and</span> recur(i, m - <span class="number">1</span>) <span class="keyword">and</span> recur(m, j <span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">return</span> recur(<span class="number">0</span>, len(posterder) - <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>剑指offer</tag>
        <tag>Tree</tag>
      </tags>
  </entry>
  <entry>
    <title>算法-链表</title>
    <url>/2021/03/01/%E7%AE%97%E6%B3%95-%E9%93%BE%E8%A1%A8/</url>
    <content><![CDATA[<h1>Linked List</h1>

<h3>剑指offer 06. 从头到尾打印链表</h3>

<blockquote>
<p>输入一个链表的头节点，从尾到头反过来返回每个节点的值（用数组返回）。</p>
<p>示例：</p>
<p>​        输入：head = [1,3,2]</p>
<p>​        输出：[2,3,1]</p>
</blockquote>
<p>这道题用python可以很简单的做出来：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">reversePrint</span><span class="params">(self, head: ListNode)</span> -&gt; List[int]:</span></span><br><span class="line">    auxiliaryArray = []</span><br><span class="line">    <span class="keyword">while</span> head:</span><br><span class="line">      auxiliaryArray.append(head.val)</span><br><span class="line">      head = head.next</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> auxiliaryArray[::<span class="number">-1</span>]</span><br></pre></td></tr></table></figure>
<p>java也可以考虑使用栈的方式：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">int</span>[] reversePrint(ListNode head) &#123;</span><br><span class="line">    Stack&lt;ListNode&gt; stack = <span class="keyword">new</span> Stack&lt;ListNode&gt;();</span><br><span class="line">    ListNode temp = head;</span><br><span class="line">    <span class="keyword">while</span> (temp != <span class="keyword">null</span>) &#123;</span><br><span class="line">      stack.push(temp);</span><br><span class="line">      temp = temp.next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">int</span> size = stack.size();</span><br><span class="line">    <span class="keyword">int</span>[] print = <span class="keyword">new</span> <span class="keyword">int</span>[size];</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; size; i++) &#123;</span><br><span class="line">      print[i] = stack.pop().val;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> print;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>也可以利用递归的思想：</p>
<p>先走至链表末端，回溯时依次将节点值加入列表，这样就可以实现链表值的倒序输出。</p>
<ol>
<li>递推阶段。每次传入head.next，以head=null（即走过链表尾部节点）为递归终止条件，此时直接返回。</li>
<li>回溯阶段。利用python语言特性，递归回溯时每次返回当前list+当前节点值[head.val]，即可实现节点的倒序输出。</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">reversePrint</span><span class="params">(self, head: ListNode)</span> -&gt; List[int]:</span></span><br><span class="line">    <span class="keyword">return</span> reverseprint(head.next) + head.val <span class="keyword">if</span> head <span class="keyword">else</span> []</span><br></pre></td></tr></table></figure>
<h3>剑指offer 22. 链表中倒数第k个结点</h3>

<blockquote>
<p>输入一个链表，输出该链表中倒数第k个结点。为了符合大多数人的习惯，本题从1开始计数，即链表的尾节点是倒数第1个节点。</p>
<p>例如，一个链表有6个节点，从头节点开始，它们的值依次是1、2、3、4、5、6。这个链表的倒数第3个节点是值为4的节点。</p>
</blockquote>
<p>注意⚠️：这道题要返回的是那个节点，而不只是那个节点的val</p>
<p>双指针</p>
<p>流程：</p>
<ol>
<li>初始化：前指针former和后指针latter在初始化时都指向头节点head</li>
<li>构建双指针距离：前指针former先向前走k步（结束后，双指针former和latter间相距k步）</li>
<li>双指针共同移动：循环中，双指针former和latter每轮都向前走一步，直至former走过链表尾节点时跳出（跳出后，latter与尾节点距离为k-1，即latter指向倒数第k个节点）</li>
<li>返回值：返回latter即可</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getKthFromEnd</span><span class="params">(self, head: ListNode, k: int)</span> -&gt; ListNode:</span></span><br><span class="line">    former, latter = head, head</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(k):</span><br><span class="line">      former = former.next</span><br><span class="line">    <span class="keyword">while</span> former:</span><br><span class="line">      former, latter = former.next, latter.next</span><br><span class="line">    <span class="keyword">return</span> latter</span><br></pre></td></tr></table></figure>
<h3>剑指offer 24. 反转链表</h3>

<blockquote>
<p>定义一个函数，输入一个链表的头节点，反转该链表并输出反转后链表的头节点。</p>
</blockquote>
<p>也用双指针/迭代的方式可以完成：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">reverseList</span><span class="params">(self, head: ListNode)</span> -&gt; ListNode:</span></span><br><span class="line">    currentNode, preNode = head, <span class="literal">None</span></span><br><span class="line">    <span class="keyword">while</span> currentNode:</span><br><span class="line">      temp = currentNode.next</span><br><span class="line">      currentNode.next = preNode</span><br><span class="line">      preNode = currentNode</span><br><span class="line">      currentNode = temp</span><br><span class="line">    <span class="keyword">return</span> preNode</span><br></pre></td></tr></table></figure>
<h3>剑指offer 25. 合并两个排序的链表</h3>

<blockquote>
<p>输入两个递增排序的链表，合并这两个链表并使新链表中的节点仍然是递增排序的。</p>
</blockquote>
<p>因为两个链表都是递增的，所以考虑采用双指针来解决。起初保证两个指针相等，然后一直移动一个指针来添加新链表元素，最后返回另外一个指针。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">mergeTwoLists</span><span class="params">(self, l1: ListNode, l2: ListNode)</span> -&gt; ListNode:</span></span><br><span class="line">    cur = dum = ListNode(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">while</span> l1 <span class="keyword">and</span> l2:</span><br><span class="line">      <span class="keyword">if</span> l1.val &lt; l2.val:</span><br><span class="line">        cur.next, l1 = l1, l1.next</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        cur.next, l2 = l2, l2.next</span><br><span class="line">      cur = cur.next //让指针往前移</span><br><span class="line">    cur.next = l1 <span class="keyword">if</span> l1 <span class="keyword">else</span> l2 //当l1 l2其中一个链表为空时的赋值</span><br><span class="line">    <span class="keyword">return</span> preNode</span><br></pre></td></tr></table></figure>
<h3>剑指offer 18. 删除链表的节点</h3>

<blockquote>
<p>给定单向链表的头指针和一个要删除的节点的值，定义一个函数删除该节点。返回删除后的链表的头节点。</p>
</blockquote>
<p>自己的版本：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">deleteNode</span><span class="params">(self, head: ListNode, val: int)</span> -&gt; ListNode:</span></span><br><span class="line">   cur, dum = head, ListNode(<span class="number">0</span>)</span><br><span class="line">   <span class="keyword">if</span> head.val == val:</span><br><span class="line">     head = head.next</span><br><span class="line">     <span class="keyword">return</span> head</span><br><span class="line">   <span class="keyword">while</span> cur:</span><br><span class="line">     <span class="keyword">if</span> cur.val == val:</span><br><span class="line">       dum.next = cur.next</span><br><span class="line">       cur = dum</span><br><span class="line">     dum = cur</span><br><span class="line">     cur = cur.next</span><br><span class="line">   <span class="keyword">return</span> head</span><br></pre></td></tr></table></figure>
<p>官方的版本：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">deleteNode</span><span class="params">(self, head: ListNode, val: int)</span> -&gt; ListNode:</span></span><br><span class="line">    <span class="keyword">if</span> head.val == val: <span class="keyword">return</span> head.next</span><br><span class="line">    pre, cur = head, head.next</span><br><span class="line">    <span class="keyword">while</span> cur <span class="keyword">and</span> cur.val != val:</span><br><span class="line">      pre, cur = cur, cur.next</span><br><span class="line">    <span class="keyword">if</span> cur: pre.next = cur.next</span><br><span class="line">    <span class="keyword">return</span> head</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>剑指offer</category>
      </categories>
      <tags>
        <tag>剑指offer</tag>
        <tag>Linked List</tag>
      </tags>
  </entry>
  <entry>
    <title>SCCAIN</title>
    <url>/2021/02/21/SCCAIN/</url>
    <content><![CDATA[<h1>SCCAIN</h1>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Semi-supervised Co-Clustering on Attributed Heterogeneous Information Networks[J]. Information Processing &amp; Management, 57( 6).</span><br><span class="line">《属性异质信息网络上的半监督双聚类》</span><br></pre></td></tr></table></figure>
<font color='blue'>早先的方法独立地针对指定类型的节点进行结构相似性度量而忽视了不同类型节点之间的关联关系。</font>

<p>本文研究<font color='red'>同时聚合不同类型节点的问题，其目的是挖掘异质节点之间的潜在关联，并同时针对不同类型的节点进行聚类划分。</font></p>
<p>有两个主要挑战：</p>
<ol>
<li>节点之间的相似性/相关性<span style="border-bottom:2px dashed red;">不仅和结构信息相关，同时也和离散/连续的节点属性相关。</span></li>
<li>聚类和相似性度量往往是相互促进的。</li>
</ol>
<p>概述：</p>
<p>本文首先利用多条元路径和节点属性，设计了一种融合结构和属性的可学习的整体相关性度量方法。继而，本文提出了属性异质网络半监督双聚类方法SCCAIN，基于约束的正交非负矩阵三分解对不同类型的节点同时进行聚类。最后，设计了一种端到端的优化框架，可以联合优化相关性度量和双聚类。</p>
<h3>Introduction</h3>

<p>异质信息网络HIN包含了多种类型的节点和边关系，融合了更多的结构信息和语义特征。<span style="border-bottom:2px dashed red;">在HIN的聚类中，如何融合多关系下的节点之间的相似性度量是关键因素。</span>最近的一些方法提出融合多条元路径，自动学习不同元路径的权重。这些方法只利用了HIN中的结构信息，而忽视了节点上的属性特征。</p>
<p>举个栗子：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2021/02/21/IL7KUwaibYnzfVc.jpg" alt="example.png"></p>
<p>在上述图片中，当只考虑结构信息时，基于A-P-A和A-P-C-P-A元路径，作者可以聚合成两类，即{$A_1,A_2,A_3$}和{$A_4,A_5$}。而当额外考虑属性信息时，我们可以更精细地将$A_2$和{$A_1,A_3$}分离开。</p>
<p><span style="border-bottom:2px dashed yellow;">为了将属性信息进行整合，一种朴素的方案是将所有的属性视作节点，其维度作为节点类型，整合到原始的异质信息网络中。</span>然而，这种方法只能处理离散特征，如类别、城市、机构等，而难以刻画连续特征如年龄、合作作者数量等。<span style="border-bottom:2px dashed blue;">一种更加合理的设计是将所有的属性信息构建成属性向量，其中每个维度表示一种属性特征。</span>最近的工作，如SCHAIN和SCAN，主要针对相同类型的节点同时进行聚类。而实际上，不同类型的类簇之间往往具有潜在的关联。例如，会议的聚类结果往往可以促进作者的聚类，反之亦然。</p>
<p><span style="border-bottom:2px dashed yellow;">为了同时聚合不同类型的节点以及挖掘不同类型类簇之间的潜在关联，双聚类/协同聚类(co-clustering)是一个较好的选择。</span>与传统聚类方法不同，<font color='red'>双聚类利用特征与样本的对偶性，实现特征与样本的同时聚类。此外，双聚类方法能够在不同节点类型的簇之间推导出潜在的对应关系，从而使得到的簇更具可解释性。</font></p>
<blockquote>
<p>对偶性（Duality）：</p>
<p>​        对偶性即导致相同的物理结果，而表面上不同的理论之间的对应。</p>
<p>傅立叶变换中的应用：</p>
<p>​        一个函数x(t)和它的傅立叶变换X(jw)之间的关系可以用下面的两个公式表示：</p>
<p>$x(t)= \frac{1}{2 \pi } \int_{-\infty}^{\infty} X(jw)e^{jwt} dw$</p>
<p>和</p>
<p>$X(jw)= \int_{-\infty}^{\infty} x(t) e^{-jwt} dt$ </p>
</blockquote>
<p>本文的主要挑战点：</p>
<ul>
<li><b>在异质信息网络中如何同时考虑属性和结构信息进行相关性度量</b>：不同类型的节点之间存在多条非对称的元路径；不同类型的节点属性特性不同，不能直接用欧式距离、余弦相似度等进行度量。</li>
<li><b>如何利用约束信息同时优化相关性度量和双聚类</b>：在异质信息网络中，存在一些约束条件，如must-link和connot-link的节点对，这些约束条件同时促进了相关性度量和双聚类。如何设计有效的半监督框架，将相关性度量和双聚类进行融合，是另一个挑战。</li>
</ul>
<h3>方法</h3>

<p><img src= "img/loading.gif" data-src="https://i.loli.net/2021/02/21/76QPH1FwGoWd5tq.jpg" alt="framework.png"></p>
<p><span style="border-bottom:2px dashed blue;">SCCAIN中，首先分别提出了异质节点的结构和属性相关性度量，继而设计一种统一的相关性度量方法。其次，设计了一种半监督的正交矩阵三分解框架，用于分解不同类型的节点聚类和聚簇相关性挖掘。最后，将相关性度量和双聚类融合到一个统一的框架中，迭代优化相关性度量和双聚类。</span></p>
<p><b>结构相关性度量</b></p>
<p>考虑到节点类型不同，节点之间的结构信息可以通过非对称元路径构成，本文基于HeteSim来度量单一元路径上的相关性，即：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2021/02/21/48wkGj1gvtRZE9P.jpg" alt="jgxgx.png"></p>
<p>其中，$V_{s,i}$和$V_{t,j}$分别表示s类型的节点i和t类型的节点j，$R_1 \omicron … \omicron R_l$表示构成元路径的关系类型序列。$O$和$I$表示出度和入度。考虑到存在多条元路径，我们分别针对每条元路径进行相关性度量，并通过元路径权重参数融合多条元路径，即：</p>
<blockquote>
<p>有关HeteSim：</p>
<p>与homogeneous同构信息网络相区别，异构信息网络的网络模式限定了对象集合以及对象间关系的类型约束。这些约束使得异构信息网络具有半结构化的特点，引导着网络语义的探究。</p>
</blockquote>
<h3>Code</h3>

<p><a href="https://github.com/yuduo93/SCCAIN" target="_blank" rel="noopener">https://github.com/yuduo93/SCCAIN</a></p>
]]></content>
      <categories>
        <category>HIN</category>
        <category>recommendation</category>
        <category>NMTF</category>
      </categories>
      <tags>
        <tag>Heterogeneous Information Networks</tag>
        <tag>NMTF</tag>
      </tags>
  </entry>
  <entry>
    <title>MacridVAE</title>
    <url>/2021/01/19/MacridVAE/</url>
    <content><![CDATA[<h1>Introduction</h1>

<p>阿里19年在NeurIPS上发表的文章<b>《Learning Disentangled Representations for Recommendation》</b></p>
<p>这篇文章从<font color='red'>认知</font>的角度来看待推荐系统。</p>
<p>在做人和商品的过程中，相比于黑盒模型，我们更想关注匹配过程中人的认知因素。人为什么喜欢一件商品，他是对哪些概念动了心，种了草，他是因为什么原因点了击，收了藏，下了单，他当下关注点在哪个认知层面的东西，我们的推荐系统能显式的知道、消化并且准确相应吗？这些所谓的认知因素，并不是商品固有的细粒度的属性、品类，而是一种从人的角度理解商品的可传播可解释的概念。<font color='red'>其更像是广告商会选择去打动人心的记忆点</font>。而推荐系统区别于搜索场景的一个点恰好在于是否能主动激活用户潜在的兴趣，帮助用户找到并接受意料之外的商品。因此，如何挖掘潜在的认知概念，并以合理的方式将潜在可接受的认知概念传递给用户可能是未来一段时间推荐系统需要有所突破的事情。</p>
<p>当然，关注这样的认知过程并不是为了端到端的做“下一个商品”的预测或者点击率预估亦或者是评分预估。至少前人在大规模数据的线上经验能表明，产品形态不变的可解释推荐，相比于黑盒模型并不能真正提高最终的点击和转化效果。因此相比于可解释推荐，认知推荐更强调人的因素，其归宿必然是技术驱动产品形态上的创新。而新的产品形态则可以创造新的需求、用户习惯和新的商业场景。</p>
<h1>文章背景</h1>

<p>文章关注两个与认知相关的子任务：</p>
<ul>
<li>商品在人的认知空间中，它们是如何表征的，这样的表征是否具有可解释性，例如是否能找到对应的某一维就能够代表一个独立的“语义”。<font color='red'>这里的语义，其所具有的可解释性其实本质是一个与认知和传播相关的概念，既是能被人们理解和传播的。</font>类似的，人在这个空间下的表征，是否也具有这样的语义。联系解离化表征(Disentangled Representation Learning)在连续型数据上的发展，作者探索其是否能从离散数据，特别是用户行为数据上学习到类似的结果。</li>
<li>基于这样的表征，我们能否提出新型的推荐应用，并至少给出一种原型方案。</li>
</ul>
<h1>什么是解离化表征</h1>

<p><a href="https://image.jiqizhixin.com/uploads/editor/e9616e74-3643-436c-bb96-9f839c420d59/640.png" target="_blank" rel="noopener">https://image.jiqizhixin.com/uploads/editor/e9616e74-3643-436c-bb96-9f839c420d59/640.png</a></p>
<p>目前来说，其实解离化向量并没有一个标准的定义。如上图所示，假设一副图片的向量表示中，不同的维度含义不同并且这个含义还能被人类所能识别，我们就称这样的向量是一个解离化的向量表示。例如第一维是方位，第二维是大小，第三维是椅子腿的风格。得到这样的向量有几个好处：1）鲁棒性，可迁移性；2）解释性；3）可控制的生成。前两点其实和因果推断的优势类似，第三点主要应用方面有较大的潜力。</p>
<h1>Method</h1>

<p><b>这个工作，仅使用到用户行为数据，并不涉及任何商品属性以及用户特征。</b>作者探索从用户行为当中，能否得到一些认知相关的决策因素并以可解离的方式对商品和用户进行表示。</p>
<p>N users   M items</p>
<p>设平台上有M个商品，记用户u与商品i的交互记录为$x_{u,i}\in {0,1}$，取值为1表示用户点击了该商品，取值为0表示没有相关记录。那么用户u的行为可以记作$x_u = \{x_{u,i}:x_{u,i}=1\}$。我们的目标是获得用户u的向量化表征，同时我们的模型也会产生商品的表征$\{z_u\}_{u=1}^N$，同时我们的模型也会产出商品的表征$\{h_i\}_{i=1}^M \in \mathbb{R}^{M \times d}$，以供我们的推荐系统根据用户的表征召回一批商品。</p>
<p>针对电商平台上用户行为的特点，我们的模型采用了层次化的设计：它在推理一个用户的表征时将依次进行宏观解离化(macro disentanglement)和微观解离化(micro disentanglement)。</p>
<p><b>宏观解离化：</b>宏观解离化的启发一方面来自于用户在综合类电商场景下的兴趣确实较为分散，另一方面也来自于人类的传统认知难题Binding Problem。首先，用户兴趣通常是非常广泛的，一个用户的点击记录往往会涉及到<font color='red'>多个独立的消费意图</font>（比如点击不同大类下的商品）。而用户在执行不同意图时的偏好往往也是独立的，比如喜欢深色的衣服并不意味着用户也喜欢深色的电器。哪怕是价格偏好也常存在不可迁移的情况，比如买高档口红和买便宜好用的笔记本电脑这两者并不互斥。另外，宏观解离化也是微观解离化的必要前提。</p>
<p>因此，我们将<font color='red'>用户的表征拆分成K个d维分量</font>$z_u = [z_u^{(1)};z_u^{(2)};…,z_u^{(K)}]$，<font color='red'>用来表示用户执行K种不同的意图时的偏好（比如这K个分量可以对应K个商品大类）。</font>同时，<font color='red'>每个商品都有对应的one-hot向量</font>$c_i = [c_{i,1};c_{i,2};…;c_{i,K}]$，其中，$c_{i,k}=1$<font color='red'>表示商品i通常与第k种宏观的消费意图相关（或属于第k个商品大类）。</font>另一方面，给定一个数据整体（一副图像、一组用户行为），binding problem在讲人是如何将整体分割成部分（图像中不同的物品，行为中不同的兴趣），并且从部分组合出新的数据。这方面的研究本身有难度，也比较有意思。</p>
<p><b>微观解离化：</b>我们希望能把用户在执行某个意图时的偏好进一步地分解到更细的粒度。比如，设第K个意图对应服饰，我们希望用户在这个意图下的偏好向量$z_u^{(K)}$的各个维度能够对应不同的商品属性，比如某一维和颜色相关，另一维和尺寸相关，等等。这里我们看到，宏观解离是微观解离的前提：不同大类的商品属性集合是很不同的，用户表征向量的某一个维度如果已经被用于刻画用户对手机电量的偏好了，那么这一维对服饰等商品就是没有任何意义的———在预测用户是否会点击某个服饰时、在通过用户行为学习某个服饰的表征时，我们都应当忽略这些只和手机相关的维度。</p>
<h2>框架结构</h2>

<p>框架图：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2021/01/20/H2nFzfq1Ko8GlDC.jpg" alt="MacridVAE_framework.png"></p>
<p>模型是一个深度生成模型，它假设数据的生成过程是这样的：</p>
<script type="math/tex; mode=display">p_\theta (x_u) = E_{p\theta (C)} [\int p_{\theta} (x_u | z_u, C)p_{\theta} (z_u)dz_u]$$,

$$p_\theta (x_u | z_u, C) = \prod_{x_{u,i} \in x_u} p_{\theta} (x_{u,i} | z_u, C)</script><p>这里的$x_u = \{x_{u,i}:x_{u,i}=1\}$是用户u的点击历史，$z_u = [z_u^{(1)};z_u^{(2)};…,z_u^{(K)}]$是用户u的表征，$C = \{ c \}_{i=1}^M$指示了这些商品通常都对应哪些宏观的消费意图。$\theta$是模型的参数，它包括模型涉及到的深度神经网络的参数，也包括了商品的表征$\{ h_i \}_{i=1}^M \in \mathbb{R}^{M \times d}$等等。$p_\theta (x_u | z_u, C) = Z_u^{-1} · \sum_{k=1}^K c_{i,k} · g_{\theta}^{(i)}(z_u^{(k)})$是一个离散概率分布，用于对用户u接下来会点击哪个商品进行建模。这里为了实现宏观解离化，在预测用户对商品i的态度时，我们限制了商品i只能和用户表征K个分量的其中一个进行比对。这里的$Z_u = \sum_{i=1}^M \sum_{k=1}^K c_{i,k} · g_{\theta}^{(i)}(z_u^{(k)})$起到归一化的作用，而$g_{\theta}^{(i)}(z_u^{(k)})$是一个简单的网络，这里使用的是$g_{\theta}^{(i)}(z_u^{(k)}) = exp(Consine(z_u^{(k)},h_i)/\tau)$。计算$z_u$需要枚举全部M个商品，非常耗时，在尝试了各种近似方法（包括negative sampling、noise contrastive estimation等），最终采用的是sampled softmax，因其表现相对比较稳定。</p>
<p>为了优化这个深度概率模型，采用了VAE框架。为此，引入了一个编码器$q_{\theta}(z_u | x_u, C)$，具体实现见伪代码，类似一个K通道的单层图卷积神经网络。整个模型的优化目标是：</p>
<script type="math/tex; mode=display">\mathbb{E}_{p_{\theta}(C)} [\mathbb{E}_{p_{\theta}(z_u|x_u,C)}[lnp_{\theta}(x_u|z_u,C)]-\beta · D_{KL}(p_{\theta}(z_u|x_u,C) || p_{\theta}(z_u))]</script><font color = 'red'>这里我们为了实现微观解离化，借鉴了beta-VAE的方法，对KL惩罚项进行了加强（令beta远大于1）。这种做法将迫使表征的各个维度去捕捉比较独立的信息，比如各种各样的不太相关的商品属性（颜色和大小就不太相关）------当然，某些商品属性之间可能存在较强的相关，这就是有待未来工作去解决的。</font>

<p>商品对应的宏观意图$C = \{ c \}_{i=1}^M$若是已知的，可以直接使用。若是未知的，我们的实验里提供了一种基于原型向量的平摊化推理方法（prototype-based amortized inference）来估计各个商品对应的宏观意图，并使用了Gumbel-Softmax技巧来估计离散变量的梯度。值得一提的是，我们的实验里广泛使用了余弦相似度，而不是更常用的点击相似度，这是为了避免发生模态崩塌（model collapse）（全部原型退化成同一个点，或是全部商品都被分配到同一个原型）。</p>
<p>伪代码：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2021/01/20/XQtahmAYirwWB4C.jpg" alt="weidaima.png"></p>
<h2>用户可控制的交互式推荐</h2>

<p>解离化表征在带来一定的可解释的同时，也带来了一定的可控制性。这种可控制性有望给推荐系统引入一种全新的用户体验。比如说，既然表征的各个维度关联的是不同的商品属性，那么我们完全可以把用户的表征向量提供给用户，允许用户自行固定绝大部分维度（比如对应的是衣服的风格、价格、尺寸等），然后单独调整某个维度的取值（比如颜色对应的维度），系统再根据这个反馈调整推荐结果。这将帮助用户更加精准地表达自己想要的、并检索得到自己想要的。</p>
<p>作者从某个商品或用户的表征出发，在固定住其他维度后，逐步地改变表征第j维的取值。然后利用论文中提供的一种类似beam search的小技巧，检索出了表征第j维大不一样、但其它维度都很一致的一批商品。</p>
<p><font color='red'>当然，并不是所有的维度都有人类可以理解的语义。而且，正如文献《Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representation. Locatello et al, ICML 2019》所指出的，在无监督的情况下，训练出可解释的模型仍然是需要运气的——-在加了beta-VAE的约束后，获得可解释模型的概率相比普通VAE大大提高，但仍然避免不了“反复训练多个模型，然后挑出最好的模型”这一陷阱。</font>因此，未来可以多关注（弱/半）监督方法，引入标签信息。</p>
<h1>离线数据上的定量实验</h1>

<h2>定量评测解离化程度</h2>

<p>作者在某个小规模数据集上定量测量了解离化程度（及其与推荐性能之间的关系）。</p>
<p>我们初步发现：</p>
<ol>
<li>解离化程度较高与推荐性能较好这两者之间有较强的相关性；</li>
<li>引入宏观解离化后，确实大大改善了微观解离化；</li>
<li>无论是解离化程度还是推荐性能，都优于基线方法；</li>
</ol>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2021/01/20/RCOEe2biyuJkNWQ.jpg" alt="a.png"></p>
<h2>Top-N推荐任务上的性能</h2>

<p>作者接着测量了方法在几个离线数据集（包括一个淘宝的数据集AliShop-7C）上的Top-N推荐表现。可以看出来所提出来的方法优于基线方法，尤其是在小规模或稀疏的数据集上。</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2021/01/20/4lweGrM3vndTkfR.jpg" alt="b.png"></p>
<h1>Conclusion</h1>

<p>随着现代电商推荐系统的技术发展，学术界和工业界在预估点击率，预测下一个点击商品这些单任务的提高越发困难，而这样的提高所带来的增量效益也难以很好的估计。更多用户体验方面的问题被摆在了决策者的眼前，比如为什么买了又推，为什么都是点过的商品，如何创造真正增量的价值。我们目前选择围绕人的认知行为和过程，来探索新的推荐形态的可能性。</p>
<h1>Reference</h1>

<p><a href="https://www.jiqizhixin.com/articles/2019-12-13-3" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/2019-12-13-3</a></p>
<p><a href="https://www.qbitai.com/2019/12/9856.html" target="_blank" rel="noopener">https://www.qbitai.com/2019/12/9856.html</a></p>
<h1>Code</h1>

<p><a href="https://jianxinma.github.io/disentangle-recsys.html" target="_blank" rel="noopener">https://jianxinma.github.io/disentangle-recsys.html</a></p>
]]></content>
      <categories>
        <category>explainable recommendation</category>
        <category>disentangle</category>
      </categories>
      <tags>
        <tag>recommendation</tag>
        <tag>explainable recommendation</tag>
      </tags>
  </entry>
  <entry>
    <title>NDCG</title>
    <url>/2021/01/05/NDCG/</url>
    <content><![CDATA[<h1>Discounted Cumulative Gain</h1>

<p>折损累积增益</p>
<p>评估网络搜索和相关任务的流行方法</p>
<p>基于两个假设：</p>
<ul>
<li>高度相关的文件比不太相关的文件更有用；</li>
<li>相关文档的排名越低，它对用户的用处就越小，因为它不太可能被检查。</li>
</ul>
<p>Uses graded relevance as a measure of usefulness, or gain, from examine a document.</p>
<p>Gain is accumulated starting at the top of the ranking and may be reduced, or discounted, at lower ranks.</p>
<hr>
<p>Typical discount is 1/log(rank).</p>
<p>Cumulative Gain (CG) at rank n:</p>
<p>​        Let the ratings of the n documents be $r_1,r_2,…,r_n$(in ranked order)</p>
<p>​        so, CG = $r_1+r_2+…+r_n$</p>
<p>Discounted Cumulative Gain (DCG) at rank n:</p>
<p>​        DCG = $r_1 + r_2 / log_2 2+ r_3 /log_2 3+…+r_n /log_2 n$</p>
<hr>
<p>So, DCG is the total gain accumulated at a particular rank p:</p>
<script type="math/tex; mode=display">DCG_p = rel_1 + \sum_{i=2}^p \frac{rel_i}{log_2i}</script><p>or</p>
<script type="math/tex; mode=display">DCG_p = \sum_{i=1}^p \frac{2^{rel_i}-1}{log(1+i)}</script><p>this formula emphasizes on retrieving highly relevant documents.</p>
<p><b>NDCG</b></p>
<p>Normalized Discounted Cumulative Gain at rank n:</p>
<p>​        * Normalize DCG at rank n by the DCG value at rank n of the ideal ranking.</p>
<p>​        * The ideal ranking would first return the documents with the highest relevance level, then the next highest relevance level, etc.</p>
<table>
    <tr>
        <td rowspan="2">i</td> 
        <td colspan="2">Ground Truth</td>
          <td colspan="2">Ranking Function 1</td>
          <td colspan="2">Ranking Function 2</td>
   </tr>
    <tr>
        <td>Document Order</td>
        <td>ri</td>
          <td>Document Order</td>
        <td>ri</td>
          <td>Document Order</td>
        <td>ri</td>
    </tr>
    <tr>
        <td>1</td>
          <td>d4</td>
        <td>2</td>
          <td>d3</td>
          <td>2</td>
          <td>d3</td>
          <td>2</td>
    </tr>
      <tr>
        <td>2</td>
          <td>d3</td>
        <td>2</td>
          <td>d4</td>
          <td>2</td>
          <td>d2</td>
          <td>1</td>
    </tr>
      <tr>
        <td>3</td>
          <td>d2</td>
        <td>1</td>
          <td>d2</td>
          <td>1</td>
          <td>d4</td>
          <td>2</td>
    </tr>
      <tr>
        <td>4</td>
          <td>d1</td>
        <td>0</td>
          <td>d1</td>
          <td>0</td>
          <td>d1</td>
          <td>0</td>
    </tr>
      <tr>
        <td> </td>
          <td colspan="2">NDCG_GT=1.00</td>
        <td colspan="2">NDCG_RF1=1.00</td>
          <td colspan="2">NDCG_RF2=0.9203</td>
    </tr>
</table>

<script type="math/tex; mode=display">DCG_{GT} = 2 + (\frac{2}{log_2 2} + \frac{1}{log_2 3} + \frac{0}{log_2 4}) = 4.6309</script><script type="math/tex; mode=display">DCG_{RF1} = 2 + (\frac{2}{log_2 2} + \frac{1}{log_2 3} + \frac{0}{log_2 4}) = 4.6309</script><script type="math/tex; mode=display">DCG_{RF2} = 2 + (\frac{1}{log_2 2} + \frac{2}{log_2 3} + \frac{0}{log_2 4}) = 4.2619</script><p><b>reference</b></p>
<p><a href="https://web.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf" target="_blank" rel="noopener">https://web.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf</a></p>
]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>evaluation</tag>
      </tags>
  </entry>
  <entry>
    <title>DisenGCN</title>
    <url>/2021/01/04/DisenGCN/</url>
    <content><![CDATA[<p><img src= "img/loading.gif" data-src="https://i.loli.net/2021/01/04/Z6PDzCYdaxENpce.jpg" alt="1.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2021/01/04/BL7wTm6q5sSe39a.jpg" alt="2.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2021/01/04/f2p3bkiNVJWlTte.jpg" alt="3.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2021/01/04/RHLCrQ4l6TzNaV5.jpg" alt="4.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2021/01/04/ECY2ZB6j3opurKm.jpg" alt="5.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2021/01/04/ZBzTu6sjyDG1XKR.jpg" alt="6.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2021/01/04/bkMjrpRHgc8TVEU.jpg" alt="7.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2021/01/04/fqJIM9XP7Khkm5R.jpg" alt="8.png"></p>
]]></content>
      <tags>
        <tag>GCN</tag>
      </tags>
  </entry>
  <entry>
    <title>covariance matrix</title>
    <url>/2020/12/28/covariance-matrix/</url>
    <content><![CDATA[<h1>Covariance matrix(协方差矩阵)</h1>

<h3>geometric interpretation</h3>

<p>协方差矩阵定义了数据的传播（方差）和方向（协方差）。因此，如果我们想用一个向量和它的大小来表示协方差矩阵，我们应该简单地尝试找到指向数据最大传播方向上的向量，其大小等于这个方向上的传播（方差）。</p>
<p>协方差矩阵的最大特征向量总是指向数据最大方差的方向，并且该向量的幅度等于相应的特征值。第二大特征向量总是正交于最大特征向量，并且指向第二大数据的传播方向。</p>
<ul>
<li>如果数据的协方差矩阵是对角矩阵，使得协方差是零，那么这意味着方差必须等于特征值。</li>
<li>如果协方差矩阵不是对角的，使得协方差不为零，那么此时，特征值仍代表数据最大传播方向的方差大小，协方差矩阵的方差分量仍然表示x轴和y轴方向上的方差大小。<br>特征值表示沿特征向量方向数据的方差，而协方差矩阵的方差分量表示沿轴的传播。</li>
</ul>
<p>协方差矩阵是非负定矩阵。</p>
<h3>references</h3>

<p><a href="https://blog.csdn.net/duanyule_cqu/article/details/54959897" target="_blank" rel="noopener">https://blog.csdn.net/duanyule_cqu/article/details/54959897</a></p>
<p><a href="https://www.cnblogs.com/jins-note/p/10826272.html" target="_blank" rel="noopener">https://www.cnblogs.com/jins-note/p/10826272.html</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/37609917" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/37609917</a></p>
<p><a href="https://www.zhihu.com/question/22237507" target="_blank" rel="noopener">https://www.zhihu.com/question/22237507</a></p>
]]></content>
      <tags>
        <tag>covariance matrix</tag>
      </tags>
  </entry>
  <entry>
    <title>similarity</title>
    <url>/2020/12/27/similarity/</url>
    <content><![CDATA[<ul>
<li>皮尔逊相关系数</li>
<li><p>欧氏距离</p>
</li>
<li><p>曼哈顿距离</p>
</li>
<li><p>切比雪夫距离</p>
</li>
<li><p>闵可夫斯基距离</p>
</li>
<li><p>标准化欧氏距离</p>
</li>
<li><p>马氏距离</p>
</li>
<li><p>夹角余弦</p>
</li>
<li><p>汉明距离</p>
</li>
<li><p>杰卡德距离 &amp; 杰卡德相似系数</p>
</li>
<li><p>相关系数 &amp; 相关距离</p>
</li>
<li><p>信息熵</p>
</li>
</ul>
<h1>Pearson correlation coefficient(PCC)</h1>

<p><b>皮尔逊相关系数</b>是一个介于-1和1之间的数，它度量两个一一对应数列之间的线形相关程度。也就是说，它表示两个数列中对应数字一起增大或者一起减少的可能性。它度量数字一起按比例改变的倾向性，也就是说两个数列中的数字存在一个大致的线形关系。当该倾向性强时，相关值趋于1；当相关性很弱时，相关值趋于0。在负相关的情况下一个序列的值很高而另一个序列的值低——-相关值趋于-1.</p>
<font color='red'>皮尔逊相关系数是两个序列协方差与两者方差乘积的比值</font>

]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>NMF</title>
    <url>/2020/12/16/NMF/</url>
    <content><![CDATA[<h1>Main idea</h1>

<script type="math/tex; mode=display">V_{m*n} \approx W_{m*r} H_{r*n}</script><h1>Methode</h1>

<h3>基于欧式距离</h3>

<p>损失函数为$J(W,H)=1/2\sum_{i,j}[V_{i,j}-(WH)_{i,j}]^2$</p>
<p>因此，我们分别对<code>W</code>和<code>H</code>求偏导得到两个变量的梯度，具体如下</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/12/16/5zE1TIF9rGRJkxb.jpg" alt="1.JPG"></p>
<p>根据上图的公式<code>(1)</code>和<code>(2)</code>，可以使用梯度下降的方式得到<code>W和H</code>的更新公式：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/12/16/OwzgdbDnk8YKlXv.jpg" alt="2.png"></p>
<h3>基于KL散度</h3>

<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/12/16/YWfhAiGXR3wQr6n.jpg" alt="kl.png"></p>
<h3>关于KL散度的一些推导</h3>

<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/12/16/nUJ7BmGMrXoV5dQ.jpg" alt="1.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/12/16/8ZeGLFdOt2zfb3p.jpg" alt="2.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/12/16/DU9utP3kzJxWjFc.jpg" alt="3.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/12/16/rgBXdzsCu5TY9RL.jpg" alt="4.png"></p>
<h1>互信息</h1>

<font color='red'>互信息（Mutual Information，MI）可以把它看成是一个随机变量中包含的有关于另一个随机变量的信息量，表示两个变量X、Y是否有关系或者指的是两变量之间的关联程度。</font>

<p>如果<code>(X,Y)~p(x,y)</code>，则X和Y之间的互信息<code>I(X,Y)</code>定义为：</p>
<p>$I(X;Y)=H(X)-H(X|Y)$</p>
<p>上式中，<code>H(X)</code>指的是X的熵值，可以写出公式：</p>
<p>$H(X)=-\sum_{x&lt;X}p(x)log_2p(x)$</p>
<p>$H(X|Y)=-\sum_{x&lt;X}\sum_{y&lt;Y}p(x,y)log_2p(x|y)$</p>
<p>所以，可以把互信息的具体值表示为：</p>
<script type="math/tex; mode=display">I(X;Y)=\sum_{x<X}\sum_{y<Y}p(x,y)log_2\frac{p(x|y)}{p(x)}</script><p>在知道其中一个随机变量后，表示另一个随机变量不确定性削弱的程度，因而互信息最小值为0，这意味着在给定一个随机变量后，对确定另一个随机变量没有任何关系。最大取值表示为一个随机变量的熵，意味着在给定一个随机变量后，则能够完全消除另一个随机变量的完全不确定性。<b>也就是说，互信息值越大，两个项之间的关系越近，相似度越大。</b></p>
<p>互信息的计算公式可简化为：</p>
<script type="math/tex; mode=display">I(x,y)=log_2{\frac{p(x,y)}{p(x)p(y)}}</script><p>上式中，<code>I(x,y)</code>表示x与y的互信息值；<code>p(x,y)</code>是x与y共同出现的频率；<code>p(x),p(y)</code>分别是x，y单独出现的频率；<code>I(x,y)</code>越大，表明x，y的关联程度比较强，即x与y越相似。</p>
<p><b>互信息不仅能说明两者是否有关系，还能反应它们之间的强弱，这个特性可以很好的计算两个项的相似性，推荐系统中，会使用到这种相似性比较，所以利用互信息算法计算项与项之间的相似度，然后根据相似值的大小推荐物品，实现对物品的推荐。</b></p>
<h1> beta-divergence family</h1>

<p>仅看损失函数：</p>
<ul>
<li>Squared Frobenius norm: <script type="math/tex; mode=display">\underbrace{arg min}_{W,H} \frac{1}{2} ||A-WH||^2_{Fro}+\alpha  \rho ||W||_1+\alpha \rho ||H||_1 + \frac{\alpha (1-\rho)}{2}||W||^2_{Fro}+\frac{\alpha (1-\rho)}{2}||H||^2_{Fro}</script></li>
</ul>
<p>其中，<script type="math/tex">\frac{1}{2} ||A-WH||^2_{Fro}=\frac{1}{2} \sum_{i,j} (A_{ij}-WH_{ij})^2</script></p>
<ul>
<li><p>Kullback-Leibler (KL):</p>
<script type="math/tex; mode=display">d_{KL}(X,Y)=\sum_{i,j}(X_{ij}log(\frac{X_{ij}}{Y_{ij}})-X_{ij}+Y_{ij})</script></li>
<li><p>Itakura-Saito (IS):</p>
<script type="math/tex; mode=display">d_{IS}(X,Y)=\sum_{ij}(\frac{X_{ij}}{Y_{ij}}-log(\frac{X_{ij}}{Y_{ij}})-1)</script></li>
</ul>
<p>实际上，上面三个公式是beta-divergence family中的三个特殊情况（分别是当beta=2，1，0），其原型是：</p>
<script type="math/tex; mode=display">d_\beta (X,Y)=\sum_{i,j}\frac{1}{\beta (\beta -1)}(X_{ij}^\beta +(\beta -1)Y_{ij}^\beta )-\beta X_{ij}Y_{ij}^{\beta -1}</script>]]></content>
      <categories>
        <category>recommendations</category>
      </categories>
      <tags>
        <tag>recommendation</tag>
        <tag>NMF</tag>
      </tags>
  </entry>
  <entry>
    <title>GraghSAGE</title>
    <url>/2020/12/08/GraghSAGE/</url>
    <content><![CDATA[<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/12/09/sc3auIX6nzE5dAb.png" alt="framework"></p>
<blockquote>
<font color='red'>GCN的缺点：</font>

<ol>
<li>GCN的训练方式需要将邻接矩阵和特征矩阵一起放到内存或者显存里，在大规模图数据上是不可取的。</li>
<li>GCN在训练时需要知道整个图的结构信息（包括待预测的节点），这在现实某些任务中也不能实现（比如用今天训练的图模型预测明天的数据，那么明天的节点是拿不到的）。</li>
</ol>
</blockquote>
<h1>Inductive learning v.s. Transductive learning</h1>

<p>与其他类型的数据不同，图数据中的每一个节点可以通过边的关系利用其他节点的信息，这样就产生了一个问题：如果训练集上的节点通过边关联到了预测集或者验证集的节点，那么在训练的时候能否用它们的信息呢？</p>
<p>如果训练时用到了测试集或验证集样本的信息（或者说，测试集和验证集在训练的时候是可见的），我们把这种学习方式叫做Transductive learning，反之，称为inductive learning。</p>
<p>显然，我们所处理的大多数机器学习问题都是inductive learning，因为我们刻意的将样本集分为训练/验证/测试，并且训练的时候只用训练样本。然而，在GCN中，训练节点收集邻居信息的时候，用到了测试或者验证样本，所以它是Transductive的。</p>
<h1>概述</h1>

<p>GraphSAGE是一个inductive框架，在具体实现中，训练时它仅仅保留训练样本到训练样本的边。Inductive learning 的优点是可以利用已知节点的信息为未知节点生成Embedding。GraphSAGE 取自Graph SAmple and aggreGatE，SAmple指如何对邻居个数进行采样，aggreGatE指拿到邻居的embedding之后如何汇聚这些embedding以更新自己的embedding信息。</p>
<p><img src= "img/loading.gif" data-src="https://pic2.zhimg.com/80/v2-f8301d7397b1c703454e5adedbc9d621_1440w.jpg" alt="img"></p>
<p>上图为它的过程，分为三步：</p>
<ol>
<li>对邻居采样；</li>
<li>采样后的邻居embedding传到节点上来，并使用一个聚合函数聚合这些邻居信息以更新节点的embedding；</li>
<li>根据更新后的embedding预测节点的标签。</li>
</ol>
<h1>算法细节</h1>

<h3>节点Embedding生成（即：前向传播）算法</h3>

<p><em>这里讨论如何给图中的节点生成（或者说更新）embedding</em></p>
<p>假设我们已经完成了GraphSAGE的训练，因此模型的所有参数都已知了。具体来说，这些参数包括<code>K</code>个聚合器$AGGREGATE_k,\forall k\in 1,…,K$（见下图算法第4行）中的参数，这些聚合器被用来将邻居embedding信息聚合到节点上，以及一系列的权重矩阵$W^k,\forall k\in 1,…,K$(下图算法第3行)，这些权重矩阵被用作在模型层与层之间传播embedding的时候做非线性变换。</p>
<p><img src= "img/loading.gif" data-src="https://pic2.zhimg.com/80/v2-5ac927cd1fca0c700e18e3fc5ef55b45_1440w.jpg" alt="img"></p>
<p>算法的主要部分为：</p>
<ol>
<li>（line 1）初始化每个节点embedding为节点的特征向量；</li>
<li>（line 3）对于每个节点$v$；</li>
<li>（line 4）拿到它采样后的邻居的embedding$h_u,u\in \mathcal N(v)$并将其聚合，这里的$\mathcal N(v)$表示对邻居采样；</li>
<li>（line 5）根据聚合后的邻居embedding$(h_{N(v)})$和自身的embedding$(h_v)$通过一个非线性变换（$\sigma (W·\square)$）更新自身embedding。</li>
</ol>
<p>算法里的<code>K</code>：它既是聚合器的数量，也是权重矩阵的数量，还是网络的层数，这是因为每一次网络中聚合器和权重矩阵是共享的。</p>
<p>网络的层数可以理解为需要最大访问到的邻居的条数（hops），比如在上上张图（figure1）中，红色节点的更新拿到了它一、二跳邻居的信息，那么网络层数就是2。</p>
<p>为了更新红色节点，首先在第一层（k=1）我们会将蓝色节点的信息聚合到红色节点上，将绿色节点的信息聚合到蓝色节点上。在第二层（k=2）红色节点的embedding被再次更新，不过这次用的是更新后的蓝色节点embedding，这样就保证了红色节点更新后的embedding包括蓝色和绿色节点的信息。</p>
<h3>采样（SAmple）算法</h3>

<p>GraphSAGE采用了定长抽样的方法，具体来说，定义需要的邻居个数<code>S</code>，然后采用有放回的重采样/负采样方法达到<code>S</code>。保证每个节点（采样后的）邻居个数一致是为了把多个节点以及他们的邻居拼成一个Tensor送到GPU中进行批训练。</p>
<p><b>这里的重采样/负采样是指：若顶点邻居数少于<code>S</code>，则采用有放回的抽样，直到采样出<code>S</code>个顶点。若顶点邻居数大于<code>S</code>，则采用无放回的抽样。</b></p>
<h3>聚合器（Aggregator）算法</h3>

<p>GraphSAGE提供了多种聚合器（MEAN、pooling、LSTM），实验中效果最好的平均聚合器（mean aggregator），思想是：每个维度取对邻居embedding相应维度的均值，这个和GCN的做法基本一致（GCN实际上用的是求和）：</p>
<p><img src= "img/loading.gif" data-src="https://www.zhihu.com/equation?tex=h%5Ek_v%5Cleftarrow+%5Csigma%28%5Cbm%7BW%7D%5Ccdot+%5Ctext%7BMEAN%7D%28%5Cleft+%5C%7B+h_v%5E%7Bk-1%7D+%5Cright+%5C%7D+%5Ccup+%5Cleft%5C%7B+h_u%5E%7Bk-1%7D%2C%5Cforall+u%5Cin+N%28v%29+%5Cright%5C%7D%29" alt="[公式]"></p>
<h3>参数学习</h3>

<p>在定义好聚合函数之后，接下来就是对函数中的参数进行学习。文章分别介绍了无监督学习和监督学习两种方式。</p>
<ul>
<li>无监督学习形式</li>
</ul>
<p>基于图的损失函数希望临近的顶点具有相似的向量表示，同时让分离的顶点的表示尽可能区分。 目标函数如下</p>
<p><img src= "img/loading.gif" data-src="https://pic3.zhimg.com/80/v2-458b2b674141585314bd51fb04de309a_1440w.png" alt="img"></p>
<p>其中v是通过固定长度的随机游走出现在u附近的顶点， <img src= "img/loading.gif" data-src="https://www.zhihu.com/equation?tex=p_n" alt="[公式]"> 是负采样的概率分布， <img src= "img/loading.gif" data-src="https://www.zhihu.com/equation?tex=Q" alt="[公式]"> 是负样本的数量。</p>
<p>与DeepWalk不同的是，这里的顶点表示向量是通过聚合顶点的邻接点特征产生的，而不是简单的进行一个embedding lookup操作得到。</p>
<ul>
<li>监督学习形式</li>
</ul>
<p>监督学习形式根据任务的不同直接设置目标函数即可，如最常用的节点分类任务使用交叉熵损失函数。</p>
<hr>
<h1>Code</h1>

<p>这里以MEAN aggregator简单讲下聚合函数的实现</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">features, node, neighbours = inputs</span><br><span class="line"></span><br><span class="line">node_feat = tf.nn.embedding_lookup(features, node)</span><br><span class="line">neigh_feat = tf.nn.embedding_lookup(features, neighbours)</span><br><span class="line"></span><br><span class="line">concat_feat = tf.concat([neigh_feat, node_feat], axis=<span class="number">1</span>)</span><br><span class="line">concat_mean = tf.reduce_mean(concat_feat,axis=<span class="number">1</span>,keep_dims=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">output = tf.matmul(concat_mean, self.neigh_weights)</span><br><span class="line"><span class="keyword">if</span> self.use_bias:</span><br><span class="line">    output += self.bias</span><br><span class="line"><span class="keyword">if</span> self.activation:</span><br><span class="line">    output = self.activation(output)</span><br></pre></td></tr></table></figure>
<p>对于第 <img src= "img/loading.gif" data-src="https://www.zhihu.com/equation?tex=k" alt="[公式]"> 层的aggregator，<code>features</code>为第 <img src= "img/loading.gif" data-src="https://www.zhihu.com/equation?tex=k-1" alt="[公式]"> 层所有顶点的向量表示矩阵，<code>node</code>和<code>neighbours</code>分别为第k层采样得到的顶点集合及其对应的邻接点集合。</p>
<p>首先通过<code>embedding_lookup</code>操作获取得到顶点和邻接点的第 <img src= "img/loading.gif" data-src="https://www.zhihu.com/equation?tex=k-1" alt="[公式]"> 层的向量表示。然后通过<code>concat</code>将他们拼接成一个<code>(batch_size,1+neighbour_size,embeding_size)</code>的张量，使用<code>reduce_mean</code>对每个维度求均值得到一个<code>(batch_size,embedding_size)</code>的张量。</p>
<p>最后经过一次非线性变换得到<code>output</code>，即所有顶点的第 <img src= "img/loading.gif" data-src="https://www.zhihu.com/equation?tex=k" alt="[公式]"> 层的表示向量</p>
<ul>
<li>下面是完整的GraphSAGE方法的代码</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">GraphSAGE</span><span class="params">(feature_dim, neighbor_num, n_hidden, n_classes, use_bias=True, activation=tf.nn.relu,</span></span></span><br><span class="line"><span class="function"><span class="params">              aggregator_type=<span class="string">'mean'</span>, dropout_rate=<span class="number">0.0</span>, l2_reg=<span class="number">0</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">    features = Input(shape=(feature_dim,))</span><br><span class="line">    node_input = Input(shape=(<span class="number">1</span>,), dtype=tf.int32)</span><br><span class="line">    neighbor_input = [Input(shape=(l,),dtype=tf.int32) <span class="keyword">for</span> l <span class="keyword">in</span> neighbor_num]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> aggregator_type == <span class="string">'mean'</span>:</span><br><span class="line">        aggregator = MeanAggregator</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        aggregator = PoolingAggregator</span><br><span class="line"></span><br><span class="line">    h = features</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(neighbor_num)):</span><br><span class="line">        <span class="keyword">if</span> i &gt; <span class="number">0</span>:</span><br><span class="line">            feature_dim = n_hidden</span><br><span class="line">        <span class="keyword">if</span> i == len(neighbor_num) - <span class="number">1</span>:</span><br><span class="line">            activation = tf.nn.softmax</span><br><span class="line">            n_hidden = n_classes</span><br><span class="line">        h = aggregator(units=n_hidden, input_dim=feature_dim, activation=activation, l2_reg=l2_reg, use_bias=use_bias,</span><br><span class="line">                       dropout_rate=dropout_rate, neigh_max=neighbor_num[i])(</span><br><span class="line">            [h, node_input,neighbor_input[i]])<span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    output = h</span><br><span class="line">    input_list = [features, node_input] + neighbor_input</span><br><span class="line">    model = Model(input_list, outputs=output)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<p>其中<code>feature_dim</code>表示顶点属性特征向量的维度，<code>neighbor_num</code>是一个<code>list</code>表示每一层抽样的邻居顶点的数量，<code>n_hidden</code>为聚合函数内部非线性变换时的参数矩阵的维度，<code>n_classes</code>表示预测的类别的数量，<code>aggregator_type</code>为使用的聚合函数的类别。</p>
<h1>Reference</h1>

<p><a href="https://zhuanlan.zhihu.com/p/79637787" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/79637787</a></p>
]]></content>
      <categories>
        <category>GNN</category>
      </categories>
      <tags>
        <tag>Graph SAmple and aggreGatE</tag>
      </tags>
  </entry>
  <entry>
    <title>GCN</title>
    <url>/2020/12/07/GCN/</url>
    <content><![CDATA[<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/12/07/JTgIyQax2VevGHP.png" alt="PNG图像-A66D4F0A9E52-1.png"></p>
<h1>GCN简介</h1>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Kipf T N , Welling M . Semi-Supervised Classification with Graph Convolutional Networks[J]. 2016.</span><br></pre></td></tr></table></figure>
<p>这篇文章是图神经网络的开山之作，之前文章中总结过这篇文章。它利用了近似的技巧推导出了一个简单而高效的模型，使得图像处理中的卷积操作能够简单的被用到图结构数据处理中来。</p>
<p>考虑图（例如引文网络）中节点（例如文档）的分类问题，假设该图中只有一小部分节点标签（label）是已知的，我们的分类任务是想通过这部分已知标签的节点和图的结构来推断另一部分未知标签的节点的标签。这类问题可以划分到基于图结构数据的半监督学习问题中。半监督学习（semi-supervised learning）是有监督学习的一个分支，主要研究的是如何利用少量的有标签数据学习大量无标签数据的标签。</p>
<p>为了对节点进行分类，首先我们可以利用节点自身的特征信息，除此之外，我们还可以利用图结构信息。因此一个典型的图半监督学习问题可以对两个损失函数一起优化：</p>
<script type="math/tex; mode=display">L = L_{labeled} + \lambda L_{reg}</script><p>其中，$L_{labeled}$是基于标签数据的损失函数，$L_{reg}$代表基于图结构信息的损失函数，$\lambda$是调节这两个损失函数相对重要性的超参。</p>
<p>一般来说，基于图结构信息的损失函数可以表示成：</p>
<script type="math/tex; mode=display">L_{reg}=\sum_{i,j}A_{ij}|f(X_i)-f(X_j)|^2=f(X)^T\Delta f(X)</script><p>其中，$f(·)$是类似神经网络的可微分函数。$X$表示节点特征向量构成的矩阵，其中$X_i$表示节点$v_i$的特征向量，邻接矩阵表示为$A\in \mathbb{R}^{N*N}$（可以是二值的，也可以是加权的），度矩阵$D_{ii}=\sum_jA_{ij}$。$\Delta=D-A$表示无向图的未正则化图拉普拉斯算子。这样的损失函数希望对相邻节点的特征向量做限制，希望它们能尽量相近。</p>
<p>显然，这样的学习策略基于图中的相邻节点标签可能相同的假设（因为损失函数要求相邻节点的特征向量尽量相似，如果它们的标签不相似的话，那么不能学习到一个从特征向量到标签的有效映射）。然而，这个假设可能会限制模型的能力，因为图的边在语义上不一定代表所连接节点相似。还有一种可能是图中有大量的噪声边。</p>
<p>因此，在这个工作中，作者不再显示的定义图结构信息的损失函数$L_{reg}$，而是使用神经网络模型$f(X,A)$直接对图结构进行编码，训练所有带标签的节点，来避免损失函数中的正则化项$L_{reg}$。</p>
<h1>数学知识</h1>



<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/12/07/MgjCfzFNWoDa4sL.jpg" alt="IMG_4140DE101473-1.jpeg"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/12/07/i2rxetFMqjsKuN4.jpg" alt="IMG_45A4D6919D7A-1.jpeg"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/12/07/RQTol8xhEPY5OjC.jpg" alt="IMG_2F75388E4672-1.jpeg"></p>
<h1>图上的快速卷积近似</h1>

<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/12/07/zoxMIKjEsu3CQ2r.png" alt="WeChatde338cdd26d1be3dd44ef1c58320b6f6.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/12/07/abNeut9GCRlsi58.png" alt="WeChat1d8bd69c63cb38990eb9503758b0c121.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/12/07/OmvFPxNELB4Rsrd.png" alt="WeChat666cd96f3ca91607ad6eed3460c822e3.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/12/07/9rLVbQ5Ma6kWltA.png" alt="WeChat8da5fee9e0779fd2525fd12026c1b8a3.png"></p>
<h1>半监督学习节点分类</h1>

<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/12/07/wrLNflKqJPuWXMn.png" alt="WeChat862668ee952cfe758d7d07c38b67fb10.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/12/07/QKcWiXeBIGMaZgu.png" alt="WeChatdc68d2b7ec40be173b8f0bb56476b844.png"></p>
<h1>Code</h1>

<p>git@github.com:Megvii-Nanjing/ML-GCN.git</p>
]]></content>
      <categories>
        <category>GNN</category>
      </categories>
      <tags>
        <tag>Graph Convolutional Networks</tag>
      </tags>
  </entry>
  <entry>
    <title>DGCF</title>
    <url>/2020/12/01/DGCF/</url>
    <content><![CDATA[<h1>Main idea</h1>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;DGCF19,</span><br><span class="line">  author    = &#123;Xiang Wang and</span><br><span class="line">               Hongye Jin and</span><br><span class="line">               An Zhang and</span><br><span class="line">               Xiangnan He and</span><br><span class="line">               Tong Xu and</span><br><span class="line">               Tat&#123;-&#125;Seng Chua&#125;,</span><br><span class="line">  title     = &#123;Disentangled Graph Collaborative Filtering&#125;,</span><br><span class="line">  booktitle = &#123;Proceedings of the 43nd International &#123;ACM&#125; &#123;SIGIR&#125; Conference on</span><br><span class="line">               Research and Development in Information Retrieval, &#123;SIGIR&#125; 2020, Xi&apos;an,</span><br><span class="line">               China, July 25-30, 2020.&#125;,</span><br><span class="line">  year      = &#123;2020&#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Disentangled Graph Collaborative Filtering (DGCF) is an explainable recommendation framework, which is equipped with <b>(1) dynamic routing mechanism of capsule networks, to refine the strengths of user-item interactions in intent-aware graphs, (2) embedding propagation mechanism of graph neural networks, to distill the pertinent information from higher-order connectivity, and (3) distance correlation of independence modeling, to ensure the independence among intents.</b> As such, It explicitly disentangle the hidden intents of users in the representation learning.</p>
<h1>Abstract</h1>

<p>从历史交互数据中学习用户、物品的表示是协同过滤中重要的一步。当前的嵌入方程利用用户-项目关系来丰富表示，从单个用户-项目实例演变为整体交互图。尽管如此，大多数的模型仍然使用统一的方式处理用户-物品之间的关系，忽略了不同用户意图对采购物品的影响，例如消磨时间、真正感兴趣、给家人买东西等。这种统一的方法对对用户进行建模容易导致次优表示（suboptimal representations），未能对不同的关系进行建模并在表示中分离用户意图。</p>
<p>在本文中，我们特别关注在用户意图的细粒度上的用户-物品关系。我们提出了一个新模型叫做DGCF（Disentangled Graph Collaborative Filtering），来分解表示。通过对每个用户项-物品交互的意图分布建模，我们迭代地完善了意图感知交互图和表示形式。同时，我们鼓励不同意图的独立性。这将导致分离的表示，有效的从每个意图中提取相关的信息。</p>
<h1>Introduction</h1>

<p>推荐系统已经在现实生活被被广泛应用。推荐系统中，准确捕捉用户偏好的能力是核心。一个有效的解决方法—协同过滤，它聚焦于历史的用户-物品交互矩阵，假设有相似行为的用户会对物品也有相似的偏好。学习富含信息的表示对提升CF性能是十分重要的。CF的发展也从最早的只使用简单的ID到个人历史，再到现在的利用历史交互图。</p>
<p>尽管这些方法有效，我们认为先前的建模用户项关系的方式不足以发现分离的用户意图。主要原因是现有的嵌入方法不能区分在不同物品上用户的意图，把一个用户-物品交互当成一个独立的数据实例并统一把它们当作在交互图上的一条边来训练神经网络。这忽略了一个基本事实：一个用户有很多种可能来和一个物品有交互，不同的意图会驱使用户有不同的行为。如图所示，用户$u$看电影$i_1$只是消磨时间，而用户$u$看电影$i_{2}$主要是因为是因为她对电影$i_{2}$的导演感兴趣。所以，过去对用户-物品关系的建模是由限制的：1）不考虑实际的用户意图很容易导致次优表示 2）由于嘈杂行为（如随机点击）通常存在于用户的交互历史中，这会混淆用户的意图并使表示对噪声交互的鲁棒性降低 3）用户的意图会变得模糊和高度纠缠在表示中，从而导致解释性差。（意图不能明确的反映在表示中）</p>
<p><img src= "img/loading.gif" data-src="https://img-blog.csdnimg.cn/20201026170124561.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4NDg1NA==,size_16,color_FFFFFF,t_70" alt="img"></p>
<p>故我们致力于在更精细的用户意图级别建立模型表示。直观的说，有很多意图会影响用户的行为，例如消磨时间、兴趣、给家人买东西。我们需要学习每种用户行为的用户意图分布，总结每种意图的置信度是用户采用某项商品的原因。联合分析所有历史交互的这种分布，我们可以获得一组意图感知的交互图，这进一步提炼了用户意图的信号。但是，由于以下挑战，这并非易事：1）如何在表示中明确地呈现出与每个意图相关的信号还不清楚，而且还没有探索 2）意图之间的独立性影响着解缠的质量，这就需要一个定制的模型</p>
<p>在本文种，我们提出了一个新的模型，DGCF，以用户意图的粒度分解用户和项的表示。特别是，我们首先把每个用户、物品的表示分成块，把每块和一个潜在意图相连。然后使用图解缠模块配合邻居路由和嵌入传播机制。邻居路由利用节点-邻居亲和力来优化意图感知图，突出了用户与项目之间有影响力关系的重要性。然后，在此类图上嵌入传播会更新节点的意图感知嵌入。通过迭代这种分离操作，我们建立了一组意图感知图和分块表示。同时，一个独立建模模块用来鼓励不同的意图是相互独立的。具体而言，在意图感知表示中采用统计量度，距离相关性。在这步最后，我们获得了分离的表示和意向的解释图。</p>
<p>总之，我们做出了一下贡献：</p>
<p>1）我们强调不同的用户项关系在协同过滤中的重要性，并且这种关系的建模可以带来更好的表示和解释性</p>
<p>2）我们提出了一个新的DGCF模型，它在用户意图的更细粒度上考虑用户项关系，并生成分离的表示</p>
<p>3）我们在三个数据上进行了实验，证明了我们的模型对于推荐是十分有效的，潜在用户意图的解构与表征的可解释性。</p>
<h1>Preliminary and related work</h1>

<p>首先介绍了CF的表示学习，指出了现有工作和用户-物品关系的不足。之后，本文提出了一种CF分解图表示的任务公式。</p>
<h2>Learning for collaborative filtering</h2>

<h3>Learning paradigm of CF</h3>

<p>在用户-物品id之间存在语义障碍，这些表面特征之间没有重叠，妨碍交互建模。未来缩小这个障碍，许多研究用来是模型学习更加有信息的用户-物品表示。总节起来就是下图：</p>
<p><img src= "img/loading.gif" data-src="https://img-blog.csdnimg.cn/20201026173122849.png" alt="img"></p>
<p>这里$u$和$i$是用户-物品的id，$e_{u}\in \mathbb{R}^{d}$和$e_{i}\in \mathbb{R}^{d}$分别是用户$u$和物品$i$的表示。通常使用下面的公式预测，它将预测任务作为同一潜在空间中用户$u$和物品$i$之间的相似性估计：</p>
<p><img src= "img/loading.gif" data-src="https://img-blog.csdnimg.cn/20201027102729900.png" alt="img"></p>
<h3>Representation learning of CF</h3>

<p>有很多方法可以应用到CF的表示学习中，</p>
<p>从交互图的角度，我们可以重新审视用户-项目关系，比如用户和项目节点之间的连通性。</p>
<p>总之，用户项目关系可以明确表示为连通性：单个ID（即无连通性），个人历史记录（即一阶连通性），整体交互图（高阶连通性）。</p>
<h3>Limitations</h3>

<p>尽管取得了成功，但是这些统一建模用户-物品关系不足以反映出用户的隐藏意图。这限制了CF表示的可解释性和理解性。具体来说，现有的嵌入表示方法都是使用黑盒神经网络应用在关系中（交互图），并且输出表示。不能区别用户购买物品的不同意图，只是假设用户行为背后有统一的动机。但是，这与现实是相违背的。</p>
<p>没有对用户意图进行建模，现有的工作难以提供可解释的嵌入表示、语义理解和什么信息被嵌入进了表示中。总的来说，每个交互$(u,i)$对于$e_{u}$的表示都是不可分辨的。结果导致每个行为背后的隐藏意图都被高度纠缠在了嵌入中，模糊意图和特定维度之间的映射。</p>
<p>在推荐中使用解缠表示是非常少见的，知道最近的$MacridVAE$，他把$\beta -VAE$应用在了交互数据中，并且实现了对用户的解缠表示。由于$\beta -VAE$的局限性，只是更具历史物品（用户的一阶邻居）被用在表示中，忽略了用户-物品关系的高阶连通性。</p>
<h2>Task formulation</h2>

<p>我们的任务有两个子任务：1）在用户意图的粒度级别上探索用户项关系 2）生成分离的CF表示</p>
<h3>Exploring user-item relationships</h3>

<p>我们的用户行为可以被多种意图驱动。以电影推荐为例，用户$u$用电影$i_{1}$打发时间，因此可能不太关心$i_{1}$的导演是否符合她的兴趣；然而$u$看$i_{2}$是因为$i_{2}$的导演是兴$u$趣的一个重要因素。准确的说，不同的用户有不同的贡献行为。</p>
<p>为了对用户和项之间的这种细粒度关系进行建模，我们的目标是了解每个行为的用户意图上的分布$A(u,i)$，<script type="math/tex">A(u,i)=(A_1(u,i,···,A_K(u,i)))</script></p>
<p> 其中$A_{k}(u,i)$ 反映了为什么$u$采取$i$的第$K$个意图的概率;$K$是一个超参数用来控制用户隐藏意图的个数。共同检查与特定意图$k$相关的分数，我们可以得出意向图$G_{k}$，被定为成$G_{k}=\left \{ (u,i,A_{k}(u,i)) \right \}$，每个历史交互都被当作一条边分配给$A_{k}(u,i)$。之后，一个带权邻接矩阵$A_{k}$建立在$G_{k}$上。最后，我们得出了一系列的意向图$G=\left \{ G_{1},…,G_{K} \right \}$来分别表示用户物品之间的关系，而不是像之前的工作一样只使用一个。</p>
<h3>Generating disentangled representations</h3>

<p>生成用户-物品的解缠表示，具体来说就是提取与个体意图相关的信息作为表示的独立部分。更具体的说，就是设计一个嵌入方程$f(\cdot )$，对于用户$u$生成一个分离的表示$e_{u}$，它含有$k$个组成部分：</p>
<script type="math/tex; mode=display">e_u=(e_{1u},e_{2u},···,e_{Ku})</script><p>这里的$e_{ku}$表示用户$u$的第$k$个隐藏意图。为简单起见，我们把这些部分都做成相同的维度，所以$e_{ku}\in \mathbb{R}^{\frac{d}{K}}$。值得注意的是$e_{ku}$应该独立于$e_{k’u}$如果$k’\neq k$，以此减少语义冗余并且鼓励对个人意图的信号进行最大限度的压缩。每块表示$e_{ku}$建立在意向图$G_{k}$中并综合相关的联系。类似地，类似的，我们同时可以生成$e_{i}$的表示。</p>
<h1>Methodology</h1>

<p>现在我们提出解缠图协同过滤，叫做DGCF，在图二中显示。它由两个关键部件组成，以实现分离 1）图解缠模块首先将嵌入的每个用户/项目切片成块，将每个块与意图耦合，然后将新的邻居路由机制引入图神经网络中，从而对交互图进行分解，细化意图感知表示。 2）独立性建模模块，利用距离相关性作为正则化器来鼓励意图的独立性。DGCF最终产生具有意图感知解释图的分离表示。</p>
<p><img src= "img/loading.gif" data-src="https://img-blog.csdnimg.cn/20201027204002659.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4NDg1NA==,size_16,color_FFFFFF,t_70" alt="img"></p>
<h2>Graph disentangling module</h2>

<p>研究表明GNNs在图结构中使用嵌入传播机制可以从高阶邻居节点中提取有用的的信息，丰富节点的表示。更具体地说，一个节点从它的邻居那里收集信息并更新它的表示。明确的说，节点间的连接提供给数据流动一个明确的通路。我们发展了GNN模型，叫做图解缠（分离）层，它在嵌入传播中引入了一种新的邻居路由机制，来更新这些图的权重。这使我们能够区分每个用户项连接的不同重要性分数，以细化交互图，进而将信号传播到意图感知块。</p>
<h3>Intent-aware embedding initialization</h3>

<p>与主流的CF模型不同，我们将ID嵌入切分到K个块中，将每个块与一个潜在意图相关联。具体的说，用户的嵌入初始化为：</p>
<p><img src= "img/loading.gif" data-src="https://img-blog.csdnimg.cn/20201027204720342.png" alt="img"></p>
<p>这里的$u\in \mathbb{R}^{d}$是一个ID嵌入来捕获用户$u$的固有特性；$u_{k}\in \mathbb{R}^{\frac{d}{K}}$是用户$u$的第$k$个意图的表示。类似地，$i=(i_{1},i_{2},…,i_{K})$表示物品<img src= "img/loading.gif" data-src="https://private.codecogs.com/gif.latex?i" alt="i">。我们分别采用随机初始化来初始化每个块表示，确保训练开始时的意图差异。</p>
<h3>Intent-aware graph initialization</h3>

<p>我们提到早期的工作不足以提取用户行为背后的意图，因为它们只有一个用户-物品交互图或者说同制评分图来展示用户和物品之间的关系。因此，我们为K个意图定义一组得分矩阵$\left \{ S_{K}|\forall k\in \left \{ 1,\cdot \cdot \cdot ,K \right \} \right \}$。关注于意向图矩阵$S_{k}$，每一项$S_{k}(u,i)$表明用户$u$和物品$i$之间的交互。换个角度，对于每个交互，我们可以得到一组评分向量$S(u,i)=((S_{1}(u,i),\cdot \cdot \cdot ,(S_{K}(u,i))\in \mathbb{R}^{K}$在个$K$隐藏意图中。我们最开始统一初始化为：</p>
<p><img src= "img/loading.gif" data-src="https://img-blog.csdnimg.cn/20201027210029875.png" alt="img"></p>
<p>这假设在建模开始时意图的贡献是相等的。因此，这种评分矩阵可以看成是意向图的邻接矩阵。</p>
<h3>Graph disentangling layer</h3>

<p>现在每个意图<img src= "img/loading.gif" data-src="https://private.codecogs.com/gif.latex?k" alt="k">都包含了一系列分离表示，$\left \{ u_{k},i_{k}|u\in U,i\in I \right \}$。在一个意向通道内，我们想要从用户-物品的高阶连通性中提取有效的信息，不仅仅只是id嵌入。为了实现此，我们提出了一种新的图解缠层，它配了邻居路由和嵌入传播机制，目标是区分每个用户-物品连接时传播信息的自适应作用。我们定义这种层为：</p>
<p><img src= "img/loading.gif" data-src="https://img-blog.csdnimg.cn/20201027210906927.png" alt="img"></p>
<p>这里的$e_{ku}^{(1)}$是在意图$k$中从$u$的邻居中收集信息。$N_{u}$是用户$u$的一阶邻居。右上角的$(1)$表示一阶邻居。</p>
<h4>Iterative update rule</h4>

<p>在这之后，如图三所示，邻居路由机制被采用：首先，我们根据意向图使用嵌入传播机制更新意向嵌入；然后，反过来，我们利用更新的嵌入来细化图并输出意图上的分布。特别地，我们设置T个迭代来实现这种迭代更新。在每次迭代$t$中，$S_{k}^{t}$和分别$u_{k}^{t}$记住邻接矩阵和嵌入的更新值，这里$t\in \left \{ 1,2,…,T \right \}$并且$T$是最后一次迭代。通过等式5和等式6初始化$S_{k}^{0}=S_{k}$、$u_{k}^{0}=u_{k}$。</p>
<p><img src= "img/loading.gif" data-src="https://img-blog.csdnimg.cn/20201027211226553.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDg4NDg1NA==,size_16,color_FFFFFF,t_70" alt="img"></p>
<h4>Cross-intent embedding propagation</h4>

<p>在第$t$次迭代，对于目标交互$(u,i)$，我们有评分向量，叫做$\left \{ S_{K}|\forall k\in \left \{ 1,\cdot \cdot \cdot ,K \right \} \right \}$。为了获得其在所有意图上的分布，我们然后通过softmax函数将这些系数归一化：</p>
<p><img src= "img/loading.gif" data-src="https://img-blog.csdnimg.cn/20201027212151498.png" alt="img"></p>
<p>这能够说明哪些意图应引起更多注意，以解释每个用户的行为$(u,i)$。结果，我们可以对每个意图$k$获得归一化的邻接矩阵$\tilde{S}_{k}^{t}$。然后在每个图上使用嵌入传播机制，这些被用户意图k影响的信息就被编码进表示中了。更正式地说，加权和聚合器定义为：</p>
<p><img src= "img/loading.gif" data-src="https://img-blog.csdnimg.cn/20201027212903697.png" alt="img"></p>
<p>在迭代$t$之后，这里的$u_{k}^{t}$是$u$的展示表示来记住邻居们$N_{u}=\left \{ i|(u,i)\in G \right \}$发出的信号。$i_{k}^{0}$是历史物品$i$的输入表示；并且$L_{k}^{t}(u,i)$是$\tilde{S}_{k}^{t}$的拉普拉斯矩阵，制定为：</p>
<p><img src= "img/loading.gif" data-src="https://img-blog.csdnimg.cn/20201027213519108.png" alt="img"></p>
<p><img src= "img/loading.gif" data-src="https://img-blog.csdnimg.cn/20201027213553751.png" alt="img"></p>
<p>这样的归一化可以处理变化的节点邻居数，从而使训练过程更加稳定。</p>
<p>值得注意的是，我们将初始的分块表示形式$\left \{ i_{k}^{0} \right \}$汇总为用户$i$提炼信息。这仅包含来自一阶连通性的信号，而排除了来自用户$u$本人和她的较高跃点邻居的信号。受到近期的SGC和LightGCN的启发，我们知道非线性变换对于CF任务来说是累赘并且它的黑匣子性质阻碍了分离过程，因此省略了转换，只使用ID嵌入。</p>
<h4>Intent-aware graph update</h4>

<p>我们基于用户（或项目）节点的邻居来迭代地调整边缘强度。使用公式9检查以用户节点$u$为根的子图结构，$u_{k}^{t}$可以看作是本地池$N_{u}=\left \{ (u,i) \right \}$中的质心，这包括了物品$u$过去的交互。直观地讲，由相同意图驱动的历史项往往具有相似的分块表示，这进一步鼓励了它们之间的关系更加牢固。我们因此迭代的更新${S}_{k}^{t}(u,i)$，更确切地说，调整质心$u$与相邻质心之间的强度：</p>
<p><img src= "img/loading.gif" data-src="https://img-blog.csdnimg.cn/20201027215027431.png" alt="img"></p>
<p>这里<img src= "img/loading.gif" data-src="https://img-blog.csdnimg.cn/20201027215048464.png" alt="img">考虑了$u_{k}^{t}$和$i_{k}^{0}$之间的密切关系。并且tanh是一个非线性激活函数来增强模型的表达能力。</p>
<p>在T次迭代之后，我们最后获得了一个图分离层的输出，它包含分离表示$e_{ku}^{(1)}=u_{k}^{T}$,同时还有意向图$A_{k}^{(1)}=\tilde{A}_{k}^{T},\forall k\in \left \{ 1,\cdot \cdot \cdot ,K \right \}$。当向前进行这种传播时，我们的模型会汇总与每个意图有关的信息并生成注意流，这可以看作是解开纠缠的解释。</p>
<font  color='red'><b>简单来说，就是通过嵌入图更新嵌入，嵌入再更新图，循环往复。</b></font>

<h3>Layer combination</h3>

<p>上面已经使用了一阶邻居，我们未来可以堆叠多层图分解层来从高阶邻居中获取影响信号。公式为：</p>
<p><img src= "img/loading.gif" data-src="https://img-blog.csdnimg.cn/20201028144042585.png" alt="img"></p>
<p>这里的$e_{ku}^{(l-1)}$和是$e_{ki}^{(l-1)}$用户$u$和物品$i$的表示，记录着从$(l-1)$阶邻居中聚集的信息。更多的是，每一个解纠缠表示都用它的解释图明确地表达了意图，权重邻接矩阵$A_{k}^{(l)}$。这样的解释图能够显示出什么信息构成了纠缠表示的合理证据。在$L$层之后，我们加和不同层的意向表示来当作最后一层。</p>
<p><img src= "img/loading.gif" data-src="https://img-blog.csdnimg.cn/20201028144553763.png" alt="img"></p>
<p>做了这个之后，我们不仅解开了CF的表示，还有了表示中每一部分的解释。值得强调的是训练参数只有第0层的用户$u$和物品$i$的嵌入表示。</p>
<h2>Independence modeling module</h2>

<p>更具【25，29】的建议，动态路由机制鼓励以不同意图为条件的分块的表示形式彼此不同。动态路由实施的差异约束不足：因子感知表征之间可能存在冗余。</p>
<p>因此，我们引入了另一个模块，该模块可以聘用诸如互信息和距离相关性之类的统计量作为正则化器，目的是鼓励因子感知表示独立。我们这里应用距离相关性，把相互信息作为未来的工作。尤其使，距离相关性能描绘两个向量之间的不同，不论是非线性还是线性关系；只有这些向量是独立的时候它的系数才为0。我们指定为：</p>
<p><img src= "img/loading.gif" data-src="https://img-blog.csdnimg.cn/20201028151727819.png" alt="img"></p>
<p>这里的$E_{k}=[e_{u_{1}k},…,e_{u_{N}k},e_{i_{1}k},…,e_{u_{N}k}]\in \mathbb{R}^{(M+N)\times \frac{d}{K}}$是用户$N=\left | U \right |$和$M=\left | I \right |$的嵌入查询表，它建立在所有用户和物品的意向表示中，图中$dCor(\cdot )$是一个距离关系方程定义为：</p>
<p><img src= "img/loading.gif" data-src="https://img-blog.csdnimg.cn/20201028161303314.png" alt="img"></p>
<p>这里的$dCor(\cdot )$代表着各个矩阵的距离斜方差，$dVar(\cdot )$代表着各个矩阵之间的距离方差。</p>
<h2>Model optimization</h2>

<p><img src= "img/loading.gif" data-src="https://img-blog.csdnimg.cn/20201028161536802.png" alt="img"></p>
]]></content>
      <categories>
        <category>explainable recommendation</category>
      </categories>
      <tags>
        <tag>recommendation</tag>
        <tag>explainable recommendation</tag>
      </tags>
  </entry>
  <entry>
    <title>Survey of SRSs</title>
    <url>/2020/11/09/Survey-of-SRSs/</url>
    <content><![CDATA[<h3>Title:</h3>

<p>《Sequential Recommender Systems Challenges Progress and Prospects》IJCAI 2019.12</p>
<h3>Detials</h3>

<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/11/25/A465EueHM8Fk1cZ.png" alt="1.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/11/25/e3FkoCcOrpYwsJK.png" alt="2.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/11/25/7VxNwCm2QiunA4z.png" alt="3.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/11/25/8MoHiXlZwAxmTS2.png" alt="4.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/11/25/zSGWkqL2MOKwYXv.png" alt="5.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/11/25/c38L2QHgnGCWm5K.png" alt="6.png"><br>        <img src= "img/loading.gif" data-src="https://i.loli.net/2020/11/25/bHO6QNdwRL1rAUC.png" alt="7.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/11/25/LnUaePfN4VwzZ6p.png" alt="8.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/11/25/CzAZv5fmUFXtnM4.png" alt="9.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/11/25/Yjb7UxWQ9r1glsk.png" alt="10.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/11/25/nwxzc1pk4LXglA3.png" alt="11.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/11/25/8Q5hMlDjgc9xTom.png" alt="12.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/11/25/UoYkJyCGePnDS1E.png" alt="13.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/11/25/bEFQILiuNhtGJs9.png" alt="14.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/11/25/INfXtcJP3Qi1Vo7.png" alt="15.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/11/25/dpJY75fjUxGkueM.png" alt="16.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/11/25/cxpdyHQY2hXkS5i.png" alt="17.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/11/25/an3AzU9g2VjWeqK.png" alt="18.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/11/25/3GwhXQV1eitFbzy.png" alt="19.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/11/25/uInF7GtvUciDJde.png" alt="20.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/11/26/zcLktgZNe165yr8.png" alt="21.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/11/26/knd2yI6WcU8jGHo.png" alt="22.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/11/26/xcMmvatsprITyRJ.png" alt="23.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/11/26/qjSEXUsKC2vBnPO.png" alt="24.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/11/26/gcbnyqPeALXFp1H.png" alt="25.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/11/26/24BELhbHTeCJq6z.png" alt="26.png"> </p>
]]></content>
      <categories>
        <category>SRSs</category>
      </categories>
      <tags>
        <tag>Sequential recommender system</tag>
      </tags>
  </entry>
  <entry>
    <title>FM</title>
    <url>/2020/10/15/FM/</url>
    <content><![CDATA[<blockquote>
<h1 id="Factorization-Machine"><a href="#Factorization-Machine" class="headerlink" title="Factorization Machine"></a>Factorization Machine</h1></blockquote>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/10/15/7BrcTiX1P8nCQNb.jpg" alt="图像.jpeg"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/10/15/rwpJ5X8hKVu3zZI.jpg" alt="图像 2.jpeg"></p>
<h4>Code</h4>

<p><a href="https://github.com/daluzi/Code-for-DL-in-RS/blob/master/2/FM.py" target="_blank" rel="noopener">https://github.com/daluzi/Code-for-DL-in-RS/blob/master/2/FM.py</a></p>
<h4>Repositories</h4>

<p>libfm</p>
<p>libffm</p>
<p>xLearn</p>
<p>tffm</p>
<p>…</p>
]]></content>
      <categories>
        <category>recommendation</category>
        <category>FM</category>
      </categories>
      <tags>
        <tag>recommedation</tag>
        <tag>FM</tag>
      </tags>
  </entry>
  <entry>
    <title>Cross Entropy Loss</title>
    <url>/2020/10/15/Cross-Entropy-Loss/</url>
    <content><![CDATA[<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/10/15/JL4KDA1PTMluHQF.jpg" alt="1C417C49-4497-494A-9408-0BF5D1861826.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/10/15/Ku6UFdoNm7sXJ81.jpg" alt="035911ED-CA66-4559-A949-8D1920EEE2BF.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/10/15/D8H6a3KMcXV91ZW.jpg" alt="45D48AE1-D789-4478-BC76-29CC7B87FE13.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/10/15/IcwD3v5mS917ZiB.jpg" alt="75F4DC32-0084-4F9F-B0A8-D40616F5559C.png"></p>
<p>在信息论中，相对熵等价于两个概率分布的信息熵的差值，若其中一个概率分布为真实分布，另一个为理论（拟合）分布，则此时相对熵等于交叉熵与真实分布的信息熵之差，表示使用理论分布拟合真实分布时产生的信息损耗 。</p>
<h4>Reference</h4>

<p><a href="https://blog.csdn.net/b1055077005/article/details/100152102" target="_blank" rel="noopener">https://blog.csdn.net/b1055077005/article/details/100152102</a></p>
]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>Cross-entropy-loss</tag>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>Python</title>
    <url>/2020/09/02/Python/</url>
    <content><![CDATA[<h2>Python</h2>

<h4>一. 数据结构</h4>

<ul>
<li>python中的<code>list</code>、<code>tuple</code>、<code>dict</code>、<code>set</code>   </li>
<li><b>list    </b><ol>
<li>构造<code>list</code>，使用<font color='red'><code>[]</code></font>把<code>list</code>的所有元素都括起来就是一个列表，用变量<code>L</code>表示。其中的元素<font color='red'>不要求是同一种数据类型</font>。  </li>
<li>列表中元素是<font color='red'>有序</font>的。可以通过索引来访问。</li>
<li>访问<code>list</code>的元素，<code>L[0]</code>：表示列表中的第一个元素，<code>L[-1]</code>：表示列表中的最后一个元素。<code>L[起始索引:终止索引]</code>：表示访问列表中的多个元素，包含头，不包含尾。</li>
<li>列表是<font color='red'>可以被修改的</font>，包括添加元素，删除元素，替换元素。<ul>
<li>添加元素：<code>L.append(元素)</code>、<code>L.insert(元素插入到列表中的位置，元素)</code> </li>
<li>删除元素：<code>L.pop()&lt;==&gt;L.pop(-1)</code>，<code>L.pop(要删除元素在列表中的位置)</code>、<code>L.remove(要删除元素)</code>，删除列表中第一次匹配到的元素</li>
<li>替换元素：<code>L[要替换元素在列表中的位置]=新的元素值</code>  </li>
</ul>
</li>
<li>计算列表的长度：<code>len(L)</code></li>
<li>计算列表中某一个元素在列表中出现的次数：<code>L.count(要统计的元素)</code></li>
<li>列表拼接，用<code>+</code>，<code>L1=[1,2],L2=[3,4],L1+L2=[1,2,3,4]</code></li>
<li>列表中元素复制，用<em>，`L= [2,3],L </em> 3 = [2,3,2,3,2,3] `</li>
</ol>
</li>
</ul>
<ul>
<li><p><b>tuple   </b></p>
<ol>
<li>构造<code>tuple</code>，使用<font color='red'><code>()</code></font>把<code>tuple</code>的所有元素都括起来就是一个元组，用变量<code>T</code>表示。python规定只有单个元素的元组应表示成<code>（元素，）</code>，避   免歧义。其中的元素<font color='red'>不要求是同一种数据类型</font>。   </li>
<li>元组中元素是<font color='red'>有序</font>的。可以通过索引访问。</li>
<li>访问<code>tuple</code>的元素，<code>T[0]</code>：表示元组中的第一个元素，<code>T[-1]</code>：表示元组中的最后一个元素。   </li>
<li>元组是不可以被修改的，因此它没有<code>append()</code>、<code>insert()</code>、<code>pop()</code>等方法。   注意：元组中可以包含列表，如<code>T= （1，2，[4，5]）</code>，  <code>T[2] = [ ]</code>,错误  <code>T[2] [0] =5</code>,正确   </li>
</ol>
</li>
<li><p><b>dict   </b></p>
<ol>
<li>构造<code>dict</code>，使用<font color='red'><code>{}</code></font>把<code>dict</code>所有元素都括起来就是一个字典，用<code>D</code>表示。<code>D</code>中元素的形式是<code>key：value</code>的形式。其中的<code>key</code>或者<code>value</code>都<font color='red'>不要求是同一种数据类型</font>。   </li>
<li>字典中元素是<font color='red'>无序的。不可以通过索引访问</font>。</li>
<li>访问<code>dict</code>的元素，使用<code>D[key]</code>来查找对应的value。如果key不存在，则会报错。为避免报错：  <ul>
<li>访问前先判断 <code>if key in D：print(D[key] )</code></li>
<li>使用get()方法<code>print(D.get[key])</code>，key不存在会输出None</li>
</ul>
</li>
<li>字典是可以被修改的。<code>D[key] = 新的value</code>，如果key不存在，则会想字典中添加这一键值对。  注意：  字典的一个最明显的特点是查找速度快，无论字典中有多少个数据，查找的速度都是一样的，因为它是按照key来查找的。但是它占用的内存大。典型的以空间换时间的思想。  </li>
</ol>
</li>
<li><p><b>set</b></p>
<ol>
<li>构建<code>set</code>，<code>S = set(传入一个列表)</code>，<code>或者S = {元素1，元素2，....}</code>。其中的元素<font color='red'>不要求是同一种数据类型</font>。如：  <code>S= set（[1,2,3,3,4,5]）  print(S) #{1,2,3,4,5}</code>  注意：创建空集合只能使用<code>S = set()</code>，而不能使用<code>S = { }</code>，这是用来创建空字典的。  </li>
<li>集合中元素是<font color='red'>无序的。不可以通过索引来访问元素</font>。  </li>
<li>集合是可以被修改的。   </li>
<li>添加元素和删除元素。 <ul>
<li><code>S.add(元素)</code>。</li>
<li><code>S.remove(元素)</code>。删除之前要进行判断：<code>if 元素 in S： S.remove(元素)</code>，否则会报错。</li>
</ul>
</li>
</ol>
</li>
</ul>
<h4>二.open方法打开文件</h4>

<ol>
<li><code>b</code> 二进制模式   </li>
<li><code>r</code> 只读，指针将会放在文件的开头    </li>
<li><code>rb</code> 二进制只读，指针将会放在文件的开头   </li>
<li><code>r+</code> 读写，指针将会放在文件的开头    </li>
<li><code>rb+</code> 二进制读写，指针将会放在文件的开头    </li>
<li><code>w</code> 写入， 如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件   </li>
<li><code>wb</code> 二进制写入，如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。一般用于非文本文件如图片等    </li>
<li><code>w+</code> 读写，如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。 </li>
<li><code>wb+</code> 二进制读写，如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。一般用于非文本文件如图片等</li>
<li><code>a</code> 追加，如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入   </li>
<li><code>ab</code> 二进制追加，如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入    </li>
<li><code>a+</code> 读写，如果该文件已存在，文件指针将会放在文件的结尾。文件打开时会是追加模式。如果该文件不存在，创建新文件用于读写。    </li>
<li><code>ab+</code> 二进制读写，如果该文件已存在，文件指针将会放在文件的结尾。如果该文件不存在，创建新文件用于读写。</li>
</ol>
<h4>三.标识符</h4>

<ol>
<li>第一个字符必须是字母表中字母或下划线 <code>_</code> 。</li>
<li>标识符的其他的部分由字母、数字和下划线组成。</li>
<li>标识符对大小写敏感。</li>
<li>不可以是python中的关键字，如<code>False</code>、<code>True</code>、<code>None</code>、<code>class</code>等。   </li>
<li>注意：<code>self</code>不是python中的关键字。类中参数self也可以用其他名称命名，但是为了规范和便于读者理解，推荐使用<code>self</code>。</li>
</ol>
<h4>四.原始字符串标识符r</h4>

<p>Python 中字符串的前导 <code>r</code> 代表原始字符串标识符，该字符串中的特殊符号不会被转义，适用于正则表达式中繁杂的特殊符号表示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">"\\n"</span>)</span><br><span class="line">print(<span class="string">r"\n"</span>)</span><br></pre></td></tr></table></figure>
<p>注意前导标识符 <code>r</code>不会被输出，只起标记作用。</p>
<h4>五.复数</h4>

<ol>
<li>虚数不能单独存在，它们总是和一个值为<code>0.0</code>的实数部分一起来构成一个复数。</li>
<li>复数由实数部分和虚数部分构成</li>
<li>表示虚数的语法： <code>real+imagj</code></li>
<li>实数部分和虚数部分都是浮点数</li>
<li>虚数部分必须有后缀 <code>j</code> 或 <code>J</code></li>
<li><font color='red'>python2和python3都不支持复数比较大小。</font>

</li>
</ol>
<h4>六.拷贝、赋值</h4>

<p>以一个例子说明：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line">a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,[<span class="string">'a'</span>,<span class="string">'b'</span>]]</span><br><span class="line">b = a</span><br><span class="line">c = copy.copy(a)</span><br><span class="line">d = copy.deepcopy(a)</span><br><span class="line">a.append(<span class="number">5</span>)</span><br><span class="line">a[<span class="number">4</span>].append(<span class="string">'c'</span>)</span><br></pre></td></tr></table></figure>
<p>用图例说明下<code>a</code>这个<code>list</code>在电脑里实际的存储情况：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/08/24/GBmb29YSzDfNWau.jpg" alt="IMG_C2E327060C3C-1.jpeg" style="zoom: 67%;" /></p>
<p>首先看看<code>b</code>的情况，<code>b</code>实际上和<code>a</code>指向的是同一个值，就好比人的大名和小名，只是叫法不同，但还是同一个人。</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/08/24/XOSsrRLUaEPkoAz.jpg" alt="IMG_4800E953B5BB-1.jpeg" style="zoom: 67%;" /></p>
<p>接下来再看看<code>c</code>的情况，<code>c</code>的情况和<code>a.copy()</code>的情况是一样的，都是所谓的浅拷贝（浅复制），浅拷贝只会拷贝父对象，不会拷贝子对象，通俗的说就是只会拷贝到第二层。</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/08/24/f9eTgXdcsnFNZuW.jpg" alt="IMG_71969EEBDF15-1.jpeg" style="zoom:67%;" /></p>
<p>若父对象发生变化，<code>c</code>不会变化，因为它已经复制的所有父对象，假如子对象发生变化则<code>c</code>会变，比如<code>c[4]</code>和<code>a[4]</code>实际都是一个变量<code>list</code>，他们都指向子对象，若子对象发生变化，他们必然都变化，比如变成<code>[&quot;a&quot;,&quot;d]</code>，那它们指向的值就会变成<code>a</code>，<code>d</code>。</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/08/24/5sZJ9SCFjGANgDw.jpg" alt="IMG_53A30C2E2E9B-1.jpeg" style="zoom:67%;" /></p>
<p>再看看<code>d</code>的情况，这就是我们所说的深复制，不管<code>a</code>进行什么操作，都不会改变<code>d</code>了，他们已经指向不同的值。</p>
<h4>七.逻辑运算符</h4>

<ul>
<li><b><code>and</code>：</b><code>x and y</code>，<font color='red'>当表达式中所有值都为真，Python会选择第二个值作为结果</font>，有值为<code>False</code>时就返回<code>False</code>；</li>
<li><b><code>or</code>：</b><font color='red'>当表达式所有值都为真，Python会选择第一个值作为结果。</font>简单的记法就是看第一个值，第一个值为真，就返回第一个值，如果为假，再看第二个值。</li>
</ul>
<h4>八.函数返回值</h4>

<p>Python没有<code>Null</code>，return没有返回值时，自动返回<code>None</code>。</p>
<h4>九.命令</h4>

<p>python中主要存在四种命名方式：       </p>
<ol>
<li><code>object</code>  #公用方法     </li>
<li><code>_object</code>  #半保护，#被看作是“protect”，意思是只有类对象和子类对象自己能访问到这些变量，                      在模块或类外不可以使用，不能用<code>from module import *</code>导入。 </li>
<li><code>_ _ object</code>  #全私有，全保护。<code>_ _ object</code> 是为了避免与子类的方法名称冲突， 对于该标识符描述的方法，父类的方法不能轻易地被子类的方法覆盖，他们的名字实际上是   <code>_ _ classname _ _methodname</code>。私有成员“private”，意思是只有类对象自己能访问，连子类对象也不能访                              问到这个数据，不能用<code>from module import *</code>导入。     </li>
<li><code>_ _ object_ _</code>   #内建方法，用户不要这样定义</li>
</ol>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>第三章</title>
    <url>/2020/08/16/%E7%AC%AC%E4%B8%89%E7%AB%A0/</url>
    <content><![CDATA[<h1>一、深度学习推荐模型的演化关系图</h1>

<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/08/17/wEVPFOIKdDo1Rj6.jpg" alt="BBA75C44-476D-4786-B990-7EEE2FB4DF45.png" style="zoom:50%;" /></p>
<p>整体来说，主要是以<font color='red'>多层感知机(Multi-Layer Perceptron，MLP)</font>为核心，通过改变神经网络的结构来构建各异的模型，比如：</p>
<ul>
<li>改变神经网络的复杂程度</li>
<li>改变特征交叉方式</li>
<li>组合多种模型</li>
<li>FM模型的深度学习演化版本</li>
<li>注意力机制与推荐模型的结合</li>
<li>序列模型与推荐模型的结合</li>
<li>强化学习与推荐模型的结合</li>
<li>等</li>
</ul>
<h1>二、AutoRec——单隐层神经网络推荐模型</h1>

<p><em>2015 澳大利亚国立大学提出。</em></p>
<p>它将<font color='red'>自编码器（AutoEncoder）的思想和协同过滤</font>结合，提出了一种单隐层神经网络推荐模型。</p>
<p><b>原理：</b></p>
<p>​        利用协同过滤中的共现矩阵，完成物品向量或用户向量的自编码。再利用自编码的结果得到用户对物品的预估评分，进而进行推荐排序。</p>
<blockquote>
<p>自编码器：假设其数据向量为r，自编码器的作用是将向量r作为输入，通过自编码器后，得到的输出向量尽量接近其本身。</p>
<p>假设自编码器的重建函数为$ h(r;\theta) $，那么自编码器的目标函数为：</p>
<script type="math/tex; mode=display">min_\theta\sum_{r\in S}||r - h(r;\theta)||_2^2</script><p>其中，S是所有数据向量的集合。</p>
<p>一般来说，重建函数的参数数量远小于输入向量的维度数量，因此自编码器相当于完成了<b>数据压缩和降维</b>的工作。</p>
</blockquote>
<p>AutoRec使用单隐层神经网络的结构来解决构建重建函数的问题。模型结构图如下：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/08/23/wo71GfOnrbxZcI6.jpg" alt="84CCD42D-718C-4122-8F08-7A8DCD155589.png" style="zoom:50%;" /></p>
<p>网络的输入层是物品的评分向量<code>r</code>，输出层是一个多分类层。图中蓝色的神经元代表模型的<code>k</code>维单隐层，其中<code>k&lt;&lt;m</code>。图中的<code>V</code>和<code>W</code>分别代表输入层到隐层，以及隐层到输出层的参数矩阵。该模型结构代表的重建函数的具体形式如：</p>
<script type="math/tex; mode=display">h(r;\theta)=f(W·g(V_r+\mu)+b)</script><p>其中，<code>f(.)</code>，<code>g(.)</code>分别为输出层神经元和隐层神经元的激活函数。</p>
<p><b>为防止过拟合，在加入<code>L2</code>正则化后，</b>AutoRec目标函数的具体形式为：</p>
<script type="math/tex; mode=display">min_\theta\sum_{i=1}^n||r^{(i)} - h(r^{(i)};\theta)||_2^2+\lambda/2·(||W||_F^2+||V||_F^2)</script><p>模型的训练利用梯度反向传播即可完成。</p>
<h1>三、Deep Crossing模型</h1>

<p><em>2016年，微软提出Deep Crossing模型，一次深度学习架构在推荐系统中的完整应用。</em></p>
<p><b>应用场景</b>：微软搜索引擎Bing中的搜索广告推荐场景。</p>
<p><b>目标：</b>用户搜索关键词后，搜索引擎除了返回相关结果，还会返回与搜索词相关的广告，因此要尽可能地<font color='red'>增加搜索广告的点击率，准确地预测广告点击率。</font></p>
<p>该模型完整的解决了从<b>特征工程、稀疏向量稠密化、多层神经网络进行优化目标拟合</b>等一系列深度学习在推荐系统中的应用问题。</p>
<p>基于此，微软使用的特征分成了三类：</p>
<ul>
<li>可以被处理成one-hot或者multi-hot向量的<b>类别型特征</b>：用户搜索词（query）、广告关键词（keyword）、广告标题（title）、落地页（landing page）、匹配类型（match type）；</li>
<li><b>数值型特征</b>：点击率、预估点击率（click prediction）；</li>
<li><b>需要进一步处理的特征</b>：广告计划（campaign）、曝光样例（impression）、点击样例（click）等，由于这些是一个特征的组别，就要把这些具体的部分拆开来分别处理。</li>
</ul>
<p><b>解决的问题：</b></p>
<ul>
<li>离散类特征编码后过于稀疏，不利于直接输入神经网络进行训练，<font color='blue'>如何解决稀疏特征向量稠密化的问题</font>（Embedding层、Stacking层）；</li>
<li><font color='blue'>如何解决特征自动交叉组合的问题</font>（Multiple Residual Units层）</li>
<li><font color='blue'>如何在输出层中达成问题设定的优化目标</font>（Scoring层）</li>
</ul>
<p><b>网络结构如下：</b></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/08/25/Y2rRUmLtgyBz3ih.jpg" alt="4EB1E155-2663-49C3-AD1E-A07C00A48211.png" style="zoom:50%;" /></p>
<p>包括4层：Embedding层、stacking层、Multiple Residual Units层和Scoring层。各层作用如下：</p>
<ol>
<li>Embedding层：作用是将稀疏的类别型特征转换成稠密的Embedding向量，具体的策略包括Word2vec、Graph Embedding等；</li>
<li>Stacking层：作用是把不同的Embedding特征和数值型特征拼接在一起，形成新的包含全部特征的特征向量；</li>
<li>Multiple Residual Units层：作用是对特征向量各个维度进行充分的交叉组合。主要结构是多层感知机，该模型采用了<b>多层残差网络</b>作为MLP的具体实现。</li>
<li>Scoring层：作为输出层，作用是为了拟合优化目标存在的，对于CTR预估这类二分类问题，Scoring层往往使用逻辑回归模型；而对于图像分类等多分类问题，Scoring层往往采用softmax模型。</li>
</ol>
<blockquote>
<p><b>残差神经网络：</b></p>
<p>残差神经网络就是由残差单元组成的神经网络，具体结构如下：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/08/25/fQyPaNArvOxdo9T.jpg" alt="E7D0AD35-937A-4A65-B49E-F8D6CCBF8D26.png" style="zoom:33%;" /></p>
<p>特点：</p>
<ol>
<li>输入经过两层以ReLU为激活函数的全连接层后，生成输出向量；</li>
<li>输入可以通过一个短路通路直接与输出向量进行元素加操作，生成最终的输出向量</li>
</ol>
<p>在这样的结构下，残差单元中的两层ReLU网络其实拟合的是<font color='red'>输出和输入之间的残差</font>（$x^0-x^i$），这就是为什么要叫做残差神经网络。</p>
<p>残差神经网络的诞生主要是为了解决两个问题：</p>
<ul>
<li>神经网络是不是越深越好？对于传统的基于感知机的神经网络，当网络加深之后，往往存在过拟合现象，即网络越深，在测试集上的表现越差。而在残差网络中，由于有输入向量短路的存在，很多时候可以越过两层ReLU网络，减少过拟合现象的发生。</li>
<li>当神经网络足够深时，往往存在严重的梯度消失现象。（梯度消失现象是指在梯度反向传播过程中，越靠近输入端，梯度的幅度越小，参数收敛的速度越慢。）为了解决这个问题，残差单元使用了ReLU激活函数取代原来sigmoid激活函数。此外，输入向量短路相当于直接把梯度毫无变化地传递到下一层，这也使残差网络的收敛速度更快。</li>
</ul>
</blockquote>
<h1>四、NeuralCF模型</h1>

<p><em>2017年，新加坡国立大学提出基于深度学习的协同过滤模型NeuralCF。</em></p>
<p>下图为传统矩阵分解的网络化形式表示和NeuralCF模型的对比：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/08/25/EF7geXozukZxjhY.jpg" alt="944F9428-2FAE-4623-A4D9-1D7F4255B2E3.png" style="zoom:50%;" /></p>
<p>图的左半部分是传统MF的网络化形式表示，其中用户隐向量和物品隐向量都可以看作是Embedding层。可以看出，NeuralCF模型用“多层神经网络+输出层”的结构替代了矩阵分解模型中简单的内积操作。优点是：</p>
<ol>
<li>让用户向量和物品向量做更充分的交叉，得到更多有价值的特征组合信息；</li>
<li>引入更多的非线性特征让模型的表达能力更强。</li>
</ol>
<p><b>广义矩阵分解模型（Generalized Matrix Factorization）：</b>用任意的互操作形式代替用户和物品向量的互操作层。</p>
<p>基于此，该论文还提出了一种混合模型，整合了原始NeuralCF模型和以元素积为互操作的广义矩阵分解模型，如下：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/08/25/Xw4WhbKeuCJMVLY.jpg" alt="45B2031A-5ADC-4C51-95D2-27D2A3C38AB0.png" style="zoom:50%;" /></p>
<blockquote>
<p><b>softmax函数：</b></p>
<p>目前很多深度模型的输出层都使用softmax函数，<font color='red'>解决多分类问题的目标拟合问题。</font></p>
<p>数学形式：</p>
<script type="math/tex; mode=display">\sigma(X)_i=\frac{exp(x_i)}{\sum^n_{j=1}exp(x_i)},当i=1,...,n且X=[x_1,...,x_n]^T\in\mathbb{R}</script><p>可以看出，softmax函数解决了从一个原始的n维向量，向一个n维的概率分布映射问题。</p>
<p>在分类问题上，softmax函数往往和交叉熵（cross-entropy）损失函数一起使用：</p>
<script type="math/tex; mode=display">LOSS_{Cross Entropy}=-\sum_iy_iln(\sigma(x)_i)</script><p>其中$y_i$是第i个分类的真实标签值，$\sigma(x)_i$代表softmax函数对第i个分类的预测值。</p>
<p>因为softmax函数把分类输出标准化成了多个分类的概率分布，而交叉熵正好刻画了预测分类和真实结果之间的相似度，所以softmax函数往往与交叉熵搭配使用。</p>
<p>softmax函数的导数形式为：</p>
<script type="math/tex; mode=display">{\frac{\partial\sigma(x)_i}{\partial x_j}}=\begin{cases}
   \sigma(x)_i(1-\sigma(x)_j), & i=j \\
   -\sigma(x)_i·\sigma(x)_j & i\neq j
   \end{cases}</script><p>基于链式法则，交叉熵函数到softmax函数第j维输入$x_j$的导数形式为：</p>
<script type="math/tex; mode=display">\frac{\partial Loss}{\partial x_j}=\frac{\partial Loss}{\partial \sigma(x)}·\frac{\partial \sigma(x)}{\partial x_j}</script><p>在多分类问题中，真实值中只有一个维度是1，其余维度都为0，假设第k维是1，即$y_k=1$，那么交叉熵损失函数可以简化成如下形式：</p>
<script type="math/tex; mode=display">LOSS_{Cross Entropy}=-\sum_iy_iln(\sigma(x)_i)=-y_k·ln(\sigma(x)_k)=-ln(\sigma(x)_k)</script><p>则有：</p>
<script type="math/tex; mode=display">\frac{\partial Loss}{\partial x_j}=\frac{\partial(-ln(\sigma(x)_k))}{\partial\sigma(x)_k}·\frac{\partial\sigma(x)_k}{\partial x_j}=-\frac{1}{\sigma(x)_k}·\frac{\partial\sigma(x)_k}{\partial x_j}=\begin{cases}\sigma(x)_j-1, & j=k \\
   \sigma(x)_j, & j\neq k
   \end{cases}</script><p>即$j=k$时，结果为算出的值减一，$j\neq k$时，为算出来的值。</p>
<p>可以看出，softmax函数与交叉熵的配合，不仅在数学含义上完美统一，而且在梯度形式上也非常简洁。基于上式的梯度形式，通过梯度反向传播的方法，即可完成整个神经网路权重的更新。</p>
</blockquote>
<h1>五、PNN模型</h1>

<p><em>2016年，上海交大提出PNN模型，给出了特征交互方式的几种设计思路。</em></p>
<p>同样是解决CTR预估和推荐系统的问题，结构上，和Deep Crossing类似，唯一的区别就是<font color='red'>PNN模型使用乘积层（Product Layer）代替了Deep Crossing模型中的Stacking层。</font>因此，不同特征之间进行了两两交互，获取了更多的交叉信息。模型结构如下图：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/09/02/P2e1UAwvNXcDHKT.jpg" alt="938F6510-F504-42CC-AD81-897BF995258B.png" style="zoom:50%;" /></p>
<p><b>Product层：</b></p>
<p>由上图，模型的乘积层由<font color='red'>内积操作部分</font>和<font color='red'>外积操作部分</font>组成。</p>
<p>【内积】就是经典的向量内积运算，假设输入特征向量分别为$f_i,f_j$，特征的内积互操作$g_{inner}(f_i,f_j)$的定义为：</p>
<script type="math/tex; mode=display">g_{inner}(f_i,f_j)=\langle f_i,f_j \rangle</script><p>【外积】就是对输入特征向量$f_i,f_j$的各维度进行两两交叉，生成特征交叉矩阵，外积互操作$g_{inner}(f_i,f_j)$的定义为：</p>
<script type="math/tex; mode=display">g_{inner}(f_i,f_j)=f_i,f_j^T</script><p>外积互操作生成的是特征向量$f_i,f_j$各维度两两交叉而成的一个$M*M$的方阵。这样的外积操作无疑会直接将问题的复杂度从原来的$M$提升到$M^2$，为了在一定程度上减小模型训练的负担，PNN模型的论文中介绍了一种降维的方法，就是把所有两两特征Embedding向量外积互操作的结果叠加（Superposition），形成一个叠加外积互操作矩阵<code>p</code>，具体形式如：</p>
<script type="math/tex; mode=display">p=\sum_{i=1}^M\sum_{j=1}^Mg_{outer}(f_i,f_j)=\sum_{i=1}^M\sum_{j=1}^Mf_i,f_j^T=f_{\sum}f_{\sum}^T=\sum_{i=1}^Mf_i</script><p><b>优点和局限：</b>PNN的结构特点在于强调了特征EMbedding向量之间的交叉方式是多种多样的，有针对性的做不同特征之间的交叉，从而让模型更容易捕获特征的交叉信息。但同时，对所有特征进行无差别的交叉，也在一定程度上忽略了原始特征向量中包含的有价值信息。</p>
<h1>六、Wide&Deep模型</h1>

<p><em>2016年，谷歌应用商店（Google Play）推荐团队提出wide&amp;deep模型。该模型由单层的Wide部分和多层Deep部分组成混合模型，其中Wide部分的主要作用是让模型具有较强的“记忆能力”，Deep部分的主要作用是让模型具有“泛化能力”。</em></p>
<blockquote>
<p>【“记忆能力”】可以被理解为模型直接学习并利用历史数据中物品或者特征的“共现频率”的能力。</p>
<p>一般来说，协同过滤、逻辑回归等简单模型有较强的“记忆能力”。由于这类模型的结构简单，原始数据往往可以直接影响推荐结果，产生类似于“如果点击过A，就推荐B”这类规则式的推荐，这就相当于模型直接记住了历史数据的分布特点，并利用这些记忆进行推荐。</p>
<p>【“泛化能力”】可以被理解为模型传递特征的相关性，以及发掘稀疏甚至从来没有出现过的稀有特征与最终标签相关性的能力。</p>
</blockquote>
<p>模型如下：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/09/03/gvwxSn9KPu62OBb.jpg" alt="EB6B7812-AA1A-4C7F-A870-DBE0E7EA8732.png" style="zoom:50%;" /></p>
<font color='red'>Wide&Deep模型把单输入层的Wide部分与由Embedding层和多隐层组成的Deep部分连接起来，一起输入最终的输出层。单层的Wide部分善于处理大量稀疏的id类特征；Deep部分利用神经网络表达能力强的特点，进行深层的特征交叉，挖掘藏在特征背后的数据模式。最终，利用逻辑回归模型，输出层将Wide部分和Deep部分组合起来，形成统一的模型。</font>

<p>详细结构如下：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/09/03/QbYWcN6yuz3LH5e.jpg" alt="960CD5C1-64BD-45C9-BED3-045F6995F40C.png" style="zoom: 33%;" /></p>
<p>Wide部分组合“已安装应用”和“曝光应用”两个特征的函数被称为交叉积变换（Cross Product Transformation）函数，其形式化定义如：</p>
<script type="math/tex; mode=display">\Phi_k(X)=\prod_{i=1}^dx_i^{c_{ki}}  ,c_{ki}\in \lbrace 0,1 \rbrace</script><p>$c_{ki}$是一个布尔变量，当第<code>i</code>个特征属于第<code>k</code>个组合特征时，$c_{ki}$的值为1，否则为0；$x_i$是第<code>i</code>个特征的值。</p>
<p><b>【Deep&amp;Cross模型】</b></p>
<p><em>2017年，斯坦福大学和谷歌的研究人员提出的，简称DCN。</em></p>
<p>模型如下：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/09/03/akYKPXtIzxhrCZf.jpg" alt="6EBC6AEB-113C-4463-8AA7-762869E224B8.png" style="zoom:33%;" /></p>
<p>用Cross网络代替了Wide部分，目的是增加特征之间的交互力度，使用多层交叉层（Cross layer）对输入向量进行特征交叉。假设第<code>l</code>层交叉层的输出向量为$x_l$，那么第<code>l+1</code>层的输出向量如：</p>
<script type="math/tex; mode=display">x_{l+1}=x_0x_l^TW_l+b_l+x_l</script><p><b>Wide&amp;Deep优点：</b></p>
<ol>
<li>抓住了业务问题的本质特点，能够融合传统模型记忆能力和深度学习模型泛化能力的优势；</li>
<li>模型的结构并不复杂，比较容易在工程上实现、训练和上线，这加速了其在业界的推广应用。</li>
</ol>
<h1>七、FM与深度学习模型的结合</h1>

<h4>7.1 FNN----用FM的隐向量完成Embedding层初始化</h4>

<p><em>2016年，伦敦大学提出FNN。</em></p>
<p>模型结构如下：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/09/03/rNISYnPe94Z1HCv.jpg" alt="CF1662F1-DF76-47AE-8F13-D503813AA832.png" style="zoom: 33%;" /></p>
<p>在神经网络的参数初始化过程中，往往采用随机初始化这种不包含任何先验信息的初始化方法。由于Embedding层的输入极端稀疏化，导致Embedding层的收敛速度非常缓慢。再加上Embedding层的参数数量往往占整个神经网络参数数量的大半以上，因此<font color='red'>模型的收敛速度往往受限于Embedding层。</font></p>
<blockquote>
<p>【为什么Embedding层的收敛速度往往很慢】</p>
<p>Embedding层的作用是将稀疏输入向量转换成稠密向量，它速度慢主要有两个原因：</p>
<ol>
<li>Embedding层的参数数量巨大。比如，假设输入层的维度是100000，Embedding层输出维度是32，上层再加5层32维的全连接层，最后输出层维度是10，那么输出层到Embedding层的参数数量是32<em>100000=3200000，其余所有层的参数总数是（32 </em> 32）<em> 4 + 32 </em> 10 = 4416。那么Embedding层的权重总数占比是3200000/（3200000 + 4416）= 99.86%。</li>
<li>由于输入向量过于稀疏，在随机梯度下降的过程中，只有与非零特征相连的Embedding层权重会被更新，这进一步降低了Embedding层的收敛速度。</li>
</ol>
</blockquote>
<font color='red'>针对Embedding层收敛速度的难题，FNN模型的解决思路是有FM模型训练好的各特征隐向量初始化Embedding层的参数，相当于在初始化神经网络参数时，已经引入了有价值的先验信息。</font>

<p>下图展示FM各参数和FNN中Embedding层各参数的对应关系：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/09/03/P1ZCdnQRc9E6gx5.jpg" alt="C705F516-03DB-40E0-8E8C-9C7D81631C47.png" style="zoom:33%;" /></p>
<p>需要注意的是，虽然把FM中的参数指向了Embedding层各神经元，但其具体意义是初始化Embedding神经元与输入神经元之间的连接权重。假设FM隐向量的维度m，第i个特征域的第k维特征的隐向量是$v_{i,k}=(v_{i,k}^1,v_{i,k}^2,…,v_{i,k}^l,…,v_{i,k}^m)$，那么隐向量的第l维$v_{i,k}^l$就会成为连接输入神经元k和Embedding神经元l之间连接权重的初始值。</p>
<h4>7.2 DeepFM---用FM代替Wide部分</h4>

<p><em>2017年，哈工大和华为联合提出DeepFM模型。</em> </p>
<p>模型结构如下：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/09/03/PsFV9AqpnyK8TxD.jpg" alt="4D2082B5-8E18-4B15-94D9-EAE55075088D.png" style="zoom:33%;" /></p>
<p>DeepFM对Wide&amp;Deep模型的改进之处在于，它用FM替换了原来的Wide部分，加强了浅层网络部分特征组合的能力。如上图，左边的FM部分对不同特征的Embedding进行了两两交叉，也就是将Embedding向量当作原FM中的特征隐向量。</p>
<h4>7.3 NFM---FM的神经网络化尝试</h4>

<p><em>2017年，新加坡国立大学提出NFM模型。</em></p>
<p>原因：无论是FM，还是FFM，归根结底是一个二阶特征交叉的模型，受组合爆炸问题的困扰，FM几乎不可能扩展到三阶以上，这就限制了FM模型的表达能力。</p>
<p>在数学形式上，NFM模型的主要思路是用一个表达能力更强的函数代替FM中二阶隐向量内积的部分。</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/09/03/ZEjbMtYq93QrfpB.jpg" alt="985A7573-A2DF-4A03-A6D1-E943A776B4A5.png" style="zoom:33%;" /></p>
<p><span style="border-bottom:2px dashed red;">如果用传统机器学习的思路来设计NFM模型中的函数f(x)，那么势必会通过一系列的数据推导构造一个表达能力更强的函数。但进入深度学习时代后，由于深度学习网络理论上有拟合任何复杂函数的能力，f(x)的构造工作可以交由某个深度学习网络来完成，并通过梯度反向传播来学习。</span></p>
<p>NFM用以替代FM二阶部分的神经网络结构如下：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/09/03/6ZY2upkWqE5RBLy.jpg" alt="DA0E357C-7A07-4AF9-B2CD-5ADE2D4A4D9A.png" style="zoom:33%;" /></p>
<p>主要就是特征交叉池化层（Bi-interaction Pooling Layer）。假设$V_x$是所有特征域的Embedding集合，那么特征交叉池化层的具体操作为：</p>
<script type="math/tex; mode=display">f_{BI}(V_x)=\sum_{i=1}^{n}\sum_{j=i+1}^n(x_iv_i)\odot(x_jv_j)</script><p>其中，$\odot$代表两个向量的元素积操作。</p>
<h1>八、注意力机制在推荐模型中的应用</h1>

<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/09/03/fct2AOhCSaN5TgI.jpg" alt="9F98393C-6F49-49E4-ABD1-322746588D12.png" style="zoom:50%;" /></p>
<h4>8.1 AFM---引入注意力机制的FM</h4>

<p><em>2017年，浙江大学提出AFM。可以在做是NFM的延续。</em></p>
<p>在NFM中，不同于的特征Embedding向量经过特征交叉池化层的交叉，将各交叉特征向量进行“加和”，输入最后由多层神经网络组成的输出层。问题的关键在于加和池化（Sum Pooling）操作，它相当于“一视同仁”地对待所有交叉特征，不考虑不同特征对结果的影响程度，事实上消解了大量有价值的信息。</p>
<p>注意力机制，<font color='red'>基于假设——不同的交叉特征对于结果的影响程度不同。</font></p>
<p>因此，AFM模型引入注意力机制是通过在特征交叉层和最终的输出层之间加入注意力网络（Attention Net）实现的。结果图如下：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/09/03/NXlJPE4hOm9qd7p.jpg" alt="5EEAB5BA-CBD3-4CF6-AADF-E53BF86147C8.png" style="zoom: 50%;" /></p>
<p>与NFM一样，AFM的特征交叉过程同样采用了元素积操作，</p>
<script type="math/tex; mode=display">f_{PI}(\varepsilon)=\lbrace (v\odot v_j)x_ix_j \rbrace_{(i,j)\in\mathbb{R}_x}</script><p>AFM加入注意力得分后的池化过程如，</p>
<script type="math/tex; mode=display">f_{Att}(f_{PI}(\varepsilon))=\sum_{(i,j)\in\mathbb{R}}a_{ij}(v_i\odot v_j)x_ix_j</script><p>对注意力得分$a_{ij}$来说，最简单的方法就是用一个权重参数来表示，但<font color='red'>为了防止交叉特征数据稀疏问题带来的权重参数难以收敛，AFM模型使用了一个在两两特征交叉层（Pair-wise Interaction Layer）和池化层之间的注意力网络来生成注意力得分。</font></p>
<p>该注意力网络的结构是一个简单的单全连接层加softmax输出层的结构，数学形式如：</p>
<script type="math/tex; mode=display">a_{ij}^{\prime}=\mathbb{h}^TReLU(W(v_i\odot v_j)x_ix_j+b)</script><script type="math/tex; mode=display">a_{ij}=\frac{exp(a_{ij}^{\prime})}{\sum_{i,j\in\mathbb{R}_X}exp(a_{ij}^\prime)}</script><p>其中要学习的参数就是特征交叉层到注意力网络全连接层的权重矩阵<code>W</code>，偏置向量<code>b</code>，以及全连接层到softmax输出层的权重向量<code>h</code>。</p>
<h4>8.2 DIN---引入注意力机制的深度学习网络</h4>

<p><em>2018年，阿里提出DIN模型。</em></p>
<p>应用场景是阿里巴巴的电商广告推荐，模型的输入特征分为两部分：</p>
<ul>
<li>一部分是用户u的特征组；</li>
<li>另一部分是候选广告a的特征组。</li>
</ul>
<p>不论是用户还是广告，都含有两个非常重要的特征——商品id（good_id）和商铺（shop_id）。用户特征里的商品id是一个序列，代表用户曾经点击过的商品集合，商品id同理；而广告特征里的商品id和商铺id就是广告对应的商品id和商铺id。结构图如下：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/09/03/bOFo2NMtRg1drkE.jpg" alt="E1587F81-D5B9-4627-94C5-1FD65BB7E846.png" style="zoom:50%;" /></p>
<p>注意力的权重：利用候选商品和历史行为商品之间的相关性计算出一个权重，这个权重就代表了“注意力”的强弱，注意力部分的形式化表达如：</p>
<script type="math/tex; mode=display">V_u=f(V_a)=\sum_{i=1}^Nw_i\cdot V_i=\sum_{i=1}^Ng(V_i,V_a)\cdot V_i</script><p>其中，$V_u$是用户的Embedding向量，$V_a$是候选广告商品的Embedding向量，$V_i$是用户u的第i次行为的EMbedding向量（这里就是那次浏览的商品或店铺的Embedding向量）。注意力得分$g(V_i,V_j)$是使用一个注意力激活单元（activation unit）来生成注意力得分，结构就是上图右上角的激活单元模块。</p>
<h1>九、DIEN——序列模型与推荐系统的结合</h1>

<p><em>2019年，阿里巴巴提出DIN的演化版本：DIEN。</em></p>
<font color='red'>特定用户的历史行为都是一个随时间排序的序列，既然是时间相关的序列，九一定存在或深或浅的前后依赖关系。</font>

<p>序列信息的重要性在于：</p>
<ol>
<li>它加强了最近行为对下次行为预测的影响；</li>
<li>序列模型能够学习到购买<font color='red'>趋势</font>的信息。</li>
</ol>
<p>模型结构图：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/09/03/F6wZUVatCumJAiH.jpg" alt="70F492DD-701F-471B-AE0F-21A0953586CE.png" style="zoom: 67%;" /></p>
<p>模型仍是输入层+Embedding层+连接层+多层全连接神经网络+输出层的整体架构。</p>
<p>图中的彩色部分叫做兴趣进化网络，也是主要创新点，包括三层：</p>
<ol>
<li>行为序列层（Behavior Layer，浅绿色部分）：主要作用是把原始的id类行为序列转换成Embedding行为序列；</li>
<li>兴趣抽取层（Interest Extractor Layer，米黄色部分）：主要作用是通过模拟用户兴趣迁移过程，抽取用户兴趣；</li>
<li>兴趣进化层（Interest Evolving Layer，浅红色部分）：主要作用是通过在兴趣抽取层基础上加入注意力机制，模拟与当前目标广告相关的兴趣进化过程。</li>
</ol>
<h4>9.1 兴趣抽取层的结构</h4>

<p>兴趣抽取层的基本结构是GRU（Gated Recurrent Unit，门循环单元）网络。相比传统的序列模型RNN（Recurrent Neural Network，循环神经网络）和LSTM（Long Short-Term Memory，长短期记忆网络），<span style="border-bottom:2px dashed red;">GRU解决了RNN的梯度消失问题（Vanishing Gradients Problem）。与LSTM相比，GRU的参数数量更少，训练收敛速度更快。</span></p>
<p>每个GRU单元的具体形式由系列公式定义</p>
<script type="math/tex; mode=display">u_t=\sigma(W^ui_t+U^uh_{t-1}+b^u)</script><script type="math/tex; mode=display">r_t=\sigma(W^ri_t+U^rh_{t-1}+b^r)</script><script type="math/tex; mode=display">\widetilde{h_t{}}=tanh(W^hi_t+r_t\circ U^hh_{t-1}+b^h)</script><script type="math/tex; mode=display">h_t=(1-u_t)\circ h_{t-1}+u_t\circ \widetilde{h_t{}}</script><p>其中，$\sigma$是Sigmoid激活函数，$\circ$是元素积操作，$W^u,W^r,W^h,U^z,U^r,U^h$是6组需要学习的参数矩阵，$i_t$是输入状态向量，也就是行为序列层的各行为Embedding向量$b(t)$，$h_t$是GRU网络中第<code>t</code>个隐状态向量。</p>
<p>经过由GRU组成的兴趣抽取层后，用户的行为向量$b(t)$被进一步抽象化，形成了兴趣状态向量$h(t)$。</p>
<h4>9.2 兴趣进化层的结构</h4>

<p>这一层加入了注意力机制，目的是更有针对性地模拟与目标广告相关的兴趣进化路径。</p>
<p>注意力得分的生成过程和DIN完全一致，都是当前状态向量与目标广告向量进行互作用的结果，即考虑了与目标广告的相关性。</p>
<p>兴趣进化层完成注意力机制的引入是通过AUGRU（GRU with Attentional Update gate，基于注意力更新门的GRU）结构，在原GRU的更新门的结构上加入了注意力得分，具体形式为</p>
<script type="math/tex; mode=display">\widetilde{u}_t^\prime=a_t\cdot u_t^\prime</script><script type="math/tex; mode=display">h_t^\prime=(1-\widetilde{u}_t^\prime)\circ h_{t-1}^\prime+\widetilde{u}_t\circ \widetilde{h_t{}}</script><h1>十、强化学习与推荐系统的结合</h1>

<p>强化学习（Reinforcement Learning）是近年来机器学习领域非常热门的研究话题，它的研究起源于机器人领域，针对智能体在不断变化的环境中决策和学习的过程进行建模。</p>
<p>2018年，宾夕法尼亚大学和微软亚洲研究院提出DRN，将强化学习应用于新闻推荐系统。</p>
<h1>十一、总结</h1>

<div class="table-container">
<table>
<thead>
<tr>
<th>模型名称</th>
<th>基本原理</th>
<th>特点</th>
<th>局限性</th>
</tr>
</thead>
<tbody>
<tr>
<td>AutoRec</td>
<td>基于自编码器，对用户或物品进行编码，利用自编码器的泛化能力进行推荐</td>
<td>单隐层神经网络结构简单，可实现快速训练和部署</td>
<td>表达能力较差</td>
</tr>
<tr>
<td>Deep Crossing</td>
<td>利用“Embedding层+多隐层+输出层”的经典深度学习框架，预完成特征的自动深度交叉</td>
<td>经典的深度学习推荐模型框架</td>
<td>利用全连接隐层进行特征交叉，针对性不强</td>
</tr>
<tr>
<td>NeuralCF</td>
<td>将传统的矩阵分解中用户向量和物品向量的点积操作，换成由神经网络代替的互操作</td>
<td>表达能力加强版的矩阵分别模型</td>
<td>只使用了用户和物品的id特征，没有加入更多其他其他特征</td>
</tr>
<tr>
<td>PNN</td>
<td>针对不同特征域之间的交叉操作，定义“内积”“外积”等多种积操作</td>
<td>在经典深度学习框架上模型对提高特征交叉能力</td>
<td>“外积”操作进行了近似化，一定程度上影响了其表达能力</td>
</tr>
<tr>
<td>Wide&amp;Deep</td>
<td>利用Wide部分加强模型的“记忆能力”，利用Deep部分加强模型的“泛化能力”</td>
<td>开创了组合模型的构造方法，对深度学习推荐模型的后续发展产生重大影响</td>
<td>Wide部分需要人工进行特征组合的筛选</td>
</tr>
<tr>
<td>Deep&amp;Cross</td>
<td>用Cross网络替代Wide&amp;Deep模型中的Wide部分</td>
<td>解决了Wide&amp;Deep模型人工组合特征的问题</td>
<td>Cross网络的复杂度较高</td>
</tr>
<tr>
<td>FNN</td>
<td>利用FM的参数来初始化深度神经网络的Embedding层参数</td>
<td>利用FM初始化参数，加快了整个网络的收敛速度</td>
<td>模型的主结构比较简单，没有针对性的特征交叉层</td>
</tr>
<tr>
<td>DeepFM</td>
<td>在Wide&amp;Deep模型的基础上，用FM替代了原来的线性Wide部分</td>
<td>加强了Wide部分的特征交叉能力</td>
<td>与经典的Wide&amp;Deep模型相比，结构差别不明显</td>
</tr>
<tr>
<td>NFM</td>
<td>用神经网络代替了FM中二阶隐向量交叉的操作</td>
<td>相比FM，NFM的表达能力和特征交叉能力更强</td>
<td>与PNN模型的结构非常相似</td>
</tr>
<tr>
<td>AFM</td>
<td>在FM的基础上，在二阶隐向量交叉的基础上对每个交叉结果加入了注意力得分，并使用注意力网络学习注意力得分</td>
<td>不同交叉特征的重要性不同</td>
<td>注意力网络的训练过程比较复杂</td>
</tr>
<tr>
<td>DIN</td>
<td>在传统深度学习推荐模型的基础上引入注意力机制，并利用用户行为历史物品和目标广告物品的相关性计算注意力得分</td>
<td>根据目标广告物品的不同，进行更有针对性的推荐</td>
<td>并没有充分利用除“历史行为”以外的其他特征</td>
</tr>
<tr>
<td>DIEN</td>
<td>将序列模型与深度学习推荐系统模型结合，使用序列模型模拟用户的兴趣进化过程</td>
<td>序列模型增强了系统对用户兴趣变迁的表达能力，使推荐系统开始考虑时间相关的行为序列中包含的有价值信息</td>
<td>序列模型的训练复杂，线上服务的延迟较长，需要进行工程上的优化</td>
</tr>
<tr>
<td>DRN</td>
<td>将强化学习的思路应用于推荐系统，进行推荐模型的线上实时学习和更新</td>
<td>模型对数据实时性的利用能力大大加强</td>
<td>线上部分较复杂，工程实现难度较大</td>
</tr>
</tbody>
</table>
</div>
<hr>
<ul>
<li>[x] 《深度学习推荐系统第三章》</li>
</ul>
]]></content>
      <categories>
        <category>《深度学习推荐系统》</category>
      </categories>
      <tags>
        <tag>recommendation</tag>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker</title>
    <url>/2020/06/28/Docker/</url>
    <content><![CDATA[<h2 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h2><p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/06/28/hnm5d3vaqzFY9sJ.png" alt="docker.png" style="zoom: 80%;" /></p>
<h4 id="purpose："><a href="#purpose：" class="headerlink" title="purpose："></a>purpose：</h4><p>​    解决环境配置麻烦的问题。</p>
<h4 id="other-options"><a href="#other-options" class="headerlink" title="other options:"></a>other options:</h4><ul>
<li>虚拟机：资源占用多、冗余步骤多、启动慢。</li>
<li>Linux容器(LXC)：LXC不是模拟一个完整的操作系统，而是对进程进行隔离。即在正常进行的外面套了一个保护层，对于容器里面的进程来说，它接触到的各种资源都是虚拟的。从而实现与底层系统的隔离。启动快、资源占用少、体积小。</li>
</ul>
<hr>
<h4 id="Docker-install"><a href="#Docker-install" class="headerlink" title="Docker install"></a>Docker install</h4><p>​    属于LXC的一种封装，提供简单易用的容器使用接口，是目前最流行的LXC。</p>
<p>​    Docker 将应用程序与该程序的依赖，打包在一个文件里面。运行这个文件，就会生成一个虚拟容器。程序在这个虚拟容器里运行，就好像在真实的物理机上运行一样。有了 Docker，就不用担心环境问题。总体来说，Docker 的接口相当简单，用户可以方便地创建和使用容器，把自己的应用放入容器。容器还可以进行版本管理、复制、分享、修改，就像管理普通的代码一样。</p>
<p>​    Docker 需要用户具有 sudo 权限，为了避免每次命令都输入<code>sudo</code>，可以把用户加入 Docker 用户组（<a href="https://docs.docker.com/install/linux/linux-postinstall/#manage-docker-as-a-non-root-user" target="_blank" rel="noopener">官方文档</a>）。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ sudo usermod -aG docker <span class="variable">$USER</span></span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    Docker 是服务器——客户端架构。命令行运行<code>docker</code>命令的时候，需要本机有 Docker 服务。如果这项服务没有启动，可以用下面的命令启动（<a href="https://docs.docker.com/config/daemon/systemd/" target="_blank" rel="noopener">官方文档</a>）。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># service 命令的用法</span></span><br><span class="line">$ sudo service docker start</span><br><span class="line"></span><br><span class="line"><span class="comment"># systemctl 命令的用法</span></span><br><span class="line">$ sudo systemctl start docker</span><br></pre></td></tr></table></figure>
</blockquote>
<h4 id="image文件"><a href="#image文件" class="headerlink" title="image文件"></a>image文件</h4><p>​    <strong>Docker 把应用程序及其依赖，打包在 image 文件里面。</strong>只有通过这个文件，才能生成 Docker 容器。image 文件可以看作是容器的模板。Docker 根据 image 文件生成容器的实例。同一个 image 文件，可以生成多个同时运行的容器实例。</p>
<p>​    image 是二进制文件。实际开发中，一个 image 文件往往通过继承另一个 image 文件，加上一些个性化设置而生成。举例来说，你可以在 Ubuntu 的 image 基础上，往里面加入 Apache 服务器，形成你的 image。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 列出本机的所有 image 文件。</span></span><br><span class="line">$ docker image ls</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除 image 文件</span></span><br><span class="line">$ docker image rm [imageName]</span><br></pre></td></tr></table></figure>
<p>​    image 文件是通用的，一台机器的 image 文件拷贝到另一台机器，照样可以使用。一般来说，为了节省时间，我们应该尽量使用别人制作好的 image 文件，而不是自己制作。即使要定制，也应该基于别人的 image 文件进行加工，而不是从零开始制作。</p>
<p>​    为了方便共享，image 文件制作完成后，可以上传到网上的仓库。Docker 的官方仓库 <a href="https://hub.docker.com/" target="_blank" rel="noopener">Docker Hub</a> 是最重要、最常用的 image 仓库。此外，出售自己制作的 image 文件也是可以的。</p>
<h4 id="hello-world实例"><a href="#hello-world实例" class="headerlink" title="hello world实例"></a>hello world实例</h4><p>​    首先，运行下面的命令，将 image 文件从仓库抓取到本地。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker image pull library/hello-world</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    上面代码中，<code>docker image pull</code>是抓取 image 文件的命令。<code>library/hello-world</code>是 image 文件在仓库里面的位置，其中<code>library</code>是 image 文件所在的组，<code>hello-world</code>是 image 文件的名字。</p>
<p>​    由于 Docker 官方提供的 image 文件，都放在<a href="https://hub.docker.com/r/library/" target="_blank" rel="noopener"><code>library</code></a>组里面，所以它的是默认组，可以省略。因此，上面的命令可以写成下面这样。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker image pull hello-world</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    抓取成功以后，就可以在本机看到这个 image 文件了。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker image ls</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    现在，运行这个 image 文件。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker container run hello-world</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    <code>docker container run</code>命令会从 image 文件，生成一个正在运行的容器实例。</p>
<p>​    注意，<code>docker container run</code>命令具有自动抓取 image 文件的功能。如果发现本地没有指定的 image 文件，就会从仓库自动抓取。因此，前面的<code>docker image pull</code>命令并不是必需的步骤。</p>
<p>如果运行成功，你会在屏幕上读到下面的输出。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker container run hello-world</span><br><span class="line"></span><br><span class="line">Hello from Docker!</span><br><span class="line">This message shows that your installation appears to be working correctly.</span><br><span class="line"></span><br><span class="line">... ...</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    输出这段提示以后，<code>hello world</code>就会停止运行，容器自动终止。</p>
<p>​    有些容器不会自动终止，因为提供的是服务。比如，安装运行 Ubuntu 的 image，就可以在命令行体验 Ubuntu 系统。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker container run -it ubuntu bash</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    对于那些不会自动终止的容器，必须使用<a href="https://docs.docker.com/engine/reference/commandline/container_kill/" target="_blank" rel="noopener"><code>docker container kill</code></a> 命令手动终止。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker container <span class="built_in">kill</span> [containID]</span><br></pre></td></tr></table></figure>
</blockquote>
<h4 id="容器文件"><a href="#容器文件" class="headerlink" title="容器文件"></a>容器文件</h4><p>​    <strong>image 文件生成的容器实例，本身也是一个文件，称为容器文件。</strong>也就是说，一旦容器生成，就会同时存在两个文件： image 文件和容器文件。而且关闭容器并不会删除容器文件，只是容器停止运行而已。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 列出本机正在运行的容器</span></span><br><span class="line">$ docker container ls</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出本机所有容器，包括终止运行的容器</span></span><br><span class="line">$ docker container ls --all</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    上面命令的输出结果之中，包括容器的 ID。很多地方都需要提供这个 ID，比如上一节终止容器运行的<code>docker container kill</code>命令。</p>
<p>​    终止运行的容器文件，依然会占据硬盘空间，可以使用<a href="https://docs.docker.com/engine/reference/commandline/container_rm/" target="_blank" rel="noopener"><code>docker container rm</code></a>命令删除。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker container rm [containerID]</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    运行上面的命令之后，再使用<code>docker container ls --all</code>命令，就会发现被删除的容器文件已经消失了。</p>
<h4 id="Dockerfile文件"><a href="#Dockerfile文件" class="headerlink" title="Dockerfile文件"></a>Dockerfile文件</h4><p>​    学会使用 image 文件以后，接下来的问题就是，如何可以生成 image 文件？如果你要推广自己的软件，势必要自己制作 image 文件。</p>
<p>​    这就需要用到 Dockerfile 文件。它是一个文本文件，用来配置 image。Docker 根据 该文件生成二进制的 image 文件。</p>
<p>​    下面通过一个实例，演示如何编写 Dockerfile 文件。</p>
<h4 id="实例：制作自己的Docker容器"><a href="#实例：制作自己的Docker容器" class="headerlink" title="实例：制作自己的Docker容器"></a>实例：制作自己的Docker容器</h4><p>​    下面我以 <a href="http://www.ruanyifeng.com/blog/2017/08/koa.html" target="_blank" rel="noopener">koa-demos</a> 项目为例，介绍怎么写 Dockerfile 文件，实现让用户在 Docker 容器里面运行 Koa 框架。</p>
<p>​    作为准备工作，请先<a href="https://github.com/ruanyf/koa-demos/archive/master.zip" target="_blank" rel="noopener">下载源码</a>。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> https://github.com/ruanyf/koa-demos.git</span><br><span class="line">$ <span class="built_in">cd</span> koa-demos</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    <b>1 编写 Dockerfile 文件</b></p>
<p>​    首先，在项目的根目录下，新建一个文本文件<code>.dockerignore</code>，写入下面的<a href="https://github.com/ruanyf/koa-demos/blob/master/.dockerignore" target="_blank" rel="noopener">内容</a>。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">.git</span><br><span class="line">node_modules</span><br><span class="line">npm-debug.log</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    上面代码表示，这三个路径要排除，不要打包进入 image 文件。如果你没有路径要排除，这个文件可以不新建。</p>
<p>​    然后，在项目的根目录下，新建一个文本文件 Dockerfile，写入下面的<a href="https://github.com/ruanyf/koa-demos/blob/master/Dockerfile" target="_blank" rel="noopener">内容</a>。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">FROM node:8.4</span><br><span class="line">COPY . /app</span><br><span class="line">WORKDIR /app</span><br><span class="line">RUN npm install --registry=https://registry.npm.taobao.org</span><br><span class="line">EXPOSE 3000</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    上面代码一共五行，含义如下。</p>
<blockquote>
<ul>
<li><code>FROM node:8.4</code>：该 image 文件继承官方的 node image，冒号表示标签，这里标签是<code>8.4</code>，即8.4版本的 node。</li>
<li><code>COPY . /app</code>：将当前目录下的所有文件（除了<code>.dockerignore</code>排除的路径），都拷贝进入 image 文件的<code>/app</code>目录。</li>
<li><code>WORKDIR /app</code>：指定接下来的工作路径为<code>/app</code>。</li>
<li><code>RUN npm install</code>：在<code>/app</code>目录下，运行<code>npm install</code>命令安装依赖。注意，安装后所有的依赖，都将打包进入 image 文件。</li>
<li><code>EXPOSE 3000</code>：将容器 3000 端口暴露出来， 允许外部连接这个端口。</li>
</ul>
</blockquote>
<p>​    <b>10.2 创建 image 文件</b></p>
<p>​    有了 Dockerfile 文件以后，就可以使用<code>docker image build</code>命令创建 image 文件了。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker image build -t koa-demo .</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">$ docker image build -t koa-demo:0.0.1 .</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    上面代码中，<code>-t</code>参数用来指定 image 文件的名字，后面还可以用冒号指定标签。如果不指定，默认的标签就是<code>latest</code>。最后的那个点表示 Dockerfile 文件所在的路径，上例是当前路径，所以是一个点。</p>
<p>​    如果运行成功，就可以看到新生成的 image 文件<code>koa-demo</code>了。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker image ls</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    <b>10.3 生成容器</b></p>
<p>​    <code>docker container run</code>命令会从 image 文件生成容器。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker container run -p 8000:3000 -it koa-demo /bin/bash</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">$ docker container run -p 8000:3000 -it koa-demo:0.0.1 /bin/bash</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    上面命令的各个参数含义如下：</p>
<blockquote>
<ul>
<li><code>-p</code>参数：容器的 3000 端口映射到本机的 8000 端口。</li>
<li><code>-it</code>参数：容器的 Shell 映射到当前的 Shell，然后你在本机窗口输入的命令，就会传入容器。</li>
<li><code>koa-demo:0.0.1</code>：image 文件的名字（如果有标签，还需要提供标签，默认是 latest 标签）。</li>
<li><code>/bin/bash</code>：容器启动以后，内部第一个执行的命令。这里是启动 Bash，保证用户可以使用 Shell。</li>
</ul>
</blockquote>
<p>​    如果一切正常，运行上面的命令以后，就会返回一个命令行提示符。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@66d80f4aaf1e:/app<span class="comment">#</span></span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    这表示你已经在容器里面了，返回的提示符就是容器内部的 Shell 提示符。执行下面的命令。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@66d80f4aaf1e:/app<span class="comment"># node demos/01.js</span></span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    这时，Koa 框架已经运行起来了。打开本机的浏览器，访问 <a href="http://127.0.0.1:8000，网页显示&quot;Not" target="_blank" rel="noopener">http://127.0.0.1:8000，网页显示&quot;Not</a> Found”，这是因为这个 <a href="https://github.com/ruanyf/koa-demos/blob/master/demos/01.js" target="_blank" rel="noopener">demo</a> 没有写路由。</p>
<p>​    这个例子中，Node 进程运行在 Docker 容器的虚拟环境里面，进程接触到的文件系统和网络接口都是虚拟的，与本机的文件系统和网络接口是隔离的，因此需要定义容器与物理机的端口映射（map）。</p>
<p>​    现在，在容器的命令行，按下 Ctrl + c 停止 Node 进程，然后按下 Ctrl + d （或者输入 exit）退出容器。此外，也可以用<code>docker container kill</code>终止容器运行。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在本机的另一个终端窗口，查出容器的 ID</span></span><br><span class="line">$ docker container ls</span><br><span class="line"></span><br><span class="line"><span class="comment"># 停止指定的容器运行</span></span><br><span class="line">$ docker container <span class="built_in">kill</span> [containerID]</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    容器停止运行之后，并不会消失，用下面的命令删除容器文件。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查出容器的 ID</span></span><br><span class="line">$ docker container ls --all</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除指定的容器文件</span></span><br><span class="line">$ docker container rm [containerID]</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    也可以使用<code>docker container run</code>命令的<code>--rm</code>参数，在容器终止运行后自动删除容器文件。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker container run --rm -p 8000:3000 -it koa-demo /bin/bash</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    <b>10.4 CMD 命令</b></p>
<p>​    上一节的例子里面，容器启动以后，需要手动输入命令<code>node demos/01.js</code>。我们可以把这个命令写在 Dockerfile 里面，这样容器启动以后，这个命令就已经执行了，不用再手动输入了。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">FROM node:8.4</span><br><span class="line">COPY . /app</span><br><span class="line">WORKDIR /app</span><br><span class="line">RUN npm install --registry=https://registry.npm.taobao.org</span><br><span class="line">EXPOSE 3000</span><br><span class="line">CMD node demos/01.js</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    上面的 Dockerfile 里面，多了最后一行<code>CMD node demos/01.js</code>，它表示容器启动后自动执行<code>node demos/01.js</code>。</p>
<p>​    你可能会问，<code>RUN</code>命令与<code>CMD</code>命令的区别在哪里？简单说，<code>RUN</code>命令在 image 文件的构建阶段执行，执行结果都会打包进入 image 文件；<code>CMD</code>命令则是在容器启动后执行。另外，一个 Dockerfile 可以包含多个<code>RUN</code>命令，但是只能有一个<code>CMD</code>命令。</p>
<p>​    注意，指定了<code>CMD</code>命令以后，<code>docker container run</code>命令就不能附加命令了（比如前面的<code>/bin/bash</code>），否则它会覆盖<code>CMD</code>命令。现在，启动容器可以使用下面的命令。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker container run --rm -p 8000:3000 -it koa-demo:0.0.1</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    <b>10.5 发布 image 文件</b></p>
<p>​    容器运行成功后，就确认了 image 文件的有效性。这时，我们就可以考虑把 image 文件分享到网上，让其他人使用。</p>
<p>​    首先，去 <a href="https://hub.docker.com/" target="_blank" rel="noopener">hub.docker.com</a> 或 <a href="https://cloud.docker.com/" target="_blank" rel="noopener">cloud.docker.com</a> 注册一个账户。然后，用下面的命令登录。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker login</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    接着，为本地的 image 标注用户名和版本。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker image tag [imageName] [username]/[repository]:[tag]</span><br><span class="line"><span class="comment"># 实例</span></span><br><span class="line">$ docker image tag koa-demos:0.0.1 ruanyf/koa-demos:0.0.1</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    也可以不标注用户名，重新构建一下 image 文件。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker image build -t [username]/[repository]:[tag] .</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    最后，发布 image 文件。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker image push [username]/[repository]:[tag]</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    发布成功以后，登录 hub.docker.com，就可以看到已经发布的 image 文件。</p>
<h4 id="其他有用的命令"><a href="#其他有用的命令" class="headerlink" title="其他有用的命令"></a>其他有用的命令</h4><p>​    docker 的主要用法就是上面这些，此外还有几个命令，也非常有用。</p>
<p>​    <strong>（1）docker container start</strong></p>
<p>​    前面的<code>docker container run</code>命令是新建容器，每运行一次，就会新建一个容器。同样的命令运行两次，就会生成两个一模一样的容器文件。如果希望重复使用容器，就要使用<code>docker container start</code>命令，它用来启动已经生成、已经停止运行的容器文件。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker container start [containerID]</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    <strong>（2）docker container stop</strong></p>
<p>​    前面的<code>docker container kill</code>命令终止容器运行，相当于向容器里面的主进程发出 SIGKILL 信号。而<code>docker container stop</code>命令也是用来终止容器运行，相当于向容器里面的主进程发出 SIGTERM 信号，然后过一段时间再发出 SIGKILL 信号。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ bash container stop [containerID]</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    这两个信号的差别是，应用程序收到 SIGTERM 信号以后，可以自行进行收尾清理工作，但也可以不理会这个信号。如果收到 SIGKILL 信号，就会强行立即终止，那些正在进行中的操作会全部丢失。</p>
<p>​    <strong>（3）docker container logs</strong></p>
<p><code>docker container logs</code>命令用来查看 docker 容器的输出，即容器里面 Shell 的标准输出。如果<code>docker run</code>命令运行容器的时候，没有使用<code>-it</code>参数，就要用这个命令查看输出。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker container logs [containerID]</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    <strong>（4）docker container exec</strong></p>
<p>​    <code>docker container exec</code>命令用于进入一个正在运行的 docker 容器。如果<code>docker run</code>命令运行容器的时候，没有使用<code>-it</code>参数，就要用这个命令进入容器。一旦进入了容器，就可以在容器的 Shell 执行命令了。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker container <span class="built_in">exec</span> -it [containerID] /bin/bash</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    <strong>（5）docker container cp</strong></p>
<p>​    <code>docker container cp</code>命令用于从正在运行的 Docker 容器里面，将文件拷贝到本机。下面是拷贝到当前目录的写法。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker container cp [containID]:[/path/to/file] .</span><br></pre></td></tr></table></figure>
</blockquote>
<h2 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h2><p>​    Windows上安装了docker之后，netkeeper报如下错误：</p>
<p>​    <b>sorry,this application cannot be run under a Virtual Machine</b></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/06/29/bpL5eSUHEQGAOtT.png" alt="error.png" style="zoom:50%;" /></p>
<p>​    搜索了到如下几种参考方法(直接看第三种即可)：</p>
<p>​    1.开机时打开BIOS，关闭virtual tchnology这一选项,具体操作参考下面链接(<a href="http://www.udaxia.com/upqd/5254.html).但对于我没法尝试.(失败" target="_blank" rel="noopener">http://www.udaxia.com/upqd/5254.html).但对于我没法尝试.(失败</a>)<br>​    2.在控制面板→程序→启用或关闭 Windows 功能下,取消Hyper-V选项,重启电脑,参考下面链接（<a href="https://blog.csdn.net/sdut15110581043/article/details/53330481/).尝试无数遍,重启无数遍,均失败.(失败" target="_blank" rel="noopener">https://blog.csdn.net/sdut15110581043/article/details/53330481/).尝试无数遍,重启无数遍,均失败.(失败</a>)</p>
<p>​    <b>可忽略上面失败的解决方案</b><br>​    3.禁用Hyper-V正确姿势来了(第二种方法不行)<br>​    ①打开Windows PowerShell（管理员）</p>
<p>​    ②运行命令bcdedit /set hypervisorlaunchtype off<br>​    注意:上述命令有一个空格的存在</p>
<p>​    ③重启计算机</p>
<p>​    如果要重新开启Hyper-V，按照方法3执行bcdedit / set hypervisorlaunchtype auto 命令并重启计算机即可。</p>
<p>原文链接：<a href="https://blog.csdn.net/qq_43942195/article/details/88600624" target="_blank" rel="noopener">https://blog.csdn.net/qq_43942195/article/details/88600624</a></p>
<h4 id="reference"><a href="#reference" class="headerlink" title="reference:"></a>reference:</h4><ul>
<li><a href="https://docs.docker.com/" target="_blank" rel="noopener">Docker官网</a></li>
<li><a href="http://www.ruanyifeng.com/blog/2018/02/docker-tutorial.html" target="_blank" rel="noopener">阮一峰的Dockers入门</a></li>
</ul>
]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>第二章：推荐系统的进化之路</title>
    <url>/2020/06/27/%E7%AC%AC%E4%BA%8C%E7%AB%A0/</url>
    <content><![CDATA[<h2>一、整体框架</h2>

<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/07/11/vndB6QLsTbFJNGg.jpg" alt="1.png"></p>
<p>传统推荐系统的发展脉络主要由以下四个部分组成：</p>
<ul>
<li><b>协同过滤算法族：</b>物品协同过滤（ItemCF）、用户协同过滤（UserCF）、矩阵分解模型（Matrix Factorization）及各分支模型；</li>
<li><b>逻辑回归模型族：</b>逻辑回归能够利用和融合更多用户、物品和上下文特征。LR模型、LS-PLM模型等；</li>
<li><b>因子分解机模型族：</b>在传统逻辑回归的基础上，加入了二阶部分，使模型具备了进行<font color='red'>特征组合</font>的能力。FM模型、FFM模型；</li>
<li><b>组合模型：</b>为了融合多个模型的优点，将不同模型组合使用是构建推荐模型常用的方法。GBDT+LR等。</li>
</ul>
<h2>二、协同过滤</h2>

<p><em>协同过滤的提出：2003年Amazon发表文章<a href="https://ieeexplore.ieee.org/document/1167344" target="_blank" rel="noopener">Amazon.com recommendations: item-to-item collaborative filtering</a></em></p>
<h4 id="2-1-定义"><a href="#2-1-定义" class="headerlink" title="2.1 定义"></a>2.1 定义</h4><p>协同大家的反馈、评价和意见一起对海量的信息进行过滤，从中筛选出目标用户可能感兴趣的信息的推荐过程。</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/07/11/Xjb4Q8YEUoDknJ3.jpg" alt="366139DF-46C7-47B9-A03F-7237B62B0EEA.png"></p>
<p><b>共现矩阵：</b>以上图为例，将有向图转成矩阵的形式，用户作为矩阵行坐标，商品作为列坐标，将“点赞”和“踩”的用户行为数据转换为矩阵中对应的元素值。</p>
<h4 id="2-2-用户相似度计算"><a href="#2-2-用户相似度计算" class="headerlink" title="2.2 用户相似度计算"></a>2.2 用户相似度计算</h4><p>对于用户向量<code>i</code>和用户向量<code>j</code></p>
<ol>
<li>余弦相似度：$sim(i,j)=cos(i,j)=\frac{i·j}{||i||·||j||}$</li>
<li>皮尔逊相关系数：<br>$sim(i,j)=\frac{\Sigma_{p\in P}(R_{i,P} - \hat{R}_i)(R_{j,P}-\hat{R}_j)}{\sqrt[]{\Sigma_{p\in P}(R_{i,P}- \hat{R}_i)^2}\sqrt[]{\Sigma_{p\in P}(R_{j,P}- \hat{R}_j)^2}}$<br>其中，<code>R_{i,p}</code>代表用户<code>i</code>对物品<code>p</code>的评分，$\hat{R}_i$是代表用户<code>i</code>对所有物品的平均评分，<code>p</code>代表所有物品的集合。</li>
<li>基于皮尔逊系数的思路，还可以通过引入物品平均分的方式，<b>减少物品评分偏置对结果的影响:</b><br>$sim(i,j)=\frac{\Sigma_{p\in P}(R_{i,P} - \hat{R}_P)(R_{j,P}-\hat{R}_P)}{\sqrt[]{\Sigma_{p\in P}(R_{i,P}- \hat{R}_P)^2}\sqrt[]{\Sigma_{p\in P}(R_{j,P}- \hat{R}_P)^2}}$<br>其中，$\hat{R}_p$代表物品<code>p</code>得到所有评分的平均分。</li>
</ol>
<h4>2.3 最终结果的排序</h4>

<p>假设“目标用户与其相似用户的喜好是相似的”</p>
<p>最常用的方式是<b>利用用户相似度和相似用户的评价的加权平均获得目标用户的评分预测</b>，如下：</p>
<script type="math/tex; mode=display">R_{u,p}=\frac{\Sigma_{s\in S}(W_{u,s}·R_{s,p})}{\Sigma_{s\in S}W_{u,s}}</script><p>其中，权重$W_{u,s}$是用户<code>u</code>和用户<code>s</code>的相似度，$R_{s,P}$是用户<code>s</code>对物品<code>p</code>的评分。</p>
<h4>2.4 ItemCF</h4>

<p>ItemCF是基于物品相似度进行推荐的协同过滤算法。通过计算共现矩阵中物品列向量的相似度得到物品之间的相似矩阵，再找到用户的历史正反馈物品的相似物品进行进一步排序和推荐。</p>
<ol>
<li>基于历史数据，构建以用户（假设用户总数为m）为行坐标，物品（物品总数为n）为列坐标的<code>m * n</code>维的共现矩阵。</li>
<li>计算共现矩阵两两列向量间的相似性（相似度的计算方式与用户相似度的计算方式相同），构建<code>n * n</code>维的物品相似度矩阵。</li>
<li>获得用户历史行为数据中的正反馈物品列表。</li>
<li>利用物品相似度矩阵，针对目标用户历史行为中的正反馈物品，找出相似的Top k个物品，组成相似物品集合。</li>
<li>对相似物品集合中的物品，利用相似度分值进行排序，生成最终的推荐列表。</li>
</ol>
<h4 id="2-5-CF的缺点"><a href="#2-5-CF的缺点" class="headerlink" title="2.5 CF的缺点"></a>2.5 CF的缺点</h4><ul>
<li>CF的天然缺陷：推荐结果的头部效应较明显，处理稀疏向量的能力弱。</li>
<li>CF仅利用用户和物品的交互信息，无法有效地引入用户年龄、性别、商品描述、商品分类、当前时间等一系列用户特征、物品特征和上下文特征。</li>
</ul>
<h2>三、矩阵分解</h2>

<p>矩阵分解在CF中“共现矩阵”的基础上，加入了<b>隐向量</b>的概念，加强了模型处理稀疏矩阵的能力。</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/07/11/bFJ4WoyBGTvUDpg.jpg" alt="A9DA0BC6-B77A-485E-8A5D-DE44BE271712.png" style="zoom:50%;" /></p>
<h4>3.1矩阵分解的求解过程</h4>

<p>主要方法有三种：</p>
<ul>
<li>特征值分解（Eigen Decomposition）：只能作用于方阵</li>
<li>奇异值分解（singular Value Decomposition, SVD）：<img src= "img/loading.gif" data-src="https://i.loli.net/2020/07/11/Uzw2Id8ER1t3Prx.jpg" alt="6812192D-0640-4957-90E1-9A131EF5B6A9.png"></li>
<li>梯度下降（Gradient Descent）:<br>求解矩阵分解的目标函数：<br>$min_{q^<em>,p^</em>}\Sigma_{(u,i)\in K}(r_{u,i}-q_i^Tp_u)^2+\lambda(||q_i||+||p_u||)^2$</li>
</ul>
<p>在矩阵分解算法中，由于隐向量的存在，使人意的用户和物品之间都可以得到预测分值。而隐向量的生成过程其实是对共现矩阵进行全局拟合的过程，因此隐向量其实是利用全局信息生成的，有更强的泛化能力。</p>
<h4 id="3-2-MF优缺点"><a href="#3-2-MF优缺点" class="headerlink" title="3.2 MF优缺点"></a>3.2 MF优缺点</h4><ul>
<li><p>优点：</p>
<ol>
<li>泛化能力强</li>
<li>空间复杂度低：(n+m)·k</li>
<li>更好的扩展性和灵活性</li>
</ol>
</li>
<li><p>缺点：</p>
<ol>
<li>MF同样不方便加入用户、物品和上下文相关的特征</li>
<li>在缺乏用户历史行为时，无法进行有效的推荐</li>
</ol>
</li>
</ul>
<h2>四、逻辑回归</h2>

<p>逻辑回归将推荐问题看成一个<font color='red'>分类</font>问题，<b>通过预测正样本的概率对物品进行排序。</b></p>
<p>即为<font color='red'>点击率（Click Through Rate，CTR）</font>预测问题。</p>
<h4>4.1 流程：</h4>

<ol>
<li>将用户年龄、性别、物品属性、物品描述、当前时间、当前地点等特征转换成数值型特征向量；</li>
<li>确定逻辑回归模型的优化目标（以优化“点击率”为例），利用已有样本数据对逻辑回归模型进行训练，确定逻辑回归模型的内部参数；</li>
<li>在模型服务阶段，将特征向量输入逻辑回归模型，经过逻辑回归模型的推断，得到用户“点击”（这里用点击作为推荐系统正反馈行为的例子）物品的概率；</li>
<li>利用“点击”概率对所有候选物品进行排序，得到推荐列表。</li>
</ol>
<h4>4.2 数学形式</h4>

<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/08/01/JqcASsOK8b2Qjz3.jpg" alt="6FA14D81-4CD7-40A6-8302-2CCE421AA3F7.png" style="zoom:50%;" /></p>
<p>由上图所示，逻辑回归模型将特征向量$x=(x_1,x_2,…,x_n)$作为模型的输入，分别乘上相应的权重系数后再想加，最终，将得到的$x^Tw$输入到sigmoid函数中，使之映射到0-1的区间，得到最终的点击率，因此，最终的数学形式为：</p>
<script type="math/tex; mode=display">f(x)=\frac{1}{1+e^{-(w·x+b)}}</script><p>由数学形式可以看出，LR模型要确定的参数就是相应的权重向量w，常见的训练方法是<b>梯度下降法、牛顿法、拟牛顿法等。</b></p>
<h4>4.3 LR的优缺点</h4>

<p>优点：</p>
<ol>
<li>数学含义上的支撑（假设因变量y服从伯努利分布）；</li>
<li>可解释性强；</li>
<li>简单、直观、易用，工程化的需要。</li>
</ol>
<p>缺点：</p>
<ul>
<li>表达能力不强。</li>
</ul>
<h2>五、自动特征交叉的解决方案</h2>

<h4>5.1 “辛普森悖论”</h4>

<p>​    LR模型表达能力不强的问题，会不可避免地造成有效信息的损失。在仅利用单一特征而非交叉特征进行判断的情况下，有时不仅是信息损失的问题，甚至会得出错误的结论。</p>
<p>​    定义：<font color=#2196F3 face="宋体">在对样本集合进行分组研究时，在分组比较中都占优势的一方，在总评中有时反而是失势的一方，这种有悖常理的现象，被称为“辛普森悖论”。</font></p>
<p>比如，在视频应用点击中：</p>
<p>男性用户：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>视频</th>
<th>点击（次）</th>
<th>曝光（次）</th>
<th>点击率</th>
</tr>
</thead>
<tbody>
<tr>
<td>视频A</td>
<td>8</td>
<td>530</td>
<td>1.51%</td>
</tr>
<tr>
<td>视频B</td>
<td>51</td>
<td>1520</td>
<td>3.36%</td>
</tr>
</tbody>
</table>
</div>
<p>女性用户：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>视频</th>
<th>点击（次）</th>
<th>曝光（次）</th>
<th>点击率</th>
</tr>
</thead>
<tbody>
<tr>
<td>视频A</td>
<td>201</td>
<td>2510</td>
<td>8.01%</td>
</tr>
<tr>
<td>视频B</td>
<td>92</td>
<td>1010</td>
<td>9.11%</td>
</tr>
</tbody>
</table>
</div>
<p>从上面两表看，无论男性还是女性用户，对视频B的点击率都高于视频A，显然推荐系统应该优先考虑向用户推荐视频B。</p>
<p>但如果<b>忽略性别</b>这个维度，将数据汇总为下表：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>视频</th>
<th>点击（次）</th>
<th>曝光（次）</th>
<th>点击率</th>
</tr>
</thead>
<tbody>
<tr>
<td>视频A</td>
<td>209</td>
<td>3040</td>
<td>6.88%</td>
</tr>
<tr>
<td>视频B</td>
<td>143</td>
<td>2530</td>
<td>5.65%</td>
</tr>
</tbody>
</table>
</div>
<p>从汇总的表看，视频A的点击率居然比视频B高，如果据此进行推荐，将得出与之前的结果完全相反的结果，这就是“辛普森悖论”。</p>
<h4>5.2 POLY2模型</h4>

<p>数学形式：</p>
<p>$\phi POLY2(w,x)=\sum_{j_1=1}^n\sum_{j_2=j_1+1}^nw_{h(j_1,j_2)}x_{j_1}x_{j_2}$</p>
<p><em>$x_{j_1}$和$x_{j_2}$是两个特征，$w_{h(j_1,j_2)}$是赋予的对应特征组合的权重。</em></p>
<p><b>缺点：</b>输入数据本来就很稀疏，POLY2进行无选择的特征交叉会使特征向量更加稀疏，无法收敛；而且会使权重参数的数量从n直接上升到$n^2$。</p>
<h4>5.3 FM模型</h4>

<p><em>2016年，由Rendle提出</em></p>
<p>数学形式：</p>
<p>$\phi FM(w,x)=\sum_{j_1=1}^n\sum_{j_2=j_1+1}^n(w_{j_1}·w_{j_2})x_{j_1}x_{j_2}$</p>
<p>与POLY2不同，FM模型用<span style="border-bottom:2px dashed red;">两个向量的内积</span>/$(w_{j_1}·w_{j_2})$<span style="border-bottom:2px dashed red;">取代了单一的权重系数</span>$w_{h(j_1,j_2)}$。该模型会为每个特征学习一个隐权重向量。</p>
<p><b>优点：</b></p>
<ol>
<li>通过引入特征隐向量，直接把POLY2模型$n^2$级别的权重参数数量减少到nk(k为隐向量维度)；</li>
<li>隐向量的引入使FM模型能更好的解决数据稀疏性的问题。</li>
</ol>
<h4>5.4 FFM模型</h4>

<p><em>2015年，基于FM提出的FFM在多项CTR预测大赛中夺魁，并被Criteo、美团等公司深度应用在推荐系统、CTR预测等领域。</em></p>
<p>主要是引入了<font color=#2196F3 face="宋体">特征域感知（field-aware）</font>概念。</p>
<p>数学形式：</p>
<p>$\phi FM(w,x)=\sum_{j_1=1}^n\sum_{j_2=j_1+1}^n(w_{j_1，f_2}·w_{j_2,f_1})x_{j_1}x_{j_2}$</p>
<p><span style="border-bottom:2px dashed red;">与FM的区别在于w不同，这意味着每个特征对应的不是唯一一个隐向量，而是一组隐向量。</span></p>
<p><b>FFM模型只能做二阶的特征交叉，如果继续提高特征交叉的维度，会不可避免的产生组合爆炸和计算复杂度过高的问题。</b></p>
<p><b>做个小结：</b></p>
<ul>
<li><span style="border-bottom:2px dashed red;">POLY2直接学习每个交叉特征权重；</span></li>
<li><span style="border-bottom:2px dashed red;">FM学习特征的k维隐向量，交叉特征由对应向量的隐向量内积得到；</span></li>
<li><span style="border-bottom:2px dashed red;">FFM每个特征选择与对方域对应的隐向量做内积。</span></li>
</ul>
<h2>六、GBDT+LR</h2>

<p><em>2014年Facebook提出基于GBDT+LR（梯度下降树+逻辑回归）组合模型。</em></p>
<p>简而言之，就是利用GBDT自动进行特征筛选和组合，进而生成新的离散特征向量，再把该特征向量当作LR模型的输入，预估CTR的模型结构，图例如下：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/08/02/FcrzyjT7ACWRgMx.jpg" alt="D508E64D-9B22-459F-BE02-3F3248AF6D54.png" style="zoom: 33%;" /></p>
<p>（GBDT和LR两个过程是独立训练的）</p>
<h2>七、LS-PLM</h2>

<p><em>大规模分段线性模型（Large Scale Piece-wise Linear Model），阿里巴巴早在2012年就主流的推荐模型，2017年公之于众。</em></p>
<p><span style="border-bottom:2px dashed red;">LS-PLM，又称为MLR（Mixed Logistic Regression，混合逻辑回归），它在逻辑回归的基础上采用分而治之的思路，先对样本进行分片，再在样本分片中应用逻辑回归进行CTR预估。</span></p>
<p><b>主要是加入了聚类的思想</b></p>
<p>数学形式：</p>
<p>$f(x)=\sum_{i=1}^{m}\pi_i(x)·\eta_i(x)=\sum_{i=1}^{m}\frac{e^{\mu_i·x}}{\sum_{j=1}^me^{\mu_j·x}}·\frac{1}{1+e^{-w_i·x}}$</p>
<p><em>其中超参数“分片数”m可以较好地平衡模型的拟合与推广能力</em></p>
<p>思路是：首先用<span style="border-bottom:2px dashed red;">聚类函数</span>$\pi$对样本进行分类（这里$\pi$采用了softmax函数对样本进行多分类），再用LR模型计算样本在分片中具体的CTR，然后将二者相乘之后求和。</p>
<p>优势：</p>
<ol>
<li>端到端的非线性学习能力；</li>
<li>模型的稀疏性强：LS-PLM在建模时引入了L1和L2,1范数，可以使最终训练出来的模型具有较高的稀疏度，使模型的部署更加轻量级。</li>
</ol>
<h2>八、总结</h2>

<div class="table-container">
<table>
<thead>
<tr>
<th>模型名称</th>
<th>基本原理</th>
<th>特点</th>
<th>局限性</th>
</tr>
</thead>
<tbody>
<tr>
<td>协同过滤</td>
<td>根据用户的行为历史生成用户-物品共现矩阵，利用用户相似性和物品相似性进行推荐</td>
<td>原理简单、直接，应用广泛</td>
<td>泛化能力差，处理稀疏矩阵的能力差，推荐结果的头部效应较明显</td>
</tr>
<tr>
<td>矩阵分解</td>
<td>将协同过滤算法中的共现矩阵分解为用户矩阵和物品矩阵，利用用户隐向量和物品隐向量的内积进行排序并推荐</td>
<td>相较协同过滤，泛化能力有所加强，对稀疏矩阵的处理能力有所加强</td>
<td>除了用户历史行为数据，难以利用其他用户、物品特征及上下文特征</td>
</tr>
<tr>
<td>逻辑回归</td>
<td>将推荐问题转换成类似CTR预估的二分类问题，将用户、物品、上下文等不同特征转换成特征向量，输入逻辑回归模型得到CTR，再按照预估CTR进行排序并推荐</td>
<td>能够融合多种类型的不同特征</td>
<td>模型不具备特征组合的能力，表达能力较差</td>
</tr>
<tr>
<td>FM</td>
<td>在逻辑回归的基础上，在模型中加入二阶特征交叉部分，为每一维特征训练得到相应特征隐向量，通过隐向量间的内积运算得到交叉特征权重</td>
<td>相比逻辑回归，具备了二阶特征交叉能力，模型的表达能力增强</td>
<td>由于组合爆炸问题的限制，模型不易扩展到三阶特征交叉阶段</td>
</tr>
<tr>
<td>FFM</td>
<td>在FM模型的基础上，加入“特征域”的概念，使每个特征在与不同域的特征交叉时采用不同的隐向量</td>
<td>相比FM，进一步加强了特征交叉的能力</td>
<td>模型的训练开销达到了$0(n^2)$</td>
</tr>
<tr>
<td>GBDT+LR</td>
<td>利用GBDT进行“自动化”的特征组合，将原始特征向量转化成离散型特征向量，并输入逻辑回归模型，进行最终的CTR预估</td>
<td>特征工程模型化，使模型具备了更高阶特征组合的能力</td>
<td>GBDT无法进行完全并行的训练，更新所需的训练时长较长</td>
</tr>
<tr>
<td>LS-PLM</td>
<td>首先对样本进行“分片”，在每个“分片”内部构建逻辑回归模型，将每个样本的各“分片”概率与逻辑回归的得分进行加权平均，得到最终的预估值</td>
<td>模型结构类似三层神经网络，具备了较强的表达能力</td>
<td>模型结构相比深度学习模型仍较为简单，有进一步提高的空间</td>
</tr>
</tbody>
</table>
</div>
<hr>
<ul>
<li>[x] 《深度学习推荐系统第二章》</li>
</ul>
]]></content>
      <categories>
        <category>《深度学习推荐系统》</category>
      </categories>
      <tags>
        <tag>recommendation</tag>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title>第一章</title>
    <url>/2020/06/27/%E7%AC%AC%E4%B8%80%E7%AB%A0/</url>
    <content><![CDATA[<h2>第一章、互联网的增长引擎---推荐系统</h2>

<hr>
<h4>1.1 为什么推荐系统是互联网的增长引擎</h4>

<p>用户角度：推荐系统解决在“信息过载”的情况下，用户如何高效获得感兴趣信息的问题。从用户需求层面看，推荐系统是在用户需求并不十分明确的情况下，进行信息的过滤。更多的是利用用户的各类历史信息“猜测”其可能喜欢的内容。</p>
<p>公司角度：推荐系统解决产品能够最大限度地吸引用户、留存用户、增加用户黏性、提高用户转化率的问题。</p>
<h4>1.2 推荐系统的架构</h4>

<p>两个部分：</p>
<ul>
<li>“数据和信息”部分逐渐发展为推荐系统中融合了数据离线批处理、实时流处理的数据流框架；</li>
<li>“算法和模型”部分则进一步细化为推荐系统中集训练（training）、评估（evaluation）、部署（deployment）、线上推断（online inference）为一体的模型框架。<b>召回层、排序层、补充策略于算法层</b></li>
</ul>
]]></content>
      <categories>
        <category>《深度学习推荐系统》</category>
      </categories>
      <tags>
        <tag>recommendation</tag>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title>KG_learning</title>
    <url>/2020/06/22/KG-learning/</url>
    <content><![CDATA[<h3>知识图谱</h3>

<hr>
<p><em>以结构化的形式描述客观世界中的概念、实体及其关系，将互联网的信息表达成更接近人类认知世界的形式，提供了一种更好的组织、管理和理解互联网海量信息的能力。</em></p>
<p><b>Google</b>首先提出。使得谷歌的搜索结果能给出更精准的答案。</p>
<p>知识图谱推理，知识图谱可视化</p>
<p>机器学习、图数据库（如Neo4j）、自然语言等技术的成熟，使得知识图谱变火。</p>
<p>语义网络-&gt;本体论-&gt;web-&gt;语义网（大规模,也即知识图谱）</p>
<p>————————————-分割线———————————</p>
<p><b>典型的知识图谱：</b></p>
<p>按照知识的主客观性：事实性知识、主观性知识</p>
<p>根据知识的变化性质：静态知识、动态</p>
<p>根据领域知识：通用知识、领域知识</p>
<p>因此常见的知识图谱：</p>
<ul>
<li>基于专家知识/人工构建：1. Cyc；2.WordNet</li>
<li>基于众包数据和其他知识图谱：1.ConceptNet; 2.YAGO; 3.Wikidata; 4.BDpedia; 5.Freebase; 6.BabelNet</li>
<li>基于机器学习：1.NELL; 2.Knowledge Vault; 3. WOE; 4.ReVerb</li>
<li>企业知识图谱：1.Google KG; 2.百度知心; 3.搜狗知立方</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">graph LR</span><br><span class="line">A[常见的知识图谱] --&gt; B&#123;基于专家知识/人工构建&#125; </span><br><span class="line">A[常见的知识图谱] --&gt; B1&#123;基于众包数据和其他知识图谱&#125;</span><br><span class="line">A[常见的知识图谱] --&gt; B2&#123;基于机器学习&#125; </span><br><span class="line">A[常见的知识图谱] --&gt; B3&#123;企业知识图谱&#125; </span><br><span class="line">		B --&gt;  C[Cyc]</span><br><span class="line">		B --&gt;  D[WordNet]</span><br><span class="line">		B1 --&gt; C1[ConceptNet]</span><br><span class="line">		B1 --&gt; C2[YAGO]		</span><br><span class="line">		B1 --&gt; C3[Wikidata]		</span><br><span class="line">		B1 --&gt; C4[BDpedia]		</span><br><span class="line">		B1 --&gt; C5[Freebase]		</span><br><span class="line">		B1 --&gt; C6[BabelNet]</span><br><span class="line">    B2 --&gt; C7[NELL]</span><br><span class="line">    B2 --&gt; C8[Knowledge Vault]    </span><br><span class="line">    B2 --&gt; C9[WOE]    </span><br><span class="line">    B2 --&gt; C10[Reverb]</span><br><span class="line">    B3 --&gt; C11[Google KG]</span><br><span class="line">    B3 --&gt; C12[百度知心]</span><br><span class="line">    B3 --&gt; C13[搜狗知立方]</span><br></pre></td></tr></table></figure>
<p><b>知识图谱的主要技术</b></p>
<ul>
<li><p>知识问答</p>
</li>
<li><p>语义搜索</p>
</li>
<li><p>可视化</p>
</li>
<li><p>知识链接</p>
</li>
<li><p>知识推理：知识补全、自动问答系统</p>
</li>
<li><p>知识众包</p>
</li>
<li><p>知识融合：对不同来源、不同语言或结构的知识进行融合，从而对已有知识图谱进行补充、更新或去重。</p>
</li>
<li><p>知识抽取</p>
</li>
<li><p>知识表示</p>
</li>
<li><p>知识存储：研究采用何种方式将已有的知识图谱进行存储。基于图的数据结构，存储方式主要是两种形式：1，RDF格式存储—-Apache Jena；2，图数据库———Neo4j</p>
</li>
</ul>
<p>————————————-分割线———————————</p>
<p><b>知识表示和建模</b></p>
<p>一阶谓词逻辑、产生式系统、框架表示法、语义网络</p>
<p><em>（语义网络不等价于语义网，语义网络中的弧表示各种语义联系，指明它所连接的节点间某种语义关系。语义网络的推理规则不十分明了。<b>语义网络重在知识的表示，对实体间的关系等不太能表示，所以和知识图谱不同，大的语义网即为知识图谱</b>）</em></p>
<p>RDF（Resource Description Framework 资源描述框架）本质上是一组表示知识的语法，由若干个三元组的组合表达。</p>
<p>RDF序列化(即存储)：方式主要有：RDF/XML, N-Triples, Turtle, RDFa, JSON-LD等</p>
<p>RDF和RDFS</p>
<p>OWL（网络本体语言）：OWL Lite  OWL DL  OWL FULL</p>
<p>SPARQL语言、cypher QL语言</p>
<p>————————————-分割线———————————</p>
<p><b>知识图谱数据存储</b></p>
<p>方式：</p>
<ul>
<li><p>基于关系型数据库的存储方式</p>
</li>
<li><p>面向RDF的存储方式</p>
</li>
<li><p>图数据库存储方式</p>
</li>
</ul>
<p>————————————-分割线———————————</p>
<p><b>PageRank</b></p>
<p>一个网页i的重要度可以使用指向网页i的其他网页j的重要度加权得到。</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/06/22/iEIjbymsGqJtgCp.jpg" alt="98A3E36B-2A01-42C4-9435-C658C3CA5294.png" style="zoom:50%;" /></p>
<p><b>TextRank</b></p>
<p>将PageRank的“词”改成“句子”</p>
<p><b>BRAT</b></p>
<p><a href="http://brat.nlplab.org" target="_blank" rel="noopener">http://brat.nlplab.org</a></p>
<p>用于人工做实体标注的工具</p>
<p><b>word embedding</b></p>
<p><b>word2vec</b></p>
<p><b>Skipgram</b></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/06/22/iWensqMk6bSpEog.jpg" alt="skipgram.png"></p>
]]></content>
      <categories>
        <category>KG</category>
      </categories>
      <tags>
        <tag>KG</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode record</title>
    <url>/2020/06/01/leetcode-record/</url>
    <content><![CDATA[<h4 id="974-和可被K整除的子数组-中等"><a href="#974-和可被K整除的子数组-中等" class="headerlink" title="974.和可被K整除的子数组(中等)"></a>974.和可被K整除的子数组(中等)</h4><hr>
<p>给定一个整数数组 <code>A</code>，返回其中元素之和可被 <code>K</code> 整除的（连续、非空）子数组的数目。</p>
<p>示例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入：A = [4,5,0,-2,-3,1], K = 5</span><br><span class="line">输出：7</span><br><span class="line">解释：</span><br><span class="line">有 7 个子数组满足其元素之和可被 K = 5 整除：</span><br><span class="line">[4, 5, 0, -2, -3, 1], [5], [5, 0], [5, 0, -2, -3], [0], [0, -2, -3], [-2, -3]</span><br></pre></td></tr></table></figure>
<p>思路：</p>
<p> 通常，涉及连续子数组问题的时候，使用前缀和来解决。</p>
<p> 我们令 P[i]=A[0]+A[1]+…+A[i] 。那么每个连续子数组的和sum(i,j) 就可以写成P[j]−P<a href="0&lt;i&lt;j">i</a> 的形式。此时，判断子数组的和能否被K整除就等价于判断(P[j]−P[i−1])modK==0，根据同余定理，只要P[j]modK==P[i−1]modK就行。</p>
<p> 因此我们可以考虑对数组进行遍历，在遍历同时统计答案。当我们遍历到第 i个元素时，我们统计以 i 结尾的符合条件的子数组个数。我们可以维护一个以前缀和模 K 的值为键，出现次数为值的哈希表 record，在遍历的同时进行更新。这样在计算以 i结尾的符合条件的子数组个数时，根据上面的分析，答案即为[0..i−1] 中前缀和模K也为P[i]modK的位置个数，即record[P[i]modK]。</p>
<p> 最后的答案即为以每一个位置为数尾的符合条件的子数组个数之和。需要注意的一个边界条件是，我们需要对哈希表初始化，记录record[0]=1，这样就考虑了前缀和本身被 K 整除的情况。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def subarraysDivByK(self, A: List[int], K: int) -&gt; int:</span><br><span class="line">        record = &#123;0: 1&#125;</span><br><span class="line">        total, ans = 0, 0</span><br><span class="line">        for elem in A:</span><br><span class="line">            total += elem</span><br><span class="line">            modulus = total % K</span><br><span class="line">            same = record.get(modulus, 0)</span><br><span class="line">            ans += same</span><br><span class="line">            record[modulus] = same + 1</span><br><span class="line">        return ans</span><br></pre></td></tr></table></figure>
<p>复杂度分析</p>
<ul>
<li>时间复杂度：O(N)，其中 N 是数组 A 的长度。我们只需要从前往后遍历一次数组，在遍历数组的过程中，维护哈希表的各个操作均为 O(1)，因此总时间复杂度为 O(N)。</li>
<li>空间复杂度：O(min(N,K))，即哈希表需要的空间。当 N≤K 时，最多有N 个前缀和，因此哈希表中最多有N+1个键值对；当 N&gt;K 时，最多有K 个不同的余数，因此哈希表中最多有 K 个键值对。也就是说，哈希表需要的空间取决于N 和 K中的较小值。</li>
</ul>
<h4 id="394-字符串解码-中等"><a href="#394-字符串解码-中等" class="headerlink" title="394.字符串解码(中等)"></a>394.字符串解码(中等)</h4><hr>
<p>给定一个经过编码的字符串，返回它解码后的字符串。</p>
<p>编码规则为:k[encodedstring]，表示其中方括号内部的 encodedstring 正好重复 k 次。注意 k 保证为正整数。你可以认为输入字符串总是有效的；输入字符串中没有额外的空格，且输入的方括号总是符合格式要求的。此外，你可以认为原始数据不包含数字，所有的数字只表示重复的次数 k ，例如不会出现像 3a 或 2[4]的输入。</p>
<p>示例:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">s = &quot;3[a]2[bc]&quot;, 返回 &quot;aaabcbc&quot;.</span><br><span class="line">s = &quot;3[a2[c]]&quot;, 返回 &quot;accaccacc&quot;.</span><br><span class="line">s = &quot;2[abc]3[cd]ef&quot;, 返回 &quot;abcabccdcdcdef&quot;.</span><br></pre></td></tr></table></figure>
<p>用栈的思想：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def decodeString(self, s: str) -&gt; str:</span><br><span class="line">        stack = [[&apos;&apos;, 1]]</span><br><span class="line">        num = &apos;&apos;</span><br><span class="line">        for c in s:</span><br><span class="line">            if c.isdigit():</span><br><span class="line">                num += c</span><br><span class="line">            elif c == &apos;[&apos;:</span><br><span class="line">                stack.append([&apos;&apos;, int(num)])</span><br><span class="line">                num = &apos;&apos;</span><br><span class="line">            elif c == &apos;]&apos;:</span><br><span class="line">                subs, k = stack.pop()</span><br><span class="line">                stack[-1][0] += subs * k</span><br><span class="line">            else:</span><br><span class="line">                stack[-1][0] += c</span><br><span class="line">        return stack[0][0] * stack[0][1]</span><br></pre></td></tr></table></figure>
<h4 id="198-打家劫舍-中等"><a href="#198-打家劫舍-中等" class="headerlink" title="198.打家劫舍(中等)"></a>198.打家劫舍(中等)</h4><hr>
<p>你是一个专业的小偷，计划偷窃沿街的房屋。每间房内都藏有一定的现金，影响你偷窃的唯一制约因素就是相邻的房屋装有相互连通的防盗系统，如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警。给定一个代表每个房屋存放金额的非负整数数组，计算你 不触动警报装置的情况下 ，一夜之内能够偷窃到的最高金额。</p>
<p>示例 1:</p>
<p>输入: [1,2,3,1] 输出: 4 解释: 偷窃 1 号房屋 (金额 = 1) ，然后偷窃 3 号房屋 (金额 = 3)。 偷窃到的最高金额 = 1 + 3 = 4 。 示例 2:</p>
<p>输入: [2,7,9,3,1] 输出: 12 解释: 偷窃 1 号房屋 (金额 = 2), 偷窃 3 号房屋 (金额 = 9)，接着偷窃 5 号房屋 (金额 = 1)。 偷窃到的最高金额 = 2 + 9 + 1 = 12 。</p>
<p><strong>思路：</strong></p>
<p>动态规划+滚动数组：</p>
<p>首先考虑最简单的情况：如果只有一个房间，则偷窃该房屋，可以偷窃到最高总金额。如果只有两个房间，则由于两间房屋相邻，不能同时偷窃，只能偷窃其中的一间屋子，因此选择其中金额较高的房屋进行偷窃，可以偷窃到最高总金额。</p>
<p>如果房屋数量大于两间，应该如何计算能够偷窃到的最高总金额呢？对于第 k(k&gt;2) 间房屋，有两个选项：</p>
<p>偷窃第 k 间房屋，那么就不能偷窃第 k−1 间房屋，偷窃总金额为前 k−2 间房屋的最高总金额与第 k 间房屋的金额之和。</p>
<p>不偷窃第k 间房屋，偷窃总金额为前 k−1间房屋的最高总金额。</p>
<p>在两个选项中选择偷窃总金额较大的选项，该选项对应的偷窃总金额即为前 k 间房屋能偷窃到的最高总金额。</p>
<p>用 dp[i]表示前 i间房屋能偷窃到的最高总金额，那么就有如下的状态转移方程：</p>
<p>dp[i]=max(dp[i−2]+nums[i],dp[i−1])</p>
<p>边界条件为：</p>
<p>{dp[0]=nums[0]只有一间房屋，则偷窃该房屋 dp[1]=max(nums[0],nums[1])只有两间房屋，选择其中金额较高的房屋进行偷窃</p>
<p>只有一间房屋，则偷窃该房屋 只有两间房屋，选择其中金额较高的房屋进行偷窃 最终的答案即为 dp[n−1]，其中 n 是数组的长度。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def rob(self, nums: List[int]) -&gt; int:</span><br><span class="line">        if not nums:</span><br><span class="line">            return 0</span><br><span class="line"></span><br><span class="line">        size = len(nums)</span><br><span class="line">        if size == 1:</span><br><span class="line">            return nums[0]</span><br><span class="line">        </span><br><span class="line">        dp = [0] * size</span><br><span class="line">        dp[0] = nums[0]</span><br><span class="line">        dp[1] = max(nums[0], nums[1])</span><br><span class="line">        for i in range(2, size):</span><br><span class="line">            dp[i] = max(dp[i - 2] + nums[i], dp[i - 1])</span><br><span class="line">        </span><br><span class="line">        return dp[size - 1]</span><br></pre></td></tr></table></figure>
<p>上述方法使用了数组存储结果。考虑到每间房屋的最高总金额只和该房屋的前两间房屋的最高总金额相关，因此可以使用滚动数组，在每个时刻只需要存储前两间房屋的最高总金额。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def rob(self, nums: List[int]) -&gt; int:</span><br><span class="line">        if not nums:</span><br><span class="line">            return 0</span><br><span class="line"></span><br><span class="line">        size = len(nums)</span><br><span class="line">        if size == 1:</span><br><span class="line">            return nums[0]</span><br><span class="line">        </span><br><span class="line">        first, second = nums[0], max(nums[0], nums[1])</span><br><span class="line">        for i in range(2, size):</span><br><span class="line">            first, second = second, max(first + nums[i], second)</span><br><span class="line">        </span><br><span class="line">        return second</span><br></pre></td></tr></table></figure>
<p>复杂度分析</p>
<p>时间复杂度：O(n)，其中 n 是数组长度。只需要对数组遍历一次。</p>
<p>空间复杂度：O(1)。使用滚动数组，可以只存储前两间房屋的最高总金额，而不需要存储整个数组的结果，因此空间复杂度是 O(1)。</p>
<h4>1431.拥有最多糖果的孩子(简单)</h4>

<hr>
<p>给你一个数组 candies 和一个整数 extraCandies ，其中 candies[i] 代表第 i 个孩子拥有的糖果数目。</p>
<p>对每一个孩子，检查是否存在一种方案，将额外的 extraCandies 个糖果分配给孩子们之后，此孩子有 最多 的糖果。注意，允许有多个孩子同时拥有 最多 的糖果数目。</p>
<p><strong>示例 1：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入：candies = [2,3,5,1,3], extraCandies = 3</span><br><span class="line">输出：[true,true,true,false,true] </span><br><span class="line">解释：</span><br><span class="line">孩子 1 有 2 个糖果，如果他得到所有额外的糖果（3个），那么他总共有 5 个糖果，他将成为拥有最多糖果的孩子。</span><br><span class="line">孩子 2 有 3 个糖果，如果他得到至少 2 个额外糖果，那么他将成为拥有最多糖果的孩子。</span><br><span class="line">孩子 3 有 5 个糖果，他已经是拥有最多糖果的孩子。</span><br><span class="line">孩子 4 有 1 个糖果，即使他得到所有额外的糖果，他也只有 4 个糖果，无法成为拥有糖果最多的孩子。</span><br><span class="line">孩子 5 有 3 个糖果，如果他得到至少 2 个额外糖果，那么他将成为拥有最多糖果的孩子。</span><br></pre></td></tr></table></figure>
<p><strong>示例 2：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入：candies = [4,2,1,1,2], extraCandies = 1</span><br><span class="line">输出：[true,false,false,false,false] </span><br><span class="line">解释：只有 1 个额外糖果，所以不管额外糖果给谁，只有孩子 1 可以成为拥有糖果最多的孩子。</span><br></pre></td></tr></table></figure>
<p><strong>示例 3：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入：candies = [12,1,12], extraCandies = 10</span><br><span class="line">输出：[true,false,true]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">kidsWithCandies</span><span class="params">(self, candies: List[int], extraCandies: int)</span> -&gt; List[bool]:</span></span><br><span class="line">        m = max(candies)</span><br><span class="line">        a = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> candies:</span><br><span class="line">            <span class="keyword">if</span> (i + extraCandies) &gt;= m:</span><br><span class="line">                a.append(<span class="literal">True</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                a.append(<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> a</span><br></pre></td></tr></table></figure>
<h4>101.对称二叉树(简单)</h4>

<hr>
<p>给定一个二叉树，检查它是否是镜像对称的。</p>
<p>例如，二叉树 [1,2,2,3,4,4,3] 是对称的。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">    1</span><br><span class="line">   / \</span><br><span class="line">  2   2</span><br><span class="line"> / \ / \</span><br><span class="line">3  4 4  3</span><br></pre></td></tr></table></figure>
<p>但是下面这个 <code>[1,2,2,null,3,null,3]</code> 则不是镜像对称的:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">  1</span><br><span class="line"> / \</span><br><span class="line">2   2</span><br><span class="line"> \   \</span><br><span class="line"> 3    3</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isSymmetric</span><span class="params">(self, root: TreeNode)</span> -&gt; bool:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root: <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">return</span> self.isSTree(root.left,root.right)</span><br><span class="line">    <span class="comment">#     global r</span></span><br><span class="line">    <span class="comment">#     r =  []</span></span><br><span class="line">    <span class="comment">#     f = 0</span></span><br><span class="line">    <span class="comment">#     self.mid(root, f)</span></span><br><span class="line">    <span class="comment">#     print(r)</span></span><br><span class="line">    <span class="comment">#     if r == r[::-1]:</span></span><br><span class="line">    <span class="comment">#         return True</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># def mid(self, root, f):</span></span><br><span class="line">    <span class="comment">#     f += 1</span></span><br><span class="line">    <span class="comment">#     if root == None:</span></span><br><span class="line">    <span class="comment">#         r.append(f)</span></span><br><span class="line">    <span class="comment">#     else:</span></span><br><span class="line">    <span class="comment">#         self.mid(root.left, f)</span></span><br><span class="line">    <span class="comment">#         r.append(root.val)</span></span><br><span class="line">    <span class="comment">#         self.mid(root.right, f)</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isSTree</span><span class="params">(self,left,right)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> left <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">and</span> right <span class="keyword">is</span> <span class="literal">None</span>: <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">if</span> left <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> right <span class="keyword">is</span> <span class="literal">None</span>: <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">if</span> left.val != right.val: <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> self.isSTree(left.left,right.right) <span class="keyword">and</span> self.isSTree(left.right, right.left)</span><br></pre></td></tr></table></figure>
<p>我采用了中序遍历和递归两种方式，注释掉的部分为中序遍历。递归判断时想清楚递归的终止情况与返回值：（1）左右子树都为空 → True （2）左右子树一个为空 → False （3）左右子树都不空，但是值不相等 → False （4）若上述情况都不满足， 检查 左左&amp;右右， 左右&amp;右左。</p>
<h4>84.柱状图中最大的矩形(困难)</h4>

<hr>
<p>给定 <em>n</em> 个非负整数，用来表示柱状图中各个柱子的高度。每个柱子彼此相邻，且宽度为 1 。</p>
<p>求在该柱状图中，能够勾勒出来的矩形的最大面积。</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/06/01/gPviFIYRh2xmOjf.jpg" alt="1.png" style="zoom:50%;" /></p>
<p>以上是柱状图的示例，其中每个柱子的宽度为 1，给定的高度为 <code>[2,1,5,6,2,3]</code>。</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/06/01/whXgI2TFcaU7rNm.jpg" alt="2.png" style="zoom:50%;" /></p>
<p>图中阴影部分为所能勾勒出的最大矩形面积，其面积为 <code>10</code> 个单位。</p>
<p><strong>示例:</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入: [2,1,5,6,2,3]</span><br><span class="line">输出: 10</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">largestRectangleArea</span><span class="params">(self, heights: List[int])</span> -&gt; int:</span></span><br><span class="line">        n = len(heights)</span><br><span class="line">        left, right = [<span class="number">0</span>] * n, [<span class="number">0</span>] * n</span><br><span class="line"></span><br><span class="line">        mono_stack = list()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="keyword">while</span> mono_stack <span class="keyword">and</span> heights[mono_stack[<span class="number">-1</span>]] &gt;= heights[i]:</span><br><span class="line">                mono_stack.pop()</span><br><span class="line">            left[i] = mono_stack[<span class="number">-1</span>] <span class="keyword">if</span> mono_stack <span class="keyword">else</span> <span class="number">-1</span></span><br><span class="line">            mono_stack.append(i)</span><br><span class="line">        </span><br><span class="line">        mono_stack = list()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n - <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">            <span class="keyword">while</span> mono_stack <span class="keyword">and</span> heights[mono_stack[<span class="number">-1</span>]] &gt;= heights[i]:</span><br><span class="line">                mono_stack.pop()</span><br><span class="line">            right[i] = mono_stack[<span class="number">-1</span>] <span class="keyword">if</span> mono_stack <span class="keyword">else</span> n</span><br><span class="line">            mono_stack.append(i)</span><br><span class="line">        </span><br><span class="line">        ans = max((right[i] - left[i] - <span class="number">1</span>) * heights[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(n)) <span class="keyword">if</span> n &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
<p>采用的是单调栈。</p>
<h4>14.最长公共前缀(简单)</h4>

<hr>
<p>编写一个函数来查找字符串数组中的最长公共前缀。</p>
<p>如果不存在公共前缀，返回空字符串 <code>&quot;&quot;</code>。</p>
<p><strong>示例 1:</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入: [&quot;flower&quot;,&quot;flow&quot;,&quot;flight&quot;]</span><br><span class="line">输出: &quot;fl&quot;</span><br></pre></td></tr></table></figure>
<p><strong>示例 2:</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入: [&quot;dog&quot;,&quot;racecar&quot;,&quot;car&quot;]</span><br><span class="line">输出: &quot;&quot;</span><br><span class="line">解释: 输入不存在公共前缀。</span><br></pre></td></tr></table></figure>
<p>我写得很简陋，就是暴力，有点像官方给的横向扫描，但是不够简洁：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">longestCommonPrefix</span><span class="params">(self, strs: List[str])</span> -&gt; str:</span></span><br><span class="line">        length = len(strs)</span><br><span class="line">        <span class="keyword">if</span> length == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">''</span></span><br><span class="line">        <span class="keyword">if</span> length == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> strs[<span class="number">0</span>]</span><br><span class="line">        L = len(strs[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">if</span> L == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">''</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            R = <span class="string">''</span></span><br><span class="line">            R += self.cons(strs[<span class="number">0</span>], strs[<span class="number">1</span>])</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, length - <span class="number">1</span>):</span><br><span class="line">                R = self.cons(R, strs[i + <span class="number">1</span>])</span><br><span class="line">            <span class="keyword">return</span> R</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cons</span><span class="params">(self, a, b)</span>:</span></span><br><span class="line">        r = <span class="string">''</span></span><br><span class="line">        <span class="keyword">if</span> len(a) == <span class="number">0</span> <span class="keyword">or</span> len(b) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> r</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(a)):</span><br><span class="line">            <span class="keyword">if</span> i &gt; len(b) - <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">if</span> a[i] == b[i]:</span><br><span class="line">                r += a[i]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">return</span> r</span><br></pre></td></tr></table></figure>
<p>官方解析给了四种思路：</p>
<h4 id="方法一：横向扫描"><a href="#方法一：横向扫描" class="headerlink" title="方法一：横向扫描"></a>方法一：横向扫描</h4><p>用$LCP(S_1…S_n)$表示字符串$S_1…S_n$的最长公共前缀。可以得出以下结论：</p>
<p>​         <script type="math/tex">LCP(S_1...S_n) = LCP(LCP(LCP(S_1,S_2),S_3),...S_n)</script></p>
<p>基于该结论，可以得到一种查找字符串数组中的最长公共前缀的简单方法。依次遍历字符串数组中的每个字符串，对于每个遍历到的字符串，更新最长公共前缀，当遍历完所有的字符串以后，即可得到字符串数组中的最长公共前缀。如果在尚未遍历完所有的字符串时，最长公共前缀已经是空串，则最长公共前缀一定是空串，因此不需要继续遍历剩下的字符串，直接返回空串即可。</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/06/15/K5HnPa1dQFhOyBG.png" alt="1.png" style="zoom:50%;" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">longestCommonPrefix</span><span class="params">(self, strs: List[str])</span> -&gt; str:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> strs:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">""</span></span><br><span class="line">        </span><br><span class="line">        prefix, count = strs[<span class="number">0</span>], len(strs)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, count):</span><br><span class="line">            prefix = self.lcp(prefix, strs[i])</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> prefix:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> prefix</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">lcp</span><span class="params">(self, str1, str2)</span>:</span></span><br><span class="line">        length, index = min(len(str1), len(str2)), <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> index &lt; length <span class="keyword">and</span> str1[index] == str2[index]:</span><br><span class="line">            index += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> str1[:index]</span><br></pre></td></tr></table></figure>
<p>复杂度分析</p>
<ul>
<li><p>时间复杂度：O(mn)O(mn)，其中 mm 是字符串数组中的字符串的平均长度，nn 是字符串的数量。最坏情况下，字符串数组中的每个字符串的每个字符都会被比较一次。</p>
</li>
<li><p>空间复杂度：O(1)O(1)。使用的额外空间复杂度为常数。</p>
</li>
</ul>
<h4 id="方法二：纵向扫描"><a href="#方法二：纵向扫描" class="headerlink" title="方法二：纵向扫描"></a>方法二：纵向扫描</h4><p>另一种方法是纵向扫描。纵向扫描时，从前往后遍历所有字符串的每一列，比较相同列上的字符是否相同，如果相同则继续对下一列进行比较，如果不相同则当前列不再属于公共前缀，当前列之前的部分为最长公共前缀。</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/06/15/SvCD5j4XQV7HZ9u.png" alt="2.png" style="zoom:50%;" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">longestCommonPrefix</span><span class="params">(self, strs: List[str])</span> -&gt; str:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> strs:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">""</span></span><br><span class="line">        </span><br><span class="line">        length, count = len(strs[<span class="number">0</span>]), len(strs)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(length):</span><br><span class="line">            c = strs[<span class="number">0</span>][i]</span><br><span class="line">            <span class="keyword">if</span> any(i == len(strs[j]) <span class="keyword">or</span> strs[j][i] != c <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, count)):</span><br><span class="line">                <span class="keyword">return</span> strs[<span class="number">0</span>][:i]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> strs[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>复杂度分析</p>
<ul>
<li><p>时间复杂度：$O(mn)$，其中 $m$ 是字符串数组中的字符串的平均长度，$n$ 是字符串的数量。最坏情况下，字符串数组中的每个字符串的每个字符都会被比较一次。</p>
</li>
<li><p>空间复杂度：$O(1)$。使用的额外空间复杂度为常数。</p>
</li>
</ul>
<h4 id="方法三：分治"><a href="#方法三：分治" class="headerlink" title="方法三：分治"></a>方法三：分治</h4><p>注意到$LCP$的计算满足结合律，有以下结论：</p>
<p>​        <script type="math/tex">LCP(S_1...S_n) = LCP(LCP(S_1...S_K),LCP(S_{k+1}...S_N))</script></p>
<p>其中$LCP(S_1…S_n)$是字符串$S_1…S_n$的最长公共前缀，$1&lt;K&lt;n$。</p>
<p>基于上述结论，可以使用分治法得到字符串数组中的最长公共前缀。对于问题$LCP(S_i…S_j)$，可以分解成两个字问题$LCP(S_i…S_{mid})$与$LCP(S_{mid+1}…S_{j})$，其中$mid$值为二分之$i+j$。对于两个子问题分别求解，然后对两个子问题的解计算最长公共前缀，即为原问题的解。</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/06/15/ybQ3iH8joIAanxJ.png" alt="3.png" style="zoom:50%;" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">longestCommonPrefix</span><span class="params">(self, strs: List[str])</span> -&gt; str:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">lcp</span><span class="params">(start, end)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> start == end:</span><br><span class="line">                <span class="keyword">return</span> strs[start]</span><br><span class="line"></span><br><span class="line">            mid = (start + end) // <span class="number">2</span></span><br><span class="line">            lcpLeft, lcpRight = lcp(start, mid), lcp(mid + <span class="number">1</span>, end)</span><br><span class="line">            minLength = min(len(lcpLeft), len(lcpRight))</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(minLength):</span><br><span class="line">                <span class="keyword">if</span> lcpLeft[i] != lcpRight[i]:</span><br><span class="line">                    <span class="keyword">return</span> lcpLeft[:i]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> lcpLeft[:minLength]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span> <span class="keyword">if</span> <span class="keyword">not</span> strs <span class="keyword">else</span> lcp(<span class="number">0</span>, len(strs) - <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h4 id="方法四：二分查找"><a href="#方法四：二分查找" class="headerlink" title="方法四：二分查找"></a>方法四：二分查找</h4><p>显然，最长公共前缀的长度不会超过字符串数组中的最短字符串的长度。用 $minLength$ 表示字符串数组中的最短字符串的长度，则可以在 $[0,minLength] $的范围内通过二分查找得到最长公共前缀的长度。每次取查找范围的中间值 $mid$，判断每个字符串的长度为 $mid$ 的前缀是否相同，如果相同则最长公共前缀的长度一定大于或等于 $mid$，如果不相同则最长公共前缀的长度一定小于$mid$，通过上述方式将查找范围缩小一半，直到得到最长公共前缀的长度。</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/06/15/PQH8k5yO3gEAoVd.png" alt="4.png" style="zoom:50%;" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">longestCommonPrefix</span><span class="params">(self, strs: List[str])</span> -&gt; str:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">isCommonPrefix</span><span class="params">(length)</span>:</span></span><br><span class="line">            str0, count = strs[<span class="number">0</span>][:length], len(strs)</span><br><span class="line">            <span class="keyword">return</span> all(strs[i][:length] == str0 <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, count))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> strs:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">""</span></span><br><span class="line"></span><br><span class="line">        minLength = min(len(s) <span class="keyword">for</span> s <span class="keyword">in</span> strs)</span><br><span class="line">        low, high = <span class="number">0</span>, minLength</span><br><span class="line">        <span class="keyword">while</span> low &lt; high:</span><br><span class="line">            mid = (high - low + <span class="number">1</span>) // <span class="number">2</span> + low</span><br><span class="line">            <span class="keyword">if</span> isCommonPrefix(mid):</span><br><span class="line">                low = mid</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                high = mid - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> strs[<span class="number">0</span>][:low]</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>The FacT: Taming Latent Factor Models for Explainability with Factorization Trees</title>
    <url>/2020/05/23/The-FacT-Taming-Latent-Factor-Models-for-Explainability-with-Factorization-Trees/</url>
    <content><![CDATA[<blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;tao2019the,</span><br><span class="line">	title=&quot;The FacT: Taming Latent Factor Models for Explainability with Factorization Trees&quot;,</span><br><span class="line">	author=&quot;Yiyi &#123;Tao&#125; and Yiling &#123;Jia&#125; and Nan &#123;Wang&#125; and Aobo &#123;Yang&#125; and Hongning &#123;Wang&#125;&quot;,</span><br><span class="line">	booktitle=&quot;SIGIR 2019 : 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval&quot;,</span><br><span class="line">	year=&quot;2019&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p> <a href="https://doi.org/10.1145/3331184.3331244" target="_blank" rel="noopener">https://doi.org/10.1145/3331184.3331244</a></p>
</blockquote>
<h2>Abstract</h2>

<p>​        The FacT model aims at explaining latent factor based recommendation algorithms with rule-based explanations. It integrates <font color='red'>regression trees</font> to guide the learning of latent factor models for recommendation, and uses the learned tree structure to explain the resulting latent factors. With user-generated reviews, regression trees on users and items are built respectively, and each node on the trees are asscoiated with a latent profile to represent users and items. </p>
<h2>Methodology</h2>

<p><b>Latent factor learning</b></p>
<p>​        latent factor models have been widely deployed in modern recommender systems becuase its excellent recommendation quality. This work choices matrix factorization due to its simplicity.</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/05/24/fK8aYMkwTldIHUi.jpg" alt="1.jpeg" style="zoom:50%;" /></p>
<p>​        (1) for point wise rating prediction loss; (2) for pairwise ranking loss.</p>
<p><b>Explanation rule induction</b></p>
<p>​        It selects the predicates among the item features extracted from user-generated reviews, and each lexicon entry takes the form of <span style="border-bottom:2px dashed red;">(feature, opinion, sentiment polarity)</span>, abbreviated as (f, o ,s), and represents the <span style="border-bottom:2px dashed red;">sentiment polarity s inferred from an opinionated text phrase o describing feature f.</span></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/05/24/iXmpjxkQTIFnDzu.jpg" alt="2.jpeg" style="zoom:50%;" /></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/05/24/Z8JDPxUqAhgm9ej.jpg" alt="3.jpeg" style="zoom:50%;" /></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/05/24/pCk8Q4oLXSz1gZ9.jpg" alt="4.jpeg" style="zoom: 50%;" /></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/05/24/fACq4EtT3X82yHP.jpg" alt="IMG_1A6D7EF91195-1.jpeg"></p>
<h2>Evaluation</h2>

<ul>
<li>Normalized Discounted Cumulative Gain(NDCG)</li>
</ul>
<h2>Data</h2>

<ul>
<li>Amazon <a href="http://jmcauley.ucsd.edu/data/amazon/" target="_blank" rel="noopener">http://jmcauley.ucsd.edu/data/amazon/</a></li>
<li>Yelp dataset challenge <a href="https://www.yelp.com/dataset" target="_blank" rel="noopener">https://www.yelp.com/dataset</a></li>
</ul>
<h2>Code</h2>

<ul>
<li><a href="https://github.com/yilingjia/TheFacT" target="_blank" rel="noopener">https://github.com/yilingjia/TheFacT</a></li>
</ul>
<h2>Reference</h2>

<ul>
<li><a href="https://blog.csdn.net/qq_38871942/article/details/104696210" target="_blank" rel="noopener">https://blog.csdn.net/qq_38871942/article/details/104696210</a></li>
<li><a href="https://www.cnblogs.com/pinard/p/9128682.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/9128682.html</a></li>
</ul>
]]></content>
      <categories>
        <category>recommendation</category>
        <category>explainable recommendation</category>
      </categories>
      <tags>
        <tag>recommendation</tag>
        <tag>explainability</tag>
        <tag>regression tree</tag>
      </tags>
  </entry>
  <entry>
    <title>Heterogeneous Graph Neural Network</title>
    <url>/2020/05/20/Heterogeneous%20Graph%20Neural%20Network/</url>
    <content><![CDATA[<blockquote>
<blockquote>
<p>Zhang C , Song D , Huang C , et al. Heterogeneous Graph Neural Network[C]// the 25th ACM SIGKDD International Conference. ACM, 2019.</p>
</blockquote>
</blockquote>
<h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><h5 id="idea-of-the-article-HetGNN"><a href="#idea-of-the-article-HetGNN" class="headerlink" title="idea of the article(HetGNN)"></a>idea of the article(HetGNN)</h5><blockquote>
<p>This paper models the heterogeneous graph network and gets each nodes’ vector representation.<br>purpose: learn how to represent the vectors of each node in a heterogeneous graph.(embedding)</p>
</blockquote>
<p>First, <font color='red'>a reboot-based random walk strategy is used to select neighbors for each node according to the node type</font>.</p>
<p>Then, two modules are used to aggregate the characteristics of neighbor nodes:</p>
<ul>
<li>Generate feature vectors by modeling <font color='red'>the features of different types of nodes.</font></li>
<li><font color='red'>Aggregate different types of neighbor nodes</font>, and assign different weights to different types of nodes by fusing attention mechanism to obtain the final vector representation.</li>
</ul>
<p>Finally, establish loss function, use mini-batch gradient descent. </p>
<p><em>the learnted vector can be used to <b>link prediction, recommendation, nodes classification, cluster, etc..</b></em></p>
<hr>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/05/21/TFq79ogQsh1xCm3.jpg" alt="HGNN_challenge.jpeg"></p>
<hr>
<h3>HetGNN模型结构</h3>



<p>针对上文提到的异构网络面临的三个挑战，HetGNN分成了三个部分：</p>
<ul>
<li><p>邻居采样策略：Sampling Heterogeneous Neighbors（挑战一）</p>
</li>
<li><p>特征编码：Encoding Heterogeneous Contents（挑战二）</p>
</li>
<li><p>聚合邻居：Aggregating Heterogeneous Neighbors（挑战三）</p>
</li>
</ul>
<p>最后，根据目标函数进行优化，进行训练和预测</p>
<h5>Sampling Heterogeneous Neighbors(挑战一)</h5>

<p><em>(挑战一：对异构图如何采样？)</em></p>
<p>在异构图中，直接采样邻居面临的问题：</p>
<ol>
<li><p><font color='red'>不能捕捉到不同类型邻居的信息。</font>比如说，在下图的作者-论文-会议的图中，作者之间并不相连，作者会议也不相连，但他们之间的关系不可被忽视。</p>
<p><div align=center><img src= "img/loading.gif" data-src="https://i.loli.net/2020/05/21/kLxdo3AM9jJPlVc.png" alt="HetG_example.png" style="zoom: 50%;"/></p>
</li>
<li><p><font color='red'>邻居数量的影响</font>。有的作者写了很多篇论文，有的作者写的少。有的商品被很多人访问，有的商品无人问津，冷启动问题不能很好的表示。</p>
</li>
<li><font color='red'>节点的特征类型不同（如图像，文字等），不能直接聚合。</font>

</li>
</ol>
<p>针对上述问题，本文采用一种<font color='red'>random walk with restart(RWR)</font>方法进行采样，主要有两步：</p>
<ol>
<li><p>从节点v<span style="border-bottom:2px dashed red;">随机游走采样</span>，采样固定长度，每次以概率p访问邻居节点或返回初始节点，每种类型节点采样数固定，确保每类节点都会被采样到。</p>
</li>
<li><p>对不同类型的邻居分组，不同类型的邻居，根据采样频率返回前k个</p>
</li>
</ol>
<p>上述采样方法中：</p>
<ol>
<li><p>对于每种类型的节点都采样到了</p>
</li>
<li><p>每种类型节点数量相同，并且高频邻居被选择</p>
</li>
<li><p>同种类型的邻居放在了一起，邻居信息可以聚合</p>
</li>
</ol>
<h5>Encoding Heterogeneous Contents(挑战二)</h5>

<p><em>(挑战二：对不同类型的特征怎样进行编码？)</em></p>
<p>同一个节点，也往往有多种类型的特征，如图像，文字等，<span style="border-bottom:2px dashed red;">文章提出先对这一类特征进行预训练，如类别特征直接利用one-hot，文本特征利用par2vec，图像特征利用CNN，训练得到每类特征的向量表示后，利用Bi-LSTM进行编码后聚合</span>。模型架构如图所示：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/05/22/u9vmF5WgNJ7PUjy.jpg" alt="embedding.png" style="zoom:50%;" /></p>
<blockquote>
<p>类别型特征（categorical feature）主要是指年龄，职业，血型等在有限类别内取值的特征。它的原始输入通常是字符串形式，除了决策树族的算法能直接接受类别型特征作为输入，对于支持向量机，逻辑回归等模型来说，必须对其做一定的处理，转换成可靠的数值特征才能正确运行。一般的处理方式就是<a href="https://zhuanlan.zhihu.com/p/88921408" target="_blank" rel="noopener">one-hot encoding</a></p>
<p><a href="https://blog.csdn.net/hero_fantao/article/details/69659457?locationNum=10&amp;fps=1" target="_blank" rel="noopener">par2vec</a>: 是根据word2vec产生的，专门针对paragraph vector。</p>
</blockquote>
<p>在数学表达式上，节点v的向量表示$f{_1}(v)$为：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/05/22/TcMxmGuFK9ZkR2r.jpg" alt="f1_v_.jpeg" style="zoom:50%;" /></p>
<h5>Aggregating Heterogeneous Neighbors(挑战三)</h5>

<p><em>(挑战三：对不同类型的节点如何聚合？)</em></p>
<p>在上一部分，得到了每个节点的特征表示，在聚合上面临两个问题：对同样类型的不同节点怎样聚合？对不同类型的节点怎样聚合？分两步解决这两个问题：</p>
<p>Same type neighbors aggregation:</p>
<p>在采样中，我们对不同类型的节点进行采样，<font color='red'>通过上一步得到了每个节点的特征，这里，需要对同一类型的节点特征进行聚合，此处仍然采用Bi-LSTM方法</font>：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/05/22/PAKxtBSkiIfaoJj.jpg" alt="f2_v_.png" style="zoom:50%;" /></p>
<p><span style="border-bottom:2px dashed red;">输入为采样得到的相同类型邻居的特征表示，输出为这一类型邻居的向量表示</span>，有点类似与GraphSAGE的思想，<span style="border-bottom:2px dashed red;">利用Bi-LSTM对相同类型的邻居节点进行聚合</span>，模型结构如图所示：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/05/22/j768MorHRWbhtUd.jpg" alt="NN-2.png" style="zoom:50%;" /></p>
<p>Type Combination:</p>
<p>上述得到了每个类型节点的向量表示，这里，希望对这些类型的节点进行聚合，考虑到不同类型节点的邻居贡献不同，因此引入注意力机制l联合学习不同类型的邻居：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/05/22/TLhV1KUcCMB6pYP.jpg" alt="FC8680C6-BD1C-4ADE-9DBF-A0B3C255DF27.png" style="zoom:50%;" /></p>
<p>模型结构如图所示：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/05/22/wVPgWO1y8K3UJ2h.jpg" alt="NN-3.png" style="zoom:50%;" /></p>
<hr>
<h3>Objective function and Framework</h3>

<p>根据目标函数学习模型参数：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/05/22/IAXLDbB9mT82onk.jpg" alt="fn.png" style="zoom:50%;" /></p>
<p>Framework:</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/05/22/E3RnvjMmPidVucN.jpg" alt="Framework.png" style="zoom:50%;" /></p>
<p><strong>分成5步：</strong></p>
<ol>
<li><p>对邻居节点进行采样，按照节点类型进行分类</p>
</li>
<li><p>NN-1：对节点不同类型特征学习</p>
</li>
<li><p>NN-2：对相同类型节点各个特征的聚合</p>
</li>
<li><p>NN-3：对不同类型节点的聚合</p>
</li>
<li><p>根据Graph Context Loss损失函数进行优化</p>
</li>
</ol>
<p>最终得到每个节点的向量表示用于下游任务</p>
<hr>
<h3>Contribution:</h3>

<ul>
<li>Define heterogeneous graph: heterogeneous of graph structure and nodes information.</li>
<li>Propose HetGNN, which can capture the heterogeneous of stucture and content at the same time. It can be applied to both direct and inductive tasks.</li>
<li>Good performance in multi-data set experiments, link prediction, node classification, clustering and other tasks.</li>
</ul>
<hr>
<p>Reference :</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/w0KhXwE3tkZtjzQcRApRow" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/w0KhXwE3tkZtjzQcRApRow</a></li>
<li><a href="https://www.jianshu.com/p/fd8355e3d5d5" target="_blank" rel="noopener">https://www.jianshu.com/p/fd8355e3d5d5</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/88921408" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/88921408</a></li>
<li><a href="https://blog.csdn.net/hero_fantao/article/details/69659457?locationNum=10&amp;fps=1" target="_blank" rel="noopener">https://blog.csdn.net/hero_fantao/article/details/69659457?locationNum=10&amp;fps=1</a></li>
</ul>
<p>Paper:</p>
<ul>
<li><a href="https://doi.org/10.1145/3292500.3330961" target="_blank" rel="noopener">https://doi.org/10.1145/3292500.3330961</a></li>
</ul>
<p>Code:</p>
<ul>
<li><a href="https://github.com/chuxuzhang/KDD2019_HetGNN" target="_blank" rel="noopener">https://github.com/chuxuzhang/KDD2019_HetGNN</a></li>
</ul>
]]></content>
      <categories>
        <category>recommendation</category>
        <category>HGNN</category>
      </categories>
      <tags>
        <tag>heterogeneous GNN</tag>
      </tags>
  </entry>
  <entry>
    <title>Summary of learning resources</title>
    <url>/2020/04/17/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB/</url>
    <content><![CDATA[<blockquote>
<blockquote>
<p>Some links about graph network.</p>
<p>Learning resources about GNN  graph embedding  LSTM etc..</p>
</blockquote>
</blockquote>
<ol>
<li><p><a href="https://mp.weixin.qq.com/s/zmX0L6ZeCqlTBYtP9XslgA" target="_blank" rel="noopener">自监督黑马SimCLRv2来了！提出蒸馏新思路，可迁移至小模型，性能精度超越有监督</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/Z1WeYOV4Kwp6os73FqG9ew" target="_blank" rel="noopener">近期必读的五篇KDD 2020【图神经网络 (GNN) 】相关论文_Part2</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/JU11nv2lv-zFesHjp2IAQQ" target="_blank" rel="noopener">Graph: 表现再差，也不进行Pre-Training? Self-Supervised Learning真香！</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/LajTagTaCHBNoqogo1zEiw" target="_blank" rel="noopener">KDD2020推荐系统论文聚焦</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/alVFJvksR0W6NMErubTM_g" target="_blank" rel="noopener">IJCAI2020 图相关论文集</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/WslDM6Mgx7_i5yW3XFjdAw" target="_blank" rel="noopener">【DL】规范化：你确定了解我吗？</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/iECG0sBte25li-E3Z_tEIw" target="_blank" rel="noopener">用 Python 训练自己的语音识别系统，这波操作稳了！</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/yKxwqwCC_Jxse_z3mK9lAw" target="_blank" rel="noopener">【CTR】MMoE-PosBias：Youtube 多任务学习框架</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/Rqz9yuSxpx_os54FrzyMqQ" target="_blank" rel="noopener">一文尽览推荐系统模型演变史(文末可下载)</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/iMU2LPDadmVMgzUifw3-XA" target="_blank" rel="noopener">浅谈电商搜索推荐中ID类特征的统一建模：Hema Embedding解读</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/ifTNRW0W7-P_LyfNldtavQ" target="_blank" rel="noopener">多任务学习方法在推荐中的演变</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/5SWMKuKC3XLvUdbKS8qJZw" target="_blank" rel="noopener">近期必读的六篇顶会 ICML 2020【图神经网络 (GNN) 】相关论文</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/80Ze31Nstqp5nuRD7B3vKw" target="_blank" rel="noopener">图深度学习：成功，挑战以及后面的路</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/4wzbZqU4rJcl6qEZTI8m2A" target="_blank" rel="noopener">KDD2020|混合时空图卷积网络：更精准的时空预测模型</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/h61viD2JvOcb6SpHrk5PQg" target="_blank" rel="noopener">一文概览如何消除广告和推荐中的Position Bias</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/c0hPqwfbgdSKGvJwN5nX3A" target="_blank" rel="noopener">WSDM 2020关于深度推荐系统与CTR预估工业界必读的论文</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/J6GUf-pAXmI9jhFGyVyU8w" target="_blank" rel="noopener">图系列|三篇图层次化表示学习(Hierarchical GNN)：图分类以及节点分类</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/A64paLV_3Arhu1LKvKc63w" target="_blank" rel="noopener">PinSAGE | GCN 在工业级推荐系统中的应用</a><br>《Graph Convolutional Neural Networks for Web-Scale Recommender Systems》2018.</p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/ewzsURiU7bfG3gObzIP2Mw" target="_blank" rel="noopener">深度长文：图神经网络欺诈检测方法总结</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/Yj_yP6LjxrG_UJX9W3cAHQ" target="_blank" rel="noopener">腾讯 at IJCAI 2020，基于内部-环境注意力网络的推荐多队列冷启动召回</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/rjWOkwzX3IE59Kc9P9leAQ" target="_blank" rel="noopener">格“物”致知：多模态预训练再次入门</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/FtUxxRWkUJ7346gWAJqQDQ" target="_blank" rel="noopener">【KDD2020-MSU】图结构学习的鲁棒图神经网络，克服对抗攻击提升GNN防御能力</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/6OMMboBdbLVA-HsEjA3bSA" target="_blank" rel="noopener">Ctr 预估之 Calibration</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/aNx9or0-eHHT1XI1BJyB7w" target="_blank" rel="noopener">综述|73页近百篇参考文献JMLR20动态图上的表示学习</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/lbRymEKPcCrLsk9r9w1tlQ" target="_blank" rel="noopener">高性能涨点的动态卷积 DyNet 与 CondConv、DynamicConv 有什么区别联系？</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/7_cZhszsnfBKyoUKYMkQhw" target="_blank" rel="noopener">近期必读的五篇计算机视觉顶会CVPR 2020【图神经网络 (GNN) 】相关论文-Part 3</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/KHbCbzP1JMgoYMjY5cMOmA" target="_blank" rel="noopener">华为诺亚实验室开源Disout算法，直接对标谷歌申请专利的Dropout算法</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/n3CSdTdsk5erpwRpZE9Qvw" target="_blank" rel="noopener">图专题|IJCAI2020两篇多层次/多视角相关的图神经网络GNN研究论文</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/_VBO_9yiZ0qngq4yfwcxEg" target="_blank" rel="noopener">从EMD、WMD到WRD：文本向量序列的相似度计算</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/Fjr2iCbeTy9AYNAwvJr7Og" target="_blank" rel="noopener">Mila唐建博士最新《图表示学习:算法与应用》2020研究进展，附59页ppt</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/DKDlFDRwoedSlhL8cu99DA" target="_blank" rel="noopener">从 Triplet loss 看推荐系统中文章Embedding</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/aCICOsif_jYNUkOTiB_-FQ" target="_blank" rel="noopener">【Code】GraphSAGE 源码解析</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/upXLNJsyq1muoNn3uJCKJg" target="_blank" rel="noopener">入门推荐系统，这25篇综述文章足够了</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/w0KhXwE3tkZtjzQcRApRow" target="_blank" rel="noopener">KDD19开源论文 Heterogeneous Graph Neural Network</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/Xn1xMXu0V7nkolX5x_h2lw" target="_blank" rel="noopener">SDM(Sequential Deep Matching Model)的复现之路</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/WnF-fqQyr2VNqr75Jzoqsw" target="_blank" rel="noopener">【GNN】Diff Pool：网络图的层次化表达</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/jIVhXMmP95G7hJP_KtksxA" target="_blank" rel="noopener">Youtube推荐RL首弹，基于Top-K的Off-Policy矫正解决推荐中的信息茧房困境</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/yyE0ki5rymOCyGa6CNfnzQ" target="_blank" rel="noopener">《深度学习推荐系统》读书笔记之Embedding技术在推荐中的应用</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/FZoiDfM05rWIqWJQg8F-5w" target="_blank" rel="noopener">RecSys 2019最佳论文：基于深度学习的推荐系统是否真的优于传统经典方法？</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/daIWHWMulVlqAsm04qyi6w" target="_blank" rel="noopener">2019年，异质图神经网络领域有哪些值得读的顶会论文？</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/63lTwUB2wsvhikWnNKa0CQ" target="_blank" rel="noopener">「工业落地」基于异质图神经网络的异常账户检测</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/Ry8R6FmiAGSq5RBC7UqcAQ" target="_blank" rel="noopener">深入理解图注意力机制（Graph Attention Network）</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/1xVPRIVwQQJfEen0RiNYvg" target="_blank" rel="noopener">谈谈推荐系统中的用户行为序列建模最新进展</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/Jmutb3fWuxxmbpX4CWS3HA" target="_blank" rel="noopener">《深度学习推荐系统》读书笔记之推荐系统的进化之路</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/zLBq2VVSAr0AUtozK7qqVA" target="_blank" rel="noopener">浅析Faiss在推荐系统中的应用及原理</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/Fg9GKw-QLlOskEqX7pl1dA" target="_blank" rel="noopener">推荐系统之FM算法原理及实现（附代码）</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/dSa0-BydmzmqDfrICJoUBQ" target="_blank" rel="noopener">ECAI2020推荐系统论文聚焦</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/wLjBcK9PQHz6lUIsiZCjgg" target="_blank" rel="noopener">长文|三大主题全方位梳理图论与图学习中的基本概念：图搜索，最短路径，聚类系数，中心度等</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/jVG_JJN5OalJcbtTGti9AA" target="_blank" rel="noopener">「工业落地」阿里异质图神经网络推荐：19KDD IntentGC</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/3gHKD8g3tB6z88DVq5AkIw" target="_blank" rel="noopener">「工业落地」阿里异构图表示学习：19KDD GATNE</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/y94JACgjTxObvNDhmUfulA" target="_blank" rel="noopener">图神经网络入门（三）GAT图注意力网络</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/3x6FtOMzGitTlwwJs-dN9w" target="_blank" rel="noopener">从ACL 2020和ICLR 2020看知识图谱嵌入的近期研究进展</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/4CPfejPrAhJuYMgSj6aGsw" target="_blank" rel="noopener">【香港理工】生成式对抗网络(GANs)最新2020综述，41页pdf阐述GAN训练、 挑战、解决方案和未来方向</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/53zPX7N7YLlUnvD23kT6pg" target="_blank" rel="noopener">SIGIR 20 | 腾讯看点首次在推荐中应用迁移学习提出PeterRec框架，从少量行为数据中学习用户表示</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/lf9BjP9qs_cn8f9vGUECpw" target="_blank" rel="noopener">比CNN更强有力，港中文贾佳亚团队提出两类新型自注意力网络｜CVPR2020</a></p>
</li>
<li><p><a href="https://www.toutiao.com/a6825468794287161860/?tt_from=weixin&amp;utm_campaign=client_share&amp;wxshare_count=1&amp;timestamp=1589325241&amp;app=news_article&amp;utm_source=weixin&amp;utm_medium=toutiao_ios&amp;req_id=202005130714000101941000345D3EC4DC&amp;group_id=6825468794287161860" target="_blank" rel="noopener">33 个神经网络「炼丹」技巧</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/azgdVJ3-Ge1HJC1mVTRVfA" target="_blank" rel="noopener">自动化神经网络理论进展缓慢，AutoML 算法的边界到底在哪</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/SqBU40sfo3IEj_iHnb4cXQ" target="_blank" rel="noopener">高效利用无标注数据：自监督学习简述</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/0Jn79XT9A70sQEMOImzFLg" target="_blank" rel="noopener">【斯坦福谷歌】最新《图机器学习》综述论文，38页pdf阐述最新图表示学习进展</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/GCoyRPKe9ND7CnG9-zWTkA" target="_blank" rel="noopener">近期必读的5篇顶会CVPR 2020【场景图+图神经网络（SG+GNN）】相关论文</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/SOaA9XNnymLgGgJ5JNSdBg" target="_blank" rel="noopener">对比学习（Contrastive Learning）相关进展梳理</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/0jPOSvSqsTEtwjQ_nRgQ_g" target="_blank" rel="noopener">干货！2019五大顶会必读的Graph Embedding相关的论文</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/W2UTCtoTqRW739EFxCFf1w" target="_blank" rel="noopener">在深度学习顶会ICLR 2020上，Transformer模型有什么新进展？</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/z0UMPHLYFOaiTCxqt65uRg" target="_blank" rel="noopener">当深度学习遇上量化交易——图与知识图谱篇</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/wLvTo_k67VwiwrfW9dfSWg" target="_blank" rel="noopener">「工业落地」阿里在图神经网络推荐的探</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/RhnyQUnXxMDybv8B-dSj4Q" target="_blank" rel="noopener">图系列|WWW2020 图学习/图神经网络GNN相关论文速览</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/byVdEPcCmVPJOk-uIyGsbw" target="_blank" rel="noopener">【重磅】GCN大佬Thomas Kipf博士论文《深度学习图结构表示》178页pdf阐述图卷积神经网络等机制与应用</a></p>
</li>
</ol>
]]></content>
      <tags>
        <tag>recommendation</tag>
        <tag>GNN</tag>
      </tags>
  </entry>
  <entry>
    <title>HIN-Graph</title>
    <url>/2020/04/03/HIN-Graph/</url>
    <content><![CDATA[<blockquote>
<blockquote>
<p>Summarying the recent works on <strong>heterogeneous graph neural networks</strong></p>
</blockquote>
</blockquote>
<h2>Links</h2>

<p>清华朱文武老师组关于图上的深度学习综述(2018) <a href="https://arxiv.org/abs/1812.04202v1" target="_blank" rel="noopener" title="With a Title">Deep Learning on Graphs: A Survey</a>. </p>
<p>IEEE Fellow关于图神经网络综述(2019) <a href="https://arxiv.org/abs/1901.00596" target="_blank" rel="noopener" title="With a Title">A Comprehensive Survey on Graph Neural Networks</a>. </p>
<p>Ziniu Hu关于异质图Transformer的研究(2020) <a href="https://arxiv.org/abs/2003.01332" target="_blank" rel="noopener" title="With a Title">Heterogeneous Graph Transformer</a>. </p>
<p>纪厚业做的分享： <a href="https://www.bilibili.com/video/BV1HE41157GD?t=6492" target="_blank" rel="noopener" title="With a Title">异质图神经网络：模型和应用</a>. 介绍了异质图网络、以及一些模型和三个实际落地的论文</p>
<p>发表在数据库顶会VLDB上的最新异质图表示学习的综述(2020) <a href="https://arxiv.org/abs/1801.05852" target="_blank" rel="noopener" title="With a Title">Heterogeneous Network Representation Learning: Survey, Benchmark, Evaluation, and Beyond</a>.</p>
<p>图数据上的对抗攻击和防御综述(2020) <a href="https://arxiv.org/abs/1812.10528v1" target="_blank" rel="noopener" title="With a Title">Adversarial Attack and Defense on Graph Data: A Survey</a>. </p>
<p>用于构建动态图的例子《Dynamic Network Embedding by Modelling Triadic Closure Process(2018)》 <a href="https://github.com/luckiezhou/DynamicTriad" target="_blank" rel="noopener" title="With a Title">https://github.com/luckiezhou/DynamicTriad</a>. </p>
<hr>
<p>主流知识图谱表示学习算法:</p>
<ul>
<li>TransE、ComplEx、DistMult、TransR、RESCAL、RotatE </li>
</ul>
]]></content>
      <tags>
        <tag>HIN</tag>
        <tag>graph-learning</tag>
        <tag>GNN/GCN</tag>
      </tags>
  </entry>
  <entry>
    <title>metapath2vec: Scalable Representation Learning for Heterogeneous Networks</title>
    <url>/2019/12/12/metapath2vec-Scalable-Representation-Learning-for-Heterogeneous-Networks/</url>
    <content><![CDATA[<blockquote>
<blockquote>
<p>This article proposed two scalable representation learning models: <strong>metapath2vec</strong> and <strong>metapath2vec++</strong>. </p>
<p>In metapath2vec: </p>
<ul>
<li>First, they propose <strong>meta-path</strong> based <strong>random walks</strong> in heterogeneous networks to generate <strong>hetetogeneous neighborhoods</strong> with network semantics for various types of nodes.</li>
<li>Second, they extend the <strong>skip-gram</strong> model to facilitate the modeling of geographically and semantically close nodes.</li>
</ul>
<p>In metapath2vec++:</p>
<ul>
<li>they develop a <strong>heterogeneous negative sampling-based</strong> method that enables the accurate and efficient prediction of a node’s heterogeneous neighborhood.</li>
</ul>
<p><a href="http://dx.doi.org/10.1145/3097983.3098036" target="_blank" rel="noopener">http://dx.doi.org/10.1145/3097983.3098036</a></p>
</blockquote>
</blockquote>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/05/25/Afdic56XJB1YWKT.jpg" alt="1.jpeg" style="zoom:50%;" /></p>
<h2>Data</h2>

<ul>
<li>AMiner Computer Science dataset</li>
<li>Database and Information Systems dataset</li>
</ul>
<h2>Coding</h2>

<ul>
<li><a href="https://ericdongyx.github.io/metapath2vec/m2v.html" target="_blank" rel="noopener">https://ericdongyx.github.io/metapath2vec/m2v.html</a> </li>
</ul>
]]></content>
      <categories>
        <category>Heterogeneous Networks</category>
      </categories>
      <tags>
        <tag>meta path</tag>
        <tag>heterogeneous</tag>
        <tag>skip-gram</tag>
        <tag>negative sampling-based method</tag>
      </tags>
  </entry>
  <entry>
    <title>Meta-Graph Based Recommendation Fusion over Heterogeneous Information Networks</title>
    <url>/2019/12/09/Meta-Graph-Based-Recommendation-Fusion-over-Heterogeneous-Information-Networks/</url>
    <content><![CDATA[<blockquote>
<blockquote>
<p>This article is paper about recommendation over heterogeneous information networks. Because HIN faces two problems: 1, how to represent the high-level semantics of recommendations and 2, how to fuse the heterogeneous information to make recommendations, the author first introduced the concept of <strong>meta-graph</strong> to HIN-based recommendation, and then solved the information fusion problem with a <strong>“matrix factorization(MF) + factorization machine(FM)”</strong> approach.</p>
<p>I think the author made two innovations:</p>
<p>1, use meta-graph and commute matrix to construct similarity matrix;</p>
<p>2, for each meta-graph, he calculate user-item’s similarity matrix, then factorize users and items’ feature vectors by using matrix factorization (MF). So he can obtain many different users and items vectors of many different meta-graph. for all of then, he use factorization machine(FM) to component then.</p>
<p>A user-item score prediction problem.</p>
</blockquote>
</blockquote>
<p><strong>KEYWORDS:  Recommendation system; Collaborative filtering; Heterogeneous information networks; Factorization machine.</strong></p>
<p><h2>Contents</h2></p>
<blockquote>
<ul>
<li><a href="#1">INTRODUCTION</a></li>
<li><a href="#2">FRAMEWORK</a><ul>
<li><a href="#2.1">Meta-graph based Similarity</a></li>
<li><a href="#2.2">Meta-graph based Latent Features</a></li>
<li><a href="2.3">Recommendation Model</a></li>
<li><a href="2.4">Comparison with Previous Latent Feature based Model</a></li>
</ul>
</li>
<li><a href="#3">MODEL OPTIMIZATION</a><ul>
<li><a href="#3.1">Optimization</a></li>
<li><a href="3.2">Complexity Analysis</a></li>
</ul>
</li>
<li><a href="4">EXPERIMENTS</a><ul>
<li><a href="4.1">Datasets</a></li>
<li><a href="#4.2">Evaluation Metric</a></li>
<li><a href="#4.3">Baseline Models</a></li>
</ul>
</li>
</ul>
</blockquote>
<p><h2 id='1'>INTRODUCTION</h2><br>​        Heterogeneous information networks(<strong>HINs</strong>) have been proposed as a general data representation for many different types of data. </p>
<p>​        At the beginning, HINs were used to handle entity search and similarity measure problems. Later, it was extended to handle heterogeneous entity recommendation problems. Then, the semantic relatedness constrained by the entity types can be defined by the similarities between two entities along meta-graph.</p>
<p>​        For traditional collaborative filtering, we always build a simple meta-path <em>Business —&gt; User</em> and learn from this meta-path to make generalization. From HIN’s schema, we can build more complicated meta-paths like <em>User—&gt;Review—&gt;Word—&gt;Review—&gt;Business</em>. </p>
<p>​        There are two major challenges when applying meta-path based similarities to recommender systems: <strong>First</strong>, meta-path may not be the best way to characterize the rich semantics. So, in this paper, they used <strong>meta-graph(or meta-structure)</strong> to compute similarity between homogeneous type of entities over HINs. <strong>Second</strong>, different meta-paths or meta-graphs result in different similarities. How to assemble them in an effective way is another challenge.</p>
<p>​        So, in this paper, in order to solve the above two problems, author first used <strong>the concept of meta-graph</strong> and then applied <strong>matrix factorization(MF) + factorization machine(FM)</strong>.</p>
<pre><code>    * for each meta-graph, compute the user-item similarity matrix
    * Use unsupervised MF to factorize it into a set of user and item latent vectors.
    * Use FM to assemble them to learn from the rating matrix. 
</code></pre><p><h2 id='2'>FRAMEWORK</h2></p>
<p><h3 id='2.1'>Meta-graph based Similarity</h3><br>​        The definition of Meta-graph:</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2019/12/09/VePI7wiHLXqt8fQ.jpg" alt="meta-graph.png"></p>
<p>​        All of the meta-graphs on Amazon and Yelp:</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2019/12/09/wjOSsENmKGR2yY9.jpg" alt="all of the meta-graphs for Amazon and Yelp"></p>
<p>​        The author used <strong>commuting matrices</strong> to compute the counting based similarity matrix of a meta-path. For example: if we have a meta-path:<img src= "img/loading.gif" data-src="https://i.loli.net/2019/12/09/Pkmpr4n9MUfvV5s.jpg" alt="meta-path.png"></p>
<p>And, we define a matrix $W_{A_iA_j}$ as the adjacency matrix between type $A_i$ and $A_j$. So the commuting matrix for path p is:<img src= "img/loading.gif" data-src="https://i.loli.net/2019/12/09/esYuT4iBoKzdtrN.jpg" alt="ABF68C6E-64CD-469A-BC2E-7D1B19ED3D31.png"></p>
<p><strong>Algorithm 1 Computing commuting matrix for CM9 :</strong></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2019/12/09/Yshg3R91MzpqZ48.jpg" alt="B07D1FB3-2139-4357-A000-0EA870DA1635.png"></p>
<p><h3 id='2.2'>Meta-graph based Latent Features</h3><br>​        By use of matrix factorization method, we can obtain user and item’s low-rank matrices:<img src= "img/loading.gif" data-src="https://i.loli.net/2019/12/09/z4JjWdDx5TnMNf3.jpg" alt="low-rank.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2019/12/09/pmxiLPM6atBTrq8.jpg" alt="702CE1AE-D96F-4C88-BA2E-B80D483F536C.png"></p>
<p>Which ${\lambda_u},{\lambda_b}$ are the hyper-parameters that control the influence of Frobenius norm regularization to aviod overfitting.</p>
<p><h3 id='2.3'>Recommendation Model</h3><br>​        For the L groups of user and item latent features and F ranks which used to factorize every similarity matrix, we concatenate all of the corresponding user and item features form all of the L meta-graphs:<img src= "img/loading.gif" data-src="https://i.loli.net/2019/12/09/abOz2gnh9PDwklq.jpg" alt="EADE9E6C-D305-4B2C-BF96-033AC1A9BAF7.png"></p>
<p>​        So the rating for the sample $x^n$ based on FM is computed as follows:</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2019/12/09/5oRL7Dfn9Sl1vGt.jpg" alt="99A1983D-A650-43E0-95B0-C2D2E7826EA3.png"></p>
<p>Where $w_0$ is the global bias representing the first-order weights for the features, and $V = [v_i]$ represents the second-order weights to model the interactions across different features. $&lt;.,.&gt;$ is the dot product of two vectors of size K.</p>
<p>​        The parameters can be learned by minimizing the mean square loss:<img src= "img/loading.gif" data-src="https://i.loli.net/2019/12/11/k63WYeTtjAdoalO.jpg" alt="mean square loss.png"></p>
<p>Where $y^n$ is an observed rating for the n-th sample, N is the number of all the observed ratings.</p>
<p><strong>There are two problems when applying the FM model to the meta-graph based latent features:</strong></p>
<ul>
<li>It may bring noise when working with many meta-graphs thus impairing the predicting capability of the model. In practice, some meta-graphs can be useless since information provided by some meta-paths can be covered by others.</li>
<li>Computational cost.</li>
</ul>
<p>​        To alleviate the above two problems, we propose a novel regularization for FM,i.e., the group lasso regularization, which is a feature selection method on a group of variables. The group lasso regularization of parameters p is defined as follows:<img src= "img/loading.gif" data-src="https://i.loli.net/2019/12/09/THbdrAQpqiRCLlI.jpg" alt="6F205C97-3FB4-4FE5-B2D8-9A5A8E0B7ECA.png"></p>
<p><h3 id='2.4'>Comparison with Previous Latent Feature based Model</h3><br>​        The previous approaches of recommendation based on HIN is not adequate, as it fails to capture the interactions between inter-meta-graph features, i.e. features across different meta-graphs, as well as the intra-meta-graph features, i.e. It may decrease the prediction ability of all of the user and item features.</p>
<p><h2 id='3'>MODEL OPTIMIZATION</h2><br>​        The author define FM over meta-graph(FMG) model with the following objective function:</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2019/12/09/FYwu3NTrWsHG5hy.jpg" alt="1CB932AA-8575-4885-82D5-3190F9CA0128.png"></p>
<p>​        We can see now the object function is non-convex and non-smooth. So author used <strong>proximal gradient (nmAPG) algorithm</strong>.</p>
<p><h2 id='4'> EXPERIMENTS</h2></p>
<p><h3 id='4.1'>Datasets</h3></p>
<ul>
<li>Yelp (<a href="https://www.yelp.com/dataset_challenge" target="_blank" rel="noopener">https://www.yelp.com/dataset_challenge</a>)</li>
<li>Amazon Electronics (<a href="http://jmcauley.ucsd.edu/data/amazon/" target="_blank" rel="noopener">http://jmcauley.ucsd.edu/data/amazon/</a>)</li>
</ul>
<p><h3 id='4.2'>Evaluation Metric</h3><br>​        RMSE</p>
<p><h3 id='4.3'>Baseline Models</h3></p>
<ul>
<li>RegSVD: RegSVD is the basic matrix factorization with L2 regularization, which uses only the user-item rating matrix. (<a href="https://www.librec.net/" target="_blank" rel="noopener">https://www.librec.net/</a>)</li>
<li>FMR: FMR is the factorization machine with only the user-item rating matrix.(<a href="http://www.libfm.org/" target="_blank" rel="noopener">http://www.libfm.org/</a>)</li>
<li>HeteRec: HeteRec method is based on meta-path based similarity between users and items.</li>
<li>SemRec: SemRec is a meta-path based recommendation on weighted HIN, which is built by connecting users and items with the same ratings.(<a href="https://github.com/zzqsmall/SemRec" target="_blank" rel="noopener">https://github.com/zzqsmall/SemRec</a>)</li>
</ul>
]]></content>
      <categories>
        <category>recommendation</category>
        <category>HIN</category>
      </categories>
      <tags>
        <tag>Recommendation</tag>
        <tag>Meta-graph</tag>
        <tag>Heterogeneous Information Networks</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo+Github搭建简单博客总结</title>
    <url>/2019/11/23/Hexo+Github%E6%90%AD%E5%BB%BA%E7%AE%80%E6%98%93%E5%8D%9A%E5%AE%A2%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h1 id="Hexo-Github搭建简易博客总结"><a href="#Hexo-Github搭建简易博客总结" class="headerlink" title="Hexo+Github搭建简易博客总结"></a>Hexo+Github搭建简易博客总结</h1><p><a href="https://www.zhihu.com/question/39183612" target="_blank" rel="noopener">参考知乎问题: 如何使用10个小时搭建出个人域名而又Geek的独立博客？</a></p>
<p>前言：</p>
<p>一直没有尝试自己搭建一个博客，之前本来想着自己前后端搭建，但是懒惰。。。。。hexo+github是一个傻瓜式的自助搭建博客方式，很简便，如果是稍微有点代码基础的，应该都能在几个小时之内熟悉搭建整个过程。</p>
<p>软件准备：<a href="https://nodejs.org/en/" target="_blank" rel="noopener">Node.js</a>、<a href="https://git-scm.com/downloads" target="_blank" rel="noopener">Git</a></p>
<ol>
<li>先安装Hexo: <code>$ npm install -g hexo-cli</code></li>
<li><p>再初始化Hexo:<code>$ npm install</code></p>
<p>Hexo的大体文件：</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">themes 存放主题的文件夹+ source 博客资源文件夹+ source/_drafts 草稿文件夹+ source/_posts 文章文件夹+ themes/landscape 默认皮肤文件夹+ themes 存放主题的文件夹+ source 博客资源文件夹+ source/_drafts 草稿文件夹+ source/_posts 文章文件夹+ themes/landscape 默认皮肤文件夹</span><br></pre></td></tr></table></figure>
<ul>
<li>_config.yml 全局配置文件。要注意的是，该文件格式要求极为严格，缺少一个空格都会导致运行错误。小提示：不要用Tab缩进，两个空格符， <strong>冒号：后面只用一个空格即可*</strong>。</li>
</ul>
<ol>
<li><p>我采用的还是最常用的<a href="https://hexo.io/themes/" target="_blank" rel="noopener">Hexo|Next主题</a>，修改了一连串配置，包括侧栏等等</p>
</li>
<li><p>Hexo部署</p>
<p>到这里，就可以启动本地服务器打开了：<code>$ hexo s</code></p>
<p>在localhost:4000端口就可以查看到效果。</p>
<p>在git上创建了一个和本地的文件夹名称一致，我都是创见的daluzi.github.io，然后把本地的文件夹朴实到远程仓库里面。</p>
<p>接下来，在站点的config.yml文件中，部署如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">deploy:  	 type: git 部署类型若有问题，其他类型自行google之  	 repository: https://github.com/daluzi/daluzi.github.io.git  	 branch: master  	 plugins: -hexo-generator-feed</span><br></pre></td></tr></table></figure>
<p>然后</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ hexo g #生成静态网页# hexo d #开始部署</span><br></pre></td></tr></table></figure>
<p>好了，在浏览器输入daluzi.github.io就能查看博客了，接下来就是挂载域名。</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>Multimodal Machine Learning:A Survey and Taxonomy</title>
    <url>/2019/11/23/Multimodal-Machine-Learning-A-Survey-and-Taxonomy/</url>
    <content><![CDATA[<blockquote>
<blockquote>
<p>This article is a survey for multimodal machine learning. This paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. They go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: <strong>representation, translation, alignment, fusion, and co-learning.</strong></p>
</blockquote>
<p>Index Terms:<strong>Multimodal, machine learning, introductory, survey</strong></p>
</blockquote>
]]></content>
      <tags>
        <tag>Muilimodal</tag>
        <tag>machine learning</tag>
        <tag>survey</tag>
      </tags>
  </entry>
  <entry>
    <title>Semi-supervised Classification with Graph Convolutional Networks</title>
    <url>/2019/11/14/Semi-supervised-Classification-with-Graph-Convolutional-Networks/</url>
    <content><![CDATA[<blockquote>
<blockquote>
<p>The convolution is extended to the data of the graph structure.Motivate the choice of our convolutional architecture via a localized first-order approximation of <strong>spectral graph convolutions</strong>.The model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes.</p>
<p>In a number of experiments on <strong>citation networks</strong> and on a <strong>knowledge graph dataset</strong>,This model works well in semi-supervised tasks.</p>
</blockquote>
</blockquote>
<h1 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h1><blockquote>
<ul>
<li><a href="#1">The main content of the article</a><ul>
<li><a href="#1.1">abstract</a></li>
<li><a href="#1.2">spectral graph convolutions</a></li>
<li><a href="#1.3">layer-wise linear model</a></li>
<li><a href="#1.4">semi-supervised node classifacation</a></li>
</ul>
</li>
<li><a href="#2">My learing understanding of semi-GCN</a></li>
</ul>
</blockquote>
<h2 id='1'>The main content of the article</h2>

<h3 id='1.1'>abstract</h3>

<p>The authors motivate the choice of their convolutional architecture via a localized first-order approximation of <strong>spectral graph convolutions</strong>.</p>
<p>By training the structural model of convolution neural network with some tagged node data in the graph structural data ,the network model can further classify the remaining untagged data.</p>
<p>So,on the whole , the way which the convolution is extended to the data of the graph structural ,can get a better data representation ,and it works well in semi-supervised tasks.</p>
<blockquote>
<p>Datasets: citation network datasets(Citeseer ,Cora ,Pubmed) ,bipartite graph dataset(NELL) </p>
</blockquote>
<h3 id='1.2'>spectral graph convolutions</h3>

<p><em>In this period ,I mainly refer to this website:<a href="https://blog.csdn.net/qq_41727666/article/details/84622965" target="_blank" rel="noopener">https://blog.csdn.net/qq_41727666/article/details/84622965</a></em></p>
<p>In fact ,there are two versions of GCN:</p>
<p>1.The first generation of GCN is this formula:<strong><script type="math/tex">g{\theta} * x = Ug{\theta}U^Tx</script></strong>.</p>
<p>In the formula ,$x$ is the eigenvector of the graph node ,$g{\theta}=diag(\theta)$ is the convolution kernel.$U$ is the matrix of eigenvectors of the normalized graph Laplacian $L$. And the $L = I_N - D^{-1/2}AD^{-1/2} = U{\Lambda}U^T$</p>
<p><strong>But ,this is too complicated to calculate</strong></p>
<p>2.To circumvent this problem ,$g{\theta}(\Lambda)$ can be well-approximated by a truncated expansion in terms of Chebyshev polynomial $T_k(x)$ up to $K^{th}$ order:</p>
<script type="math/tex; mode=display">g{\theta}^` {\approx} {\sum_{k=0}^{K}{\theta}^`T_k(\Lambda^1)}</script><p>With a rescaled $\Lambda^1 = {\frac{2}{\lambda_{max}}}\Lambda - I_N$. $\lambda_{max}$ Denotes the largest eigenvalue of $L$. $\theta^`$ is now a vector of Chebyshev coefficients. The Chebyshev polynomials are recursively defined as $T_k(x) = 2xT_{k-1}(x) - T_{k-2}(x)$, with $T_0(x) = 1$ and $T_1(x) = x$.</p>
<p>So, now the formula is:</p>
<script type="math/tex; mode=display">g{\theta}^` * x \approx {\sum_{k=0}^{K}{\theta}_k^`T_k(L^1)x}</script><p>With $L^1 = {\frac{2}{\lambda_{max}}L - I_N}$ .This expression is now K-localized since it is a <strong>$K^{th}$-order ploynomial in the Laplacian</strong>.</p>
<h3 id='1.3'>layer-wise linear model</h3>

<p>The authors approximate $\lambda_{max} \approx 2$, so now the formula is simplified to:</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2019/11/17/LdqIwO9NfxzXWJv.png" alt="semi_eq5.png"></p>
<h3 id='1.4'>semi-supervised node classification</h3>

<p>In this example, they consider a two-layer GCN for semi-supervised node classification on a graph with a symmetric adjacency matrix A(binary or weighted). </p>
<p>First, calculate $A^` = D1^{-1/2}A^1D1^{-1/2}$.</p>
<p>And the forward model then takes the simple form:<script type="math/tex">Z = f(X,A) = softmax(A^`{ReLU}(A^`XW^{(0)})W^{(1)})</script></p>
<p>Here, $W^{(0)}$ is an input-to-hidden weight matrix for a hidden layer with H feature maps.</p>
<p>$W^{(1)}$ is a hidden-to-output weight matrix.</p>
<p>The softmax activation function defined as $softmax(x_i) = \frac{1}{\sum_i{exp(x_i)}}exp(x_i)$</p>
<p>For semi-supervised multiclass classification, they then evaluate the cross-entropy error over all labeled examples :$\xi = -\sum_{l\epsilon{y_l}}\sum_{f=1}^{F}Y_{lf}lnZ_{lf}$</p>
<p>The author perform <strong>batch gradient descent</strong>. Stochasticity in the training process is in produced via <strong>dropout</strong>. </p>
<h2 id='2'>My learing understanding of semi-GCN</h2>

<p><em>mainly depend on <a href="https://zhuanlan.zhihu.com/p/58178060" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/58178060</a></em></p>
]]></content>
      <tags>
        <tag>GCN</tag>
        <tag>Deeplearning</tag>
      </tags>
  </entry>
  <entry>
    <title>Personalized Recommendation Combining User Interest and Social Circle</title>
    <url>/2019/11/13/Personalized-Recommendation-Combining-User-Interest-and-Social-Circle/</url>
    <content><![CDATA[<blockquote>
<blockquote>
<p>Interpersonal influence and interest based on circles of friends can be used to solve cold start and sparsity problem of datasets.This paper used three social factors:<strong>personal interest</strong>,<strong>interpersonal interest similarity</strong> and <strong>interpersonal influence</strong> to make personalized recommendation.</p>
</blockquote>
</blockquote>
<h1 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h1><blockquote>
<ul>
<li><a href="#1">Introduction</a></li>
<li><a href="#2">Problem formulation</a></li>
<li><a href="#3">Related work</a><ul>
<li><a href="3.1">Basic matrix factorization</a></li>
<li><a href="3.2">CircleCon model</a></li>
<li><a href="3.3">ContextMF model</a></li>
</ul>
</li>
<li><a href="4">The approach</a><ul>
<li><a href="4.1">User interest factor</a></li>
<li><a href="4.2">Personalized recommendation model</a></li>
<li><a href="4.3">Model training</a></li>
</ul>
</li>
<li><a href="5">Experiments</a><ul>
<li><a href="5.1">Datasets</a></li>
<li><a href="5.2">Performance measures</a></li>
<li><a href="5.3">Evaluation</a></li>
</ul>
</li>
</ul>
</blockquote>
<h2 id='1'>Introduction</h2>
Recommender system(RS) has been successfully exploited to solve information overload.Many methods proposed to improve the performance of RS.And there are two main problems needed to solve:**1.the problems of cold start for users(new users into the RS with little historical behavior)2.the sparsity of datasets(the proportion of rated user-item pairs in all the user-item pairs of RS)**.

In this paper, three social factors,**personal interest,interpersonal interest similarity,and interpersonal influence**, fuse into a unified personalized recommendation model based on probabilistic matrix factorization.

<h2 id='2'>Problem formulation</h2>



<h1>Code</h1>

<p><a href="https://github.com/daluzi/PRM2" target="_blank" rel="noopener">https://github.com/daluzi/PRM2</a></p>
]]></content>
      <tags>
        <tag>RS</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2019/11/12/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
  </entry>
</search>
