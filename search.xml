<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>test</title>
    <url>/2020/07/12/test/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Docker</title>
    <url>/2020/06/28/Docker/</url>
    <content><![CDATA[<html><head></head><body><h2 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h2><img alt="docker.png" style="zoom: 80%;" data-src="https://i.loli.net/2020/06/28/hnm5d3vaqzFY9sJ.png" class="lazyload">



<h4 id="purpose："><a href="#purpose：" class="headerlink" title="purpose："></a>purpose：</h4><p>​    解决环境配置麻烦的问题。</p>
<h4 id="other-options"><a href="#other-options" class="headerlink" title="other options:"></a>other options:</h4><ul>
<li>虚拟机：资源占用多、冗余步骤多、启动慢。</li>
<li>Linux容器(LXC)：LXC不是模拟一个完整的操作系统，而是对进程进行隔离。即在正常进行的外面套了一个保护层，对于容器里面的进程来说，它接触到的各种资源都是虚拟的。从而实现与底层系统的隔离。启动快、资源占用少、体积小。</li>
</ul>
<hr>
<h4 id="Docker-install"><a href="#Docker-install" class="headerlink" title="Docker install"></a>Docker install</h4><p>​    属于LXC的一种封装，提供简单易用的容器使用接口，是目前最流行的LXC。</p>
<p>​    Docker 将应用程序与该程序的依赖，打包在一个文件里面。运行这个文件，就会生成一个虚拟容器。程序在这个虚拟容器里运行，就好像在真实的物理机上运行一样。有了 Docker，就不用担心环境问题。总体来说，Docker 的接口相当简单，用户可以方便地创建和使用容器，把自己的应用放入容器。容器还可以进行版本管理、复制、分享、修改，就像管理普通的代码一样。</p>
<p>​    Docker 需要用户具有 sudo 权限，为了避免每次命令都输入<code>sudo</code>，可以把用户加入 Docker 用户组（<a href="https://docs.docker.com/install/linux/linux-postinstall/#manage-docker-as-a-non-root-user" target="_blank" rel="noopener">官方文档</a>）。</p>
<blockquote>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ sudo usermod -aG docker <span class="variable">$USER</span></span><br></pre></td></tr></tbody></table></figure>
</blockquote>
<p>​    Docker 是服务器—-客户端架构。命令行运行<code>docker</code>命令的时候，需要本机有 Docker 服务。如果这项服务没有启动，可以用下面的命令启动（<a href="https://docs.docker.com/config/daemon/systemd/" target="_blank" rel="noopener">官方文档</a>）。</p>
<blockquote>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># service 命令的用法</span></span><br><span class="line">$ sudo service docker start</span><br><span class="line"></span><br><span class="line"><span class="comment"># systemctl 命令的用法</span></span><br><span class="line">$ sudo systemctl start docker</span><br></pre></td></tr></tbody></table></figure>
</blockquote>
<h4 id="image文件"><a href="#image文件" class="headerlink" title="image文件"></a>image文件</h4><p>​    <strong>Docker 把应用程序及其依赖，打包在 image 文件里面。</strong>只有通过这个文件，才能生成 Docker 容器。image 文件可以看作是容器的模板。Docker 根据 image 文件生成容器的实例。同一个 image 文件，可以生成多个同时运行的容器实例。</p>
<p>​    image 是二进制文件。实际开发中，一个 image 文件往往通过继承另一个 image 文件，加上一些个性化设置而生成。举例来说，你可以在 Ubuntu 的 image 基础上，往里面加入 Apache 服务器，形成你的 image。</p>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 列出本机的所有 image 文件。</span></span><br><span class="line">$ docker image ls</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除 image 文件</span></span><br><span class="line">$ docker image rm [imageName]</span><br></pre></td></tr></tbody></table></figure>

<p>​    image 文件是通用的，一台机器的 image 文件拷贝到另一台机器，照样可以使用。一般来说，为了节省时间，我们应该尽量使用别人制作好的 image 文件，而不是自己制作。即使要定制，也应该基于别人的 image 文件进行加工，而不是从零开始制作。</p>
<p>​    为了方便共享，image 文件制作完成后，可以上传到网上的仓库。Docker 的官方仓库 <a href="https://hub.docker.com/" target="_blank" rel="noopener">Docker Hub</a> 是最重要、最常用的 image 仓库。此外，出售自己制作的 image 文件也是可以的。</p>
<h4 id="hello-world实例"><a href="#hello-world实例" class="headerlink" title="hello world实例"></a>hello world实例</h4><p>​    首先，运行下面的命令，将 image 文件从仓库抓取到本地。</p>
<blockquote>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ docker image pull library/hello-world</span><br></pre></td></tr></tbody></table></figure>
</blockquote>
<p>​    上面代码中，<code>docker image pull</code>是抓取 image 文件的命令。<code>library/hello-world</code>是 image 文件在仓库里面的位置，其中<code>library</code>是 image 文件所在的组，<code>hello-world</code>是 image 文件的名字。</p>
<p>​    由于 Docker 官方提供的 image 文件，都放在<a href="https://hub.docker.com/r/library/" target="_blank" rel="noopener"><code>library</code></a>组里面，所以它的是默认组，可以省略。因此，上面的命令可以写成下面这样。</p>
<blockquote>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ docker image pull hello-world</span><br></pre></td></tr></tbody></table></figure>
</blockquote>
<p>​    抓取成功以后，就可以在本机看到这个 image 文件了。</p>
<blockquote>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ docker image ls</span><br></pre></td></tr></tbody></table></figure>
</blockquote>
<p>​    现在，运行这个 image 文件。</p>
<blockquote>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ docker container run hello-world</span><br></pre></td></tr></tbody></table></figure>
</blockquote>
<p>​    <code>docker container run</code>命令会从 image 文件，生成一个正在运行的容器实例。</p>
<p>​    注意，<code>docker container run</code>命令具有自动抓取 image 文件的功能。如果发现本地没有指定的 image 文件，就会从仓库自动抓取。因此，前面的<code>docker image pull</code>命令并不是必需的步骤。</p>
<p>如果运行成功，你会在屏幕上读到下面的输出。</p>
<blockquote>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ docker container run hello-world</span><br><span class="line"></span><br><span class="line">Hello from Docker!</span><br><span class="line">This message shows that your installation appears to be working correctly.</span><br><span class="line"></span><br><span class="line">... ...</span><br></pre></td></tr></tbody></table></figure>
</blockquote>
<p>​    输出这段提示以后，<code>hello world</code>就会停止运行，容器自动终止。</p>
<p>​    有些容器不会自动终止，因为提供的是服务。比如，安装运行 Ubuntu 的 image，就可以在命令行体验 Ubuntu 系统。</p>
<blockquote>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ docker container run -it ubuntu bash</span><br></pre></td></tr></tbody></table></figure>
</blockquote>
<p>​    对于那些不会自动终止的容器，必须使用<a href="https://docs.docker.com/engine/reference/commandline/container_kill/" target="_blank" rel="noopener"><code>docker container kill</code></a> 命令手动终止。</p>
<blockquote>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ docker container <span class="built_in">kill</span> [containID]</span><br></pre></td></tr></tbody></table></figure>
</blockquote>
<h4 id="容器文件"><a href="#容器文件" class="headerlink" title="容器文件"></a>容器文件</h4><p>​    <strong>image 文件生成的容器实例，本身也是一个文件，称为容器文件。</strong>也就是说，一旦容器生成，就会同时存在两个文件： image 文件和容器文件。而且关闭容器并不会删除容器文件，只是容器停止运行而已。</p>
<blockquote>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 列出本机正在运行的容器</span></span><br><span class="line">$ docker container ls</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出本机所有容器，包括终止运行的容器</span></span><br><span class="line">$ docker container ls --all</span><br></pre></td></tr></tbody></table></figure>
</blockquote>
<p>​    上面命令的输出结果之中，包括容器的 ID。很多地方都需要提供这个 ID，比如上一节终止容器运行的<code>docker container kill</code>命令。</p>
<p>​    终止运行的容器文件，依然会占据硬盘空间，可以使用<a href="https://docs.docker.com/engine/reference/commandline/container_rm/" target="_blank" rel="noopener"><code>docker container rm</code></a>命令删除。</p>
<blockquote>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ docker container rm [containerID]</span><br></pre></td></tr></tbody></table></figure>
</blockquote>
<p>​    运行上面的命令之后，再使用<code>docker container ls --all</code>命令，就会发现被删除的容器文件已经消失了。</p>
<h4 id="Dockerfile文件"><a href="#Dockerfile文件" class="headerlink" title="Dockerfile文件"></a>Dockerfile文件</h4><p>​    学会使用 image 文件以后，接下来的问题就是，如何可以生成 image 文件？如果你要推广自己的软件，势必要自己制作 image 文件。</p>
<p>​    这就需要用到 Dockerfile 文件。它是一个文本文件，用来配置 image。Docker 根据 该文件生成二进制的 image 文件。</p>
<p>​    下面通过一个实例，演示如何编写 Dockerfile 文件。</p>
<h4 id="实例：制作自己的Docker容器"><a href="#实例：制作自己的Docker容器" class="headerlink" title="实例：制作自己的Docker容器"></a>实例：制作自己的Docker容器</h4><p>​    下面我以 <a href="http://www.ruanyifeng.com/blog/2017/08/koa.html" target="_blank" rel="noopener">koa-demos</a> 项目为例，介绍怎么写 Dockerfile 文件，实现让用户在 Docker 容器里面运行 Koa 框架。</p>
<p>​    作为准备工作，请先<a href="https://github.com/ruanyf/koa-demos/archive/master.zip" target="_blank" rel="noopener">下载源码</a>。</p>
<blockquote>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> https://github.com/ruanyf/koa-demos.git</span><br><span class="line">$ <span class="built_in">cd</span> koa-demos</span><br></pre></td></tr></tbody></table></figure>
</blockquote>
<p>​    <b>1 编写 Dockerfile 文件</b></p>
<p>​    首先，在项目的根目录下，新建一个文本文件<code>.dockerignore</code>，写入下面的<a href="https://github.com/ruanyf/koa-demos/blob/master/.dockerignore" target="_blank" rel="noopener">内容</a>。</p>
<blockquote>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">.git</span><br><span class="line">node_modules</span><br><span class="line">npm-debug.log</span><br></pre></td></tr></tbody></table></figure>
</blockquote>
<p>​    上面代码表示，这三个路径要排除，不要打包进入 image 文件。如果你没有路径要排除，这个文件可以不新建。</p>
<p>​    然后，在项目的根目录下，新建一个文本文件 Dockerfile，写入下面的<a href="https://github.com/ruanyf/koa-demos/blob/master/Dockerfile" target="_blank" rel="noopener">内容</a>。</p>
<blockquote>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">FROM node:8.4</span><br><span class="line">COPY . /app</span><br><span class="line">WORKDIR /app</span><br><span class="line">RUN npm install --registry=https://registry.npm.taobao.org</span><br><span class="line">EXPOSE 3000</span><br></pre></td></tr></tbody></table></figure>
</blockquote>
<p>​    上面代码一共五行，含义如下。</p>
<blockquote>
<ul>
<li><code>FROM node:8.4</code>：该 image 文件继承官方的 node image，冒号表示标签，这里标签是<code>8.4</code>，即8.4版本的 node。</li>
<li><code>COPY . /app</code>：将当前目录下的所有文件（除了<code>.dockerignore</code>排除的路径），都拷贝进入 image 文件的<code>/app</code>目录。</li>
<li><code>WORKDIR /app</code>：指定接下来的工作路径为<code>/app</code>。</li>
<li><code>RUN npm install</code>：在<code>/app</code>目录下，运行<code>npm install</code>命令安装依赖。注意，安装后所有的依赖，都将打包进入 image 文件。</li>
<li><code>EXPOSE 3000</code>：将容器 3000 端口暴露出来， 允许外部连接这个端口。</li>
</ul>
</blockquote>
<p>​    <b>10.2 创建 image 文件</b></p>
<p>​    有了 Dockerfile 文件以后，就可以使用<code>docker image build</code>命令创建 image 文件了。</p>
<blockquote>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ docker image build -t koa-demo .</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">$ docker image build -t koa-demo:0.0.1 .</span><br></pre></td></tr></tbody></table></figure>
</blockquote>
<p>​    上面代码中，<code>-t</code>参数用来指定 image 文件的名字，后面还可以用冒号指定标签。如果不指定，默认的标签就是<code>latest</code>。最后的那个点表示 Dockerfile 文件所在的路径，上例是当前路径，所以是一个点。</p>
<p>​    如果运行成功，就可以看到新生成的 image 文件<code>koa-demo</code>了。</p>
<blockquote>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ docker image ls</span><br></pre></td></tr></tbody></table></figure>
</blockquote>
<p>​    <b>10.3 生成容器</b></p>
<p>​    <code>docker container run</code>命令会从 image 文件生成容器。</p>
<blockquote>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ docker container run -p 8000:3000 -it koa-demo /bin/bash</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">$ docker container run -p 8000:3000 -it koa-demo:0.0.1 /bin/bash</span><br></pre></td></tr></tbody></table></figure>
</blockquote>
<p>​    上面命令的各个参数含义如下：</p>
<blockquote>
<ul>
<li><code>-p</code>参数：容器的 3000 端口映射到本机的 8000 端口。</li>
<li><code>-it</code>参数：容器的 Shell 映射到当前的 Shell，然后你在本机窗口输入的命令，就会传入容器。</li>
<li><code>koa-demo:0.0.1</code>：image 文件的名字（如果有标签，还需要提供标签，默认是 latest 标签）。</li>
<li><code>/bin/bash</code>：容器启动以后，内部第一个执行的命令。这里是启动 Bash，保证用户可以使用 Shell。</li>
</ul>
</blockquote>
<p>​    如果一切正常，运行上面的命令以后，就会返回一个命令行提示符。</p>
<blockquote>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">root@66d80f4aaf1e:/app<span class="comment">#</span></span><br></pre></td></tr></tbody></table></figure>
</blockquote>
<p>​    这表示你已经在容器里面了，返回的提示符就是容器内部的 Shell 提示符。执行下面的命令。</p>
<blockquote>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">root@66d80f4aaf1e:/app<span class="comment"># node demos/01.js</span></span><br></pre></td></tr></tbody></table></figure>
</blockquote>
<p>​    这时，Koa 框架已经运行起来了。打开本机的浏览器，访问 <a href="http://127.0.0.1:8000，网页显示"Not" target="_blank" rel="noopener">http://127.0.0.1:8000，网页显示"Not</a> Found”，这是因为这个 <a href="https://github.com/ruanyf/koa-demos/blob/master/demos/01.js" target="_blank" rel="noopener">demo</a> 没有写路由。</p>
<p>​    这个例子中，Node 进程运行在 Docker 容器的虚拟环境里面，进程接触到的文件系统和网络接口都是虚拟的，与本机的文件系统和网络接口是隔离的，因此需要定义容器与物理机的端口映射（map）。</p>
<p>​    现在，在容器的命令行，按下 Ctrl + c 停止 Node 进程，然后按下 Ctrl + d （或者输入 exit）退出容器。此外，也可以用<code>docker container kill</code>终止容器运行。</p>
<blockquote>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 在本机的另一个终端窗口，查出容器的 ID</span></span><br><span class="line">$ docker container ls</span><br><span class="line"></span><br><span class="line"><span class="comment"># 停止指定的容器运行</span></span><br><span class="line">$ docker container <span class="built_in">kill</span> [containerID]</span><br></pre></td></tr></tbody></table></figure>
</blockquote>
<p>​    容器停止运行之后，并不会消失，用下面的命令删除容器文件。</p>
<blockquote>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 查出容器的 ID</span></span><br><span class="line">$ docker container ls --all</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除指定的容器文件</span></span><br><span class="line">$ docker container rm [containerID]</span><br></pre></td></tr></tbody></table></figure>
</blockquote>
<p>​    也可以使用<code>docker container run</code>命令的<code>--rm</code>参数，在容器终止运行后自动删除容器文件。</p>
<blockquote>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ docker container run --rm -p 8000:3000 -it koa-demo /bin/bash</span><br></pre></td></tr></tbody></table></figure>
</blockquote>
<p>​    <b>10.4 CMD 命令</b></p>
<p>​    上一节的例子里面，容器启动以后，需要手动输入命令<code>node demos/01.js</code>。我们可以把这个命令写在 Dockerfile 里面，这样容器启动以后，这个命令就已经执行了，不用再手动输入了。</p>
<blockquote>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">FROM node:8.4</span><br><span class="line">COPY . /app</span><br><span class="line">WORKDIR /app</span><br><span class="line">RUN npm install --registry=https://registry.npm.taobao.org</span><br><span class="line">EXPOSE 3000</span><br><span class="line">CMD node demos/01.js</span><br></pre></td></tr></tbody></table></figure>
</blockquote>
<p>​    上面的 Dockerfile 里面，多了最后一行<code>CMD node demos/01.js</code>，它表示容器启动后自动执行<code>node demos/01.js</code>。</p>
<p>​    你可能会问，<code>RUN</code>命令与<code>CMD</code>命令的区别在哪里？简单说，<code>RUN</code>命令在 image 文件的构建阶段执行，执行结果都会打包进入 image 文件；<code>CMD</code>命令则是在容器启动后执行。另外，一个 Dockerfile 可以包含多个<code>RUN</code>命令，但是只能有一个<code>CMD</code>命令。</p>
<p>​    注意，指定了<code>CMD</code>命令以后，<code>docker container run</code>命令就不能附加命令了（比如前面的<code>/bin/bash</code>），否则它会覆盖<code>CMD</code>命令。现在，启动容器可以使用下面的命令。</p>
<blockquote>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ docker container run --rm -p 8000:3000 -it koa-demo:0.0.1</span><br></pre></td></tr></tbody></table></figure>
</blockquote>
<p>​    <b>10.5 发布 image 文件</b></p>
<p>​    容器运行成功后，就确认了 image 文件的有效性。这时，我们就可以考虑把 image 文件分享到网上，让其他人使用。</p>
<p>​    首先，去 <a href="https://hub.docker.com/" target="_blank" rel="noopener">hub.docker.com</a> 或 <a href="https://cloud.docker.com/" target="_blank" rel="noopener">cloud.docker.com</a> 注册一个账户。然后，用下面的命令登录。</p>
<blockquote>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ docker login</span><br></pre></td></tr></tbody></table></figure>
</blockquote>
<p>​    接着，为本地的 image 标注用户名和版本。</p>
<blockquote>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ docker image tag [imageName] [username]/[repository]:[tag]</span><br><span class="line"><span class="comment"># 实例</span></span><br><span class="line">$ docker image tag koa-demos:0.0.1 ruanyf/koa-demos:0.0.1</span><br></pre></td></tr></tbody></table></figure>
</blockquote>
<p>​    也可以不标注用户名，重新构建一下 image 文件。</p>
<blockquote>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ docker image build -t [username]/[repository]:[tag] .</span><br></pre></td></tr></tbody></table></figure>
</blockquote>
<p>​    最后，发布 image 文件。</p>
<blockquote>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ docker image push [username]/[repository]:[tag]</span><br></pre></td></tr></tbody></table></figure>
</blockquote>
<p>​    发布成功以后，登录 hub.docker.com，就可以看到已经发布的 image 文件。</p>
<h4 id="其他有用的命令"><a href="#其他有用的命令" class="headerlink" title="其他有用的命令"></a>其他有用的命令</h4><p>​    docker 的主要用法就是上面这些，此外还有几个命令，也非常有用。</p>
<p>​    <strong>（1）docker container start</strong></p>
<p>​    前面的<code>docker container run</code>命令是新建容器，每运行一次，就会新建一个容器。同样的命令运行两次，就会生成两个一模一样的容器文件。如果希望重复使用容器，就要使用<code>docker container start</code>命令，它用来启动已经生成、已经停止运行的容器文件。</p>
<blockquote>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ docker container start [containerID]</span><br></pre></td></tr></tbody></table></figure>
</blockquote>
<p>​    <strong>（2）docker container stop</strong></p>
<p>​    前面的<code>docker container kill</code>命令终止容器运行，相当于向容器里面的主进程发出 SIGKILL 信号。而<code>docker container stop</code>命令也是用来终止容器运行，相当于向容器里面的主进程发出 SIGTERM 信号，然后过一段时间再发出 SIGKILL 信号。</p>
<blockquote>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ bash container stop [containerID]</span><br></pre></td></tr></tbody></table></figure>
</blockquote>
<p>​    这两个信号的差别是，应用程序收到 SIGTERM 信号以后，可以自行进行收尾清理工作，但也可以不理会这个信号。如果收到 SIGKILL 信号，就会强行立即终止，那些正在进行中的操作会全部丢失。</p>
<p>​    <strong>（3）docker container logs</strong></p>
<p><code>docker container logs</code>命令用来查看 docker 容器的输出，即容器里面 Shell 的标准输出。如果<code>docker run</code>命令运行容器的时候，没有使用<code>-it</code>参数，就要用这个命令查看输出。</p>
<blockquote>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ docker container logs [containerID]</span><br></pre></td></tr></tbody></table></figure>
</blockquote>
<p>​    <strong>（4）docker container exec</strong></p>
<p>​    <code>docker container exec</code>命令用于进入一个正在运行的 docker 容器。如果<code>docker run</code>命令运行容器的时候，没有使用<code>-it</code>参数，就要用这个命令进入容器。一旦进入了容器，就可以在容器的 Shell 执行命令了。</p>
<blockquote>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ docker container <span class="built_in">exec</span> -it [containerID] /bin/bash</span><br></pre></td></tr></tbody></table></figure>
</blockquote>
<p>​    <strong>（5）docker container cp</strong></p>
<p>​    <code>docker container cp</code>命令用于从正在运行的 Docker 容器里面，将文件拷贝到本机。下面是拷贝到当前目录的写法。</p>
<blockquote>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ docker container cp [containID]:[/path/to/file] .</span><br></pre></td></tr></tbody></table></figure>
</blockquote>
<h2 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h2><p>​    Windows上安装了docker之后，netkeeper报如下错误：</p>
<p>​    <b>sorry,this application cannot be run under a Virtual Machine</b></p>
<img alt="error.png" style="zoom:50%;" data-src="https://i.loli.net/2020/06/29/bpL5eSUHEQGAOtT.png" class="lazyload">

<p>​    搜索了到如下几种参考方法(直接看第三种即可)：</p>
<p>​    1.开机时打开BIOS，关闭virtual tchnology这一选项,具体操作参考下面链接(<a href="http://www.udaxia.com/upqd/5254.html).但对于我没法尝试.(失败)" target="_blank" rel="noopener">http://www.udaxia.com/upqd/5254.html).但对于我没法尝试.(失败)</a><br>​    2.在控制面板→程序→启用或关闭 Windows 功能下,取消Hyper-V选项,重启电脑,参考下面链接（<a href="https://blog.csdn.net/sdut15110581043/article/details/53330481/).尝试无数遍,重启无数遍,均失败.(失败)" target="_blank" rel="noopener">https://blog.csdn.net/sdut15110581043/article/details/53330481/).尝试无数遍,重启无数遍,均失败.(失败)</a></p>
<p>​    <b>可忽略上面失败的解决方案</b><br>​    3.禁用Hyper-V正确姿势来了(第二种方法不行)<br>​    ①打开Windows PowerShell（管理员）</p>
<p>​    ②运行命令bcdedit /set hypervisorlaunchtype off<br>​    注意:上述命令有一个空格的存在</p>
<p>​    ③重启计算机</p>
<p>​    如果要重新开启Hyper-V，按照方法3执行bcdedit / set hypervisorlaunchtype auto 命令并重启计算机即可。</p>
<p>原文链接：<a href="https://blog.csdn.net/qq_43942195/article/details/88600624" target="_blank" rel="noopener">https://blog.csdn.net/qq_43942195/article/details/88600624</a></p>
<h4 id="reference"><a href="#reference" class="headerlink" title="reference:"></a>reference:</h4><ul>
<li><a href="https://docs.docker.com/" target="_blank" rel="noopener">Docker官网</a></li>
<li><a href="http://www.ruanyifeng.com/blog/2018/02/docker-tutorial.html" target="_blank" rel="noopener">阮一峰的Dockers入门</a></li>
</ul>
</body></html>]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>第二章：推荐系统的进化之路</title>
    <url>/2020/06/27/%E7%AC%AC%E4%BA%8C%E7%AB%A0/</url>
    <content><![CDATA[<html><head></head><body><h2>一、整体框架</h2>

<p><img alt="1.png" data-src="https://i.loli.net/2020/07/11/vndB6QLsTbFJNGg.jpg" class="lazyload"></p>
<p>传统推荐系统的发展脉络主要由以下四个部分组成：</p>
<ul>
<li><b>协同过滤算法族：</b>物品协同过滤（ItemCF）、用户协同过滤（UserCF）、矩阵分解模型（Matrix Factorization）及各分支模型；</li>
<li><b>逻辑回归模型族：</b>逻辑回归能够利用和融合更多用户、物品和上下文特征。LR模型、LS-PLM模型等；</li>
<li><b>因子分解机模型族：</b>在传统逻辑回归的基础上，加入了二阶部分，使模型具备了进行<font color="red">特征组合</font>的能力。FM模型、FFM模型；</li>
<li><b>组合模型：</b>为了融合多个模型的优点，将不同模型组合使用是构建推荐模型常用的方法。GBDT+LR等。</li>
</ul>
<h2>二、协同过滤</h2>

<p><em>协同过滤的提出：2003年Amazon发表文章<a href="https://ieeexplore.ieee.org/document/1167344" target="_blank" rel="noopener">Amazon.com recommendations: item-to-item collaborative filtering</a></em></p>
<h4 id="2-1-定义"><a href="#2-1-定义" class="headerlink" title="2.1 定义"></a>2.1 定义</h4><p>协同大家的反馈、评价和意见一起对海量的信息进行过滤，从中筛选出目标用户可能感兴趣的信息的推荐过程。</p>
<p><img alt="366139DF-46C7-47B9-A03F-7237B62B0EEA.png" data-src="https://i.loli.net/2020/07/11/Xjb4Q8YEUoDknJ3.jpg" class="lazyload"></p>
<p><b>共现矩阵：</b>以上图为例，将有向图转成矩阵的形式，用户作为矩阵行坐标，商品作为列坐标，将“点赞”和“踩”的用户行为数据转换为矩阵中对应的元素值。</p>
<h4 id="2-2-用户相似度计算"><a href="#2-2-用户相似度计算" class="headerlink" title="2.2 用户相似度计算"></a>2.2 用户相似度计算</h4><p>对于用户向量<code>i</code>和用户向量<code>j</code></p>
<ol>
<li>余弦相似度：$sim(i,j)=cos(i,j)=\frac{i·j}{||i||·||j||}$</li>
<li>皮尔逊相关系数：<br>$$sim(i,j)=\frac{\Sigma_{p\in P}(R_{i,P} - \hat{R}<em>i)(R</em>{j,P}-\hat{R}<em>j)}{\sqrt[]{\Sigma</em>{p\in P}(R_{i,P}- \hat{R}<em>i)^2}\sqrt[]{\Sigma</em>{p\in P}(R_{j,P}- \hat{R}<em>j)^2}}$$<br>其中，`R</em>{i,p}<code>代表用户</code>i<code>对物品</code>p<code>的评分，$\hat{R}_i$是代表用户</code>i<code>对所有物品的平均评分，</code>p`代表所有物品的集合。</li>
<li>基于皮尔逊系数的思路，还可以通过引入物品平均分的方式，<b>减少物品评分偏置对结果的影响:</b><br>$$sim(i,j)=\frac{\Sigma_{p\in P}(R_{i,P} - \hat{R}<em>P)(R</em>{j,P}-\hat{R}<em>P)}{\sqrt[]{\Sigma</em>{p\in P}(R_{i,P}- \hat{R}<em>P)^2}\sqrt[]{\Sigma</em>{p\in P}(R_{j,P}- \hat{R}_P)^2}}$$<br>其中，$\hat{R}_p$代表物品<code>p</code>得到所有评分的平均分。</li>
</ol>
<h4>2.3 最终结果的排序</h4>

<p>假设“目标用户与其相似用户的喜好是相似的”</p>
<p>最常用的方式是<b>利用用户相似度和相似用户的评价的加权平均获得目标用户的评分预测</b>，如下：</p>
<p>$$R_{u,p}=\frac{\Sigma_{s\in S}(W_{u,s}·R_{s,p})}{\Sigma_{s\in S}W_{u,s}}$$</p>
<p>其中，权重$W_{u,s}$是用户<code>u</code>和用户<code>s</code>的相似度，$R_{s,P}$是用户<code>s</code>对物品<code>p</code>的评分。</p>
<h4>2.4 ItemCF</h4>

<p>ItemCF是基于物品相似度进行推荐的协同过滤算法。通过计算共现矩阵中物品列向量的相似度得到物品之间的相似矩阵，再找到用户的历史正反馈物品的相似物品进行进一步排序和推荐。</p>
<ol>
<li>基于历史数据，构建以用户（假设用户总数为m）为行坐标，物品（物品总数为n）为列坐标的<code>m * n</code>维的共现矩阵。</li>
<li>计算共现矩阵两两列向量间的相似性（相似度的计算方式与用户相似度的计算方式相同），构建<code>n * n</code>维的物品相似度矩阵。</li>
<li>获得用户历史行为数据中的正反馈物品列表。</li>
<li>利用物品相似度矩阵，针对目标用户历史行为中的正反馈物品，找出相似的Top k个物品，组成相似物品集合。</li>
<li>对相似物品集合中的物品，利用相似度分值进行排序，生成最终的推荐列表。</li>
</ol>
<h4 id="2-5-CF的缺点"><a href="#2-5-CF的缺点" class="headerlink" title="2.5 CF的缺点"></a>2.5 CF的缺点</h4><ul>
<li>CF的天然缺陷：推荐结果的头部效应较明显，处理稀疏向量的能力弱。</li>
<li>CF仅利用用户和物品的交互信息，无法有效地引入用户年龄、性别、商品描述、商品分类、当前时间等一系列用户特征、物品特征和上下文特征。</li>
</ul>
<h2>三、矩阵分解</h2>

<p>矩阵分解在CF中“共现矩阵”的基础上，加入了<b>隐向量</b>的概念，加强了模型处理稀疏矩阵的能力。</p>
<img alt="A9DA0BC6-B77A-485E-8A5D-DE44BE271712.png" style="zoom:50%;" data-src="https://i.loli.net/2020/07/11/bFJ4WoyBGTvUDpg.jpg" class="lazyload">

<h4>3.1矩阵分解的求解过程</h4>

<p>主要方法有三种：</p>
<ul>
<li>特征值分解（Eigen Decomposition）：只能作用于方阵</li>
<li>奇异值分解（singular Value Decomposition, SVD）：<img alt="6812192D-0640-4957-90E1-9A131EF5B6A9.png" data-src="https://i.loli.net/2020/07/11/Uzw2Id8ER1t3Prx.jpg" class="lazyload"></li>
<li>梯度下降（Gradient Descent）:<br>求解矩阵分解的目标函数：<br>$$min_{q^<em>,p^</em>}\Sigma_{(u,i)\in K}(r_{u,i}-q_i^Tp_u)^2+\lambda(||q_i||+||p_u||)^2$$</li>
</ul>
<p>在矩阵分解算法中，由于隐向量的存在，使人意的用户和物品之间都可以得到预测分值。而隐向量的生成过程其实是对共现矩阵进行全局拟合的过程，因此隐向量其实是利用全局信息生成的，有更强的泛化能力。</p>
<h4 id="3-2-MF优缺点"><a href="#3-2-MF优缺点" class="headerlink" title="3.2 MF优缺点"></a>3.2 MF优缺点</h4><ul>
<li><p>优点：</p>
<ol>
<li>泛化能力强</li>
<li>空间复杂度低：(n+m)·k</li>
<li>更好的扩展性和灵活性</li>
</ol>
</li>
<li><p>缺点：</p>
<ol>
<li>MF同样不方便加入用户、物品和上下文相关的特征</li>
<li>在缺乏用户历史行为时，无法进行有效的推荐</li>
</ol>
</li>
</ul>
<h2>四、逻辑回归</h2>

</body></html>]]></content>
      <categories>
        <category>《深度学习推荐系统》</category>
      </categories>
      <tags>
        <tag>recommendation</tag>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title>第一章</title>
    <url>/2020/06/27/%E7%AC%AC%E4%B8%80%E7%AB%A0/</url>
    <content><![CDATA[<h2>第一章、互联网的增长引擎---推荐系统</h2>

<hr>
<h4>1.1 为什么推荐系统是互联网的增长引擎</h4>

<p>用户角度：推荐系统解决在“信息过载”的情况下，用户如何高效获得感兴趣信息的问题。从用户需求层面看，推荐系统是在用户需求并不十分明确的情况下，进行信息的过滤。更多的是利用用户的各类历史信息“猜测”其可能喜欢的内容。</p>
<p>公司角度：推荐系统解决产品能够最大限度地吸引用户、留存用户、增加用户黏性、提高用户转化率的问题。</p>
<h4>1.2 推荐系统的架构</h4>

<p>两个部分：</p>
<ul>
<li>“数据和信息”部分逐渐发展为推荐系统中融合了数据离线批处理、实时流处理的数据流框架；</li>
<li>“算法和模型”部分则进一步细化为推荐系统中集训练（training）、评估（evaluation）、部署（deployment）、线上推断（online inference）为一体的模型框架。<b>召回层、排序层、补充策略于算法层</b></li>
</ul>
]]></content>
      <categories>
        <category>《深度学习推荐系统》</category>
      </categories>
      <tags>
        <tag>recommendation</tag>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title>KG_learning</title>
    <url>/2020/06/22/KG-learning/</url>
    <content><![CDATA[<html><head></head><body><h3>知识图谱</h3>

<hr>
<p><em>以结构化的形式描述客观世界中的概念、实体及其关系，将互联网的信息表达成更接近人类认知世界的形式，提供了一种更好的组织、管理和理解互联网海量信息的能力。</em></p>
<p><b>Google</b>首先提出。使得谷歌的搜索结果能给出更精准的答案。</p>
<p>知识图谱推理，知识图谱可视化</p>
<p>机器学习、图数据库（如Neo4j）、自然语言等技术的成熟，使得知识图谱变火。</p>
<p>语义网络->本体论->web->语义网（大规模,也即知识图谱）</p>
<p>————————-分割线———————-</p>
<p><b>典型的知识图谱：</b></p>
<p>按照知识的主客观性：事实性知识、主观性知识</p>
<p>根据知识的变化性质：静态知识、动态</p>
<p>根据领域知识：通用知识、领域知识</p>
<p>因此常见的知识图谱：</p>
<ul>
<li>基于专家知识/人工构建：1. Cyc；2.WordNet</li>
<li>基于众包数据和其他知识图谱：1.ConceptNet; 2.YAGO; 3.Wikidata; 4.BDpedia; 5.Freebase; 6.BabelNet</li>
<li>基于机器学习：1.NELL; 2.Knowledge Vault; 3. WOE; 4.ReVerb</li>
<li>企业知识图谱：1.Google KG; 2.百度知心; 3.搜狗知立方</li>
</ul>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">graph LR</span><br><span class="line">A[常见的知识图谱] --> B{基于专家知识/人工构建} </span><br><span class="line">A[常见的知识图谱] --> B1{基于众包数据和其他知识图谱}</span><br><span class="line">A[常见的知识图谱] --> B2{基于机器学习} </span><br><span class="line">A[常见的知识图谱] --> B3{企业知识图谱} </span><br><span class="line">		B -->  C[Cyc]</span><br><span class="line">		B -->  D[WordNet]</span><br><span class="line">		B1 --> C1[ConceptNet]</span><br><span class="line">		B1 --> C2[YAGO]		</span><br><span class="line">		B1 --> C3[Wikidata]		</span><br><span class="line">		B1 --> C4[BDpedia]		</span><br><span class="line">		B1 --> C5[Freebase]		</span><br><span class="line">		B1 --> C6[BabelNet]</span><br><span class="line">    B2 --> C7[NELL]</span><br><span class="line">    B2 --> C8[Knowledge Vault]    </span><br><span class="line">    B2 --> C9[WOE]    </span><br><span class="line">    B2 --> C10[Reverb]</span><br><span class="line">    B3 --> C11[Google KG]</span><br><span class="line">    B3 --> C12[百度知心]</span><br><span class="line">    B3 --> C13[搜狗知立方]</span><br></pre></td></tr></tbody></table></figure>



<p><b>知识图谱的主要技术</b></p>
<ul>
<li><p>知识问答</p>
</li>
<li><p>语义搜索</p>
</li>
<li><p>可视化</p>
</li>
<li><p>知识链接</p>
</li>
<li><p>知识推理：知识补全、自动问答系统</p>
</li>
<li><p>知识众包</p>
</li>
<li><p>知识融合：对不同来源、不同语言或结构的知识进行融合，从而对已有知识图谱进行补充、更新或去重。</p>
</li>
<li><p>知识抽取</p>
</li>
<li><p>知识表示</p>
</li>
<li><p>知识存储：研究采用何种方式将已有的知识图谱进行存储。基于图的数据结构，存储方式主要是两种形式：1，RDF格式存储—Apache Jena；2，图数据库——Neo4j</p>
</li>
</ul>
<p>————————-分割线———————-</p>
<p><b>知识表示和建模</b></p>
<p>一阶谓词逻辑、产生式系统、框架表示法、语义网络</p>
<p><em>（语义网络不等价于语义网，语义网络中的弧表示各种语义联系，指明它所连接的节点间某种语义关系。语义网络的推理规则不十分明了。<b>语义网络重在知识的表示，对实体间的关系等不太能表示，所以和知识图谱不同，大的语义网即为知识图谱</b>）</em></p>
<p>RDF（Resource Description Framework 资源描述框架）本质上是一组表示知识的语法，由若干个三元组的组合表达。</p>
<p>RDF序列化(即存储)：方式主要有：RDF/XML, N-Triples, Turtle, RDFa, JSON-LD等</p>
<p>RDF和RDFS</p>
<p>OWL（网络本体语言）：OWL Lite \ OWL DL \ OWL FULL</p>
<p>SPARQL语言、cypher QL语言</p>
<p>————————-分割线———————-</p>
<p><b>知识图谱数据存储</b></p>
<p>方式：</p>
<ul>
<li><p>基于关系型数据库的存储方式</p>
</li>
<li><p>面向RDF的存储方式</p>
</li>
<li><p>图数据库存储方式</p>
</li>
</ul>
<p>————————-分割线———————-</p>
<p><b>PageRank</b></p>
<p>一个网页i的重要度可以使用指向网页i的其他网页j的重要度加权得到。</p>
<img alt="98A3E36B-2A01-42C4-9435-C658C3CA5294.png" style="zoom:50%;" data-src="https://i.loli.net/2020/06/22/iEIjbymsGqJtgCp.jpg" class="lazyload">

<p><b>TextRank</b></p>
<p>将PageRank的“词”改成“句子”</p>
<p><b>BRAT</b></p>
<p><a href="http://brat.nlplab.org" target="_blank" rel="noopener">http://brat.nlplab.org</a></p>
<p>用于人工做实体标注的工具</p>
<p><b>word embedding</b></p>
<p><b>word2vec</b></p>
<p><b>Skipgram</b></p>
<p><img alt="skipgram.png" data-src="https://i.loli.net/2020/06/22/iWensqMk6bSpEog.jpg" class="lazyload"></p>
</body></html>]]></content>
      <categories>
        <category>KG</category>
      </categories>
      <tags>
        <tag>KG</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode record</title>
    <url>/2020/06/01/leetcode-record/</url>
    <content><![CDATA[<html><head></head><body><h4 id="974-和可被K整除的子数组-中等"><a href="#974-和可被K整除的子数组-中等" class="headerlink" title="974.和可被K整除的子数组(中等)"></a>974.和可被K整除的子数组(中等)</h4><hr>
<p>给定一个整数数组 <code>A</code>，返回其中元素之和可被 <code>K</code> 整除的（连续、非空）子数组的数目。</p>
<p>示例：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">输入：A = [4,5,0,-2,-3,1], K = 5</span><br><span class="line">输出：7</span><br><span class="line">解释：</span><br><span class="line">有 7 个子数组满足其元素之和可被 K = 5 整除：</span><br><span class="line">[4, 5, 0, -2, -3, 1], [5], [5, 0], [5, 0, -2, -3], [0], [0, -2, -3], [-2, -3]</span><br></pre></td></tr></tbody></table></figure>

<p>思路：</p>
<p> 通常，涉及连续子数组问题的时候，使用前缀和来解决。</p>
<p> 我们令 P[i]=A[0]+A[1]+…+A[i] 。那么每个连续子数组的和sum(i,j) 就可以写成P[j]−P<a href="0<i<j">i</a> 的形式。此时，判断子数组的和能否被K整除就等价于判断(P[j]−P[i−1])modK==0，根据同余定理，只要P[j]modK==P[i−1]modK就行。</p>
<p> 因此我们可以考虑对数组进行遍历，在遍历同时统计答案。当我们遍历到第 i个元素时，我们统计以 i 结尾的符合条件的子数组个数。我们可以维护一个以前缀和模 K 的值为键，出现次数为值的哈希表 record，在遍历的同时进行更新。这样在计算以 i结尾的符合条件的子数组个数时，根据上面的分析，答案即为[0..i−1] 中前缀和模K也为P[i]modK的位置个数，即record[P[i]modK]。</p>
<p> 最后的答案即为以每一个位置为数尾的符合条件的子数组个数之和。需要注意的一个边界条件是，我们需要对哈希表初始化，记录record[0]=1，这样就考虑了前缀和本身被 K 整除的情况。</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def subarraysDivByK(self, A: List[int], K: int) -> int:</span><br><span class="line">        record = {0: 1}</span><br><span class="line">        total, ans = 0, 0</span><br><span class="line">        for elem in A:</span><br><span class="line">            total += elem</span><br><span class="line">            modulus = total % K</span><br><span class="line">            same = record.get(modulus, 0)</span><br><span class="line">            ans += same</span><br><span class="line">            record[modulus] = same + 1</span><br><span class="line">        return ans</span><br></pre></td></tr></tbody></table></figure>

<p>复杂度分析</p>
<ul>
<li>时间复杂度：O(N)，其中 N 是数组 A 的长度。我们只需要从前往后遍历一次数组，在遍历数组的过程中，维护哈希表的各个操作均为 O(1)，因此总时间复杂度为 O(N)。</li>
<li>空间复杂度：O(min(N,K))，即哈希表需要的空间。当 N≤K 时，最多有N 个前缀和，因此哈希表中最多有N+1个键值对；当 N>K 时，最多有K 个不同的余数，因此哈希表中最多有 K 个键值对。也就是说，哈希表需要的空间取决于N 和 K中的较小值。</li>
</ul>
<h4 id="394-字符串解码-中等"><a href="#394-字符串解码-中等" class="headerlink" title="394.字符串解码(中等)"></a>394.字符串解码(中等)</h4><hr>
<p>给定一个经过编码的字符串，返回它解码后的字符串。</p>
<p>编码规则为:k[encodedstring]，表示其中方括号内部的 encodedstring 正好重复 k 次。注意 k 保证为正整数。你可以认为输入字符串总是有效的；输入字符串中没有额外的空格，且输入的方括号总是符合格式要求的。此外，你可以认为原始数据不包含数字，所有的数字只表示重复的次数 k ，例如不会出现像 3a 或 2[4]的输入。</p>
<p>示例:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">s = "3[a]2[bc]", 返回 "aaabcbc".</span><br><span class="line">s = "3[a2[c]]", 返回 "accaccacc".</span><br><span class="line">s = "2[abc]3[cd]ef", 返回 "abcabccdcdcdef".</span><br></pre></td></tr></tbody></table></figure>

<p>用栈的思想：</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def decodeString(self, s: str) -> str:</span><br><span class="line">        stack = [['', 1]]</span><br><span class="line">        num = ''</span><br><span class="line">        for c in s:</span><br><span class="line">            if c.isdigit():</span><br><span class="line">                num += c</span><br><span class="line">            elif c == '[':</span><br><span class="line">                stack.append(['', int(num)])</span><br><span class="line">                num = ''</span><br><span class="line">            elif c == ']':</span><br><span class="line">                subs, k = stack.pop()</span><br><span class="line">                stack[-1][0] += subs * k</span><br><span class="line">            else:</span><br><span class="line">                stack[-1][0] += c</span><br><span class="line">        return stack[0][0] * stack[0][1]</span><br></pre></td></tr></tbody></table></figure>

<h4 id="198-打家劫舍-中等"><a href="#198-打家劫舍-中等" class="headerlink" title="198.打家劫舍(中等)"></a>198.打家劫舍(中等)</h4><hr>
<p>你是一个专业的小偷，计划偷窃沿街的房屋。每间房内都藏有一定的现金，影响你偷窃的唯一制约因素就是相邻的房屋装有相互连通的防盗系统，如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警。给定一个代表每个房屋存放金额的非负整数数组，计算你 不触动警报装置的情况下 ，一夜之内能够偷窃到的最高金额。</p>
<p>示例 1:</p>
<p>输入: [1,2,3,1] 输出: 4 解释: 偷窃 1 号房屋 (金额 = 1) ，然后偷窃 3 号房屋 (金额 = 3)。 偷窃到的最高金额 = 1 + 3 = 4 。 示例 2:</p>
<p>输入: [2,7,9,3,1] 输出: 12 解释: 偷窃 1 号房屋 (金额 = 2), 偷窃 3 号房屋 (金额 = 9)，接着偷窃 5 号房屋 (金额 = 1)。 偷窃到的最高金额 = 2 + 9 + 1 = 12 。</p>
<p><strong>思路：</strong></p>
<p>动态规划+滚动数组：</p>
<p>首先考虑最简单的情况：如果只有一个房间，则偷窃该房屋，可以偷窃到最高总金额。如果只有两个房间，则由于两间房屋相邻，不能同时偷窃，只能偷窃其中的一间屋子，因此选择其中金额较高的房屋进行偷窃，可以偷窃到最高总金额。</p>
<p>如果房屋数量大于两间，应该如何计算能够偷窃到的最高总金额呢？对于第 k(k>2) 间房屋，有两个选项：</p>
<p>偷窃第 k 间房屋，那么就不能偷窃第 k−1 间房屋，偷窃总金额为前 k−2 间房屋的最高总金额与第 k 间房屋的金额之和。</p>
<p>不偷窃第k 间房屋，偷窃总金额为前 k−1间房屋的最高总金额。</p>
<p>在两个选项中选择偷窃总金额较大的选项，该选项对应的偷窃总金额即为前 k 间房屋能偷窃到的最高总金额。</p>
<p>用 dp[i]表示前 i间房屋能偷窃到的最高总金额，那么就有如下的状态转移方程：</p>
<p>dp[i]=max(dp[i−2]+nums[i],dp[i−1])</p>
<p>边界条件为：</p>
<p>{dp[0]=nums[0]只有一间房屋，则偷窃该房屋 dp[1]=max(nums[0],nums[1])只有两间房屋，选择其中金额较高的房屋进行偷窃</p>
<p>只有一间房屋，则偷窃该房屋 只有两间房屋，选择其中金额较高的房屋进行偷窃 最终的答案即为 dp[n−1]，其中 n 是数组的长度。</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def rob(self, nums: List[int]) -> int:</span><br><span class="line">        if not nums:</span><br><span class="line">            return 0</span><br><span class="line"></span><br><span class="line">        size = len(nums)</span><br><span class="line">        if size == 1:</span><br><span class="line">            return nums[0]</span><br><span class="line">        </span><br><span class="line">        dp = [0] * size</span><br><span class="line">        dp[0] = nums[0]</span><br><span class="line">        dp[1] = max(nums[0], nums[1])</span><br><span class="line">        for i in range(2, size):</span><br><span class="line">            dp[i] = max(dp[i - 2] + nums[i], dp[i - 1])</span><br><span class="line">        </span><br><span class="line">        return dp[size - 1]</span><br></pre></td></tr></tbody></table></figure>

<p>上述方法使用了数组存储结果。考虑到每间房屋的最高总金额只和该房屋的前两间房屋的最高总金额相关，因此可以使用滚动数组，在每个时刻只需要存储前两间房屋的最高总金额。</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def rob(self, nums: List[int]) -> int:</span><br><span class="line">        if not nums:</span><br><span class="line">            return 0</span><br><span class="line"></span><br><span class="line">        size = len(nums)</span><br><span class="line">        if size == 1:</span><br><span class="line">            return nums[0]</span><br><span class="line">        </span><br><span class="line">        first, second = nums[0], max(nums[0], nums[1])</span><br><span class="line">        for i in range(2, size):</span><br><span class="line">            first, second = second, max(first + nums[i], second)</span><br><span class="line">        </span><br><span class="line">        return second</span><br></pre></td></tr></tbody></table></figure>

<p>复杂度分析</p>
<p>时间复杂度：O(n)，其中 n 是数组长度。只需要对数组遍历一次。</p>
<p>空间复杂度：O(1)。使用滚动数组，可以只存储前两间房屋的最高总金额，而不需要存储整个数组的结果，因此空间复杂度是 O(1)。</p>
<h4>1431.拥有最多糖果的孩子(简单)</h4>

<hr>
<p>给你一个数组 candies 和一个整数 extraCandies ，其中 candies[i] 代表第 i 个孩子拥有的糖果数目。</p>
<p>对每一个孩子，检查是否存在一种方案，将额外的 extraCandies 个糖果分配给孩子们之后，此孩子有 最多 的糖果。注意，允许有多个孩子同时拥有 最多 的糖果数目。</p>
<p><strong>示例 1：</strong></p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">输入：candies = [2,3,5,1,3], extraCandies = 3</span><br><span class="line">输出：[true,true,true,false,true] </span><br><span class="line">解释：</span><br><span class="line">孩子 1 有 2 个糖果，如果他得到所有额外的糖果（3个），那么他总共有 5 个糖果，他将成为拥有最多糖果的孩子。</span><br><span class="line">孩子 2 有 3 个糖果，如果他得到至少 2 个额外糖果，那么他将成为拥有最多糖果的孩子。</span><br><span class="line">孩子 3 有 5 个糖果，他已经是拥有最多糖果的孩子。</span><br><span class="line">孩子 4 有 1 个糖果，即使他得到所有额外的糖果，他也只有 4 个糖果，无法成为拥有糖果最多的孩子。</span><br><span class="line">孩子 5 有 3 个糖果，如果他得到至少 2 个额外糖果，那么他将成为拥有最多糖果的孩子。</span><br></pre></td></tr></tbody></table></figure>

<p><strong>示例 2：</strong></p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">输入：candies = [4,2,1,1,2], extraCandies = 1</span><br><span class="line">输出：[true,false,false,false,false] </span><br><span class="line">解释：只有 1 个额外糖果，所以不管额外糖果给谁，只有孩子 1 可以成为拥有糖果最多的孩子。</span><br></pre></td></tr></tbody></table></figure>

<p><strong>示例 3：</strong></p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">输入：candies = [12,1,12], extraCandies = 10</span><br><span class="line">输出：[true,false,true]</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">kidsWithCandies</span><span class="params">(self, candies: List[int], extraCandies: int)</span> -> List[bool]:</span></span><br><span class="line">        m = max(candies)</span><br><span class="line">        a = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> candies:</span><br><span class="line">            <span class="keyword">if</span> (i + extraCandies) >= m:</span><br><span class="line">                a.append(<span class="literal">True</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                a.append(<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> a</span><br></pre></td></tr></tbody></table></figure>

<h4>101.对称二叉树(简单)</h4>

<hr>
<p>给定一个二叉树，检查它是否是镜像对称的。</p>
<p>例如，二叉树 [1,2,2,3,4,4,3] 是对称的。</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">    1</span><br><span class="line">   / \</span><br><span class="line">  2   2</span><br><span class="line"> / \ / \</span><br><span class="line">3  4 4  3</span><br></pre></td></tr></tbody></table></figure>

<p>但是下面这个 <code>[1,2,2,null,3,null,3]</code> 则不是镜像对称的:</p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">  1</span><br><span class="line"> / \</span><br><span class="line">2   2</span><br><span class="line"> \   \</span><br><span class="line"> 3    3</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isSymmetric</span><span class="params">(self, root: TreeNode)</span> -> bool:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root: <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">return</span> self.isSTree(root.left,root.right)</span><br><span class="line">    <span class="comment">#     global r</span></span><br><span class="line">    <span class="comment">#     r =  []</span></span><br><span class="line">    <span class="comment">#     f = 0</span></span><br><span class="line">    <span class="comment">#     self.mid(root, f)</span></span><br><span class="line">    <span class="comment">#     print(r)</span></span><br><span class="line">    <span class="comment">#     if r == r[::-1]:</span></span><br><span class="line">    <span class="comment">#         return True</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># def mid(self, root, f):</span></span><br><span class="line">    <span class="comment">#     f += 1</span></span><br><span class="line">    <span class="comment">#     if root == None:</span></span><br><span class="line">    <span class="comment">#         r.append(f)</span></span><br><span class="line">    <span class="comment">#     else:</span></span><br><span class="line">    <span class="comment">#         self.mid(root.left, f)</span></span><br><span class="line">    <span class="comment">#         r.append(root.val)</span></span><br><span class="line">    <span class="comment">#         self.mid(root.right, f)</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isSTree</span><span class="params">(self,left,right)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> left <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">and</span> right <span class="keyword">is</span> <span class="literal">None</span>: <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">if</span> left <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> right <span class="keyword">is</span> <span class="literal">None</span>: <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">if</span> left.val != right.val: <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> self.isSTree(left.left,right.right) <span class="keyword">and</span> self.isSTree(left.right, right.left)</span><br></pre></td></tr></tbody></table></figure>

<p>我采用了中序遍历和递归两种方式，注释掉的部分为中序遍历。递归判断时想清楚递归的终止情况与返回值：（1）左右子树都为空 → True （2）左右子树一个为空 → False （3）左右子树都不空，但是值不相等 → False （4）若上述情况都不满足， 检查 左左&右右， 左右&右左。</p>
<h4>84.柱状图中最大的矩形(困难)</h4>

<hr>
<p>给定 <em>n</em> 个非负整数，用来表示柱状图中各个柱子的高度。每个柱子彼此相邻，且宽度为 1 。</p>
<p>求在该柱状图中，能够勾勒出来的矩形的最大面积。</p>
<img alt="1.png" style="zoom:50%;" data-src="https://i.loli.net/2020/06/01/gPviFIYRh2xmOjf.jpg" class="lazyload">

<p>以上是柱状图的示例，其中每个柱子的宽度为 1，给定的高度为 <code>[2,1,5,6,2,3]</code>。</p>
<img alt="2.png" style="zoom:50%;" data-src="https://i.loli.net/2020/06/01/whXgI2TFcaU7rNm.jpg" class="lazyload">

<p>图中阴影部分为所能勾勒出的最大矩形面积，其面积为 <code>10</code> 个单位。</p>
<p><strong>示例:</strong></p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">输入: [2,1,5,6,2,3]</span><br><span class="line">输出: 10</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">largestRectangleArea</span><span class="params">(self, heights: List[int])</span> -> int:</span></span><br><span class="line">        n = len(heights)</span><br><span class="line">        left, right = [<span class="number">0</span>] * n, [<span class="number">0</span>] * n</span><br><span class="line"></span><br><span class="line">        mono_stack = list()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="keyword">while</span> mono_stack <span class="keyword">and</span> heights[mono_stack[<span class="number">-1</span>]] >= heights[i]:</span><br><span class="line">                mono_stack.pop()</span><br><span class="line">            left[i] = mono_stack[<span class="number">-1</span>] <span class="keyword">if</span> mono_stack <span class="keyword">else</span> <span class="number">-1</span></span><br><span class="line">            mono_stack.append(i)</span><br><span class="line">        </span><br><span class="line">        mono_stack = list()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n - <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">            <span class="keyword">while</span> mono_stack <span class="keyword">and</span> heights[mono_stack[<span class="number">-1</span>]] >= heights[i]:</span><br><span class="line">                mono_stack.pop()</span><br><span class="line">            right[i] = mono_stack[<span class="number">-1</span>] <span class="keyword">if</span> mono_stack <span class="keyword">else</span> n</span><br><span class="line">            mono_stack.append(i)</span><br><span class="line">        </span><br><span class="line">        ans = max((right[i] - left[i] - <span class="number">1</span>) * heights[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(n)) <span class="keyword">if</span> n > <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></tbody></table></figure>

<p>采用的是单调栈。</p>
<h4>14.最长公共前缀(简单)</h4>

<hr>
<p>编写一个函数来查找字符串数组中的最长公共前缀。</p>
<p>如果不存在公共前缀，返回空字符串 <code>""</code>。</p>
<p><strong>示例 1:</strong></p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">输入: ["flower","flow","flight"]</span><br><span class="line">输出: "fl"</span><br></pre></td></tr></tbody></table></figure>

<p><strong>示例 2:</strong></p>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">输入: ["dog","racecar","car"]</span><br><span class="line">输出: ""</span><br><span class="line">解释: 输入不存在公共前缀。</span><br></pre></td></tr></tbody></table></figure>

<p>我写得很简陋，就是暴力，有点像官方给的横向扫描，但是不够简洁：</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">longestCommonPrefix</span><span class="params">(self, strs: List[str])</span> -> str:</span></span><br><span class="line">        length = len(strs)</span><br><span class="line">        <span class="keyword">if</span> length == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">''</span></span><br><span class="line">        <span class="keyword">if</span> length == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> strs[<span class="number">0</span>]</span><br><span class="line">        L = len(strs[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">if</span> L == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">''</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            R = <span class="string">''</span></span><br><span class="line">            R += self.cons(strs[<span class="number">0</span>], strs[<span class="number">1</span>])</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, length - <span class="number">1</span>):</span><br><span class="line">                R = self.cons(R, strs[i + <span class="number">1</span>])</span><br><span class="line">            <span class="keyword">return</span> R</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cons</span><span class="params">(self, a, b)</span>:</span></span><br><span class="line">        r = <span class="string">''</span></span><br><span class="line">        <span class="keyword">if</span> len(a) == <span class="number">0</span> <span class="keyword">or</span> len(b) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> r</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(a)):</span><br><span class="line">            <span class="keyword">if</span> i > len(b) - <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">if</span> a[i] == b[i]:</span><br><span class="line">                r += a[i]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">return</span> r</span><br></pre></td></tr></tbody></table></figure>

<p>官方解析给了四种思路：</p>
<h4 id="方法一：横向扫描"><a href="#方法一：横向扫描" class="headerlink" title="方法一：横向扫描"></a>方法一：横向扫描</h4><p>用$LCP(S_1…S_n)$表示字符串$S_1…S_n$的最长公共前缀。可以得出以下结论：</p>
<p>​         $$LCP(S_1…S_n) = LCP(LCP(LCP(S_1,S_2),S_3),…S_n)$$</p>
<p>基于该结论，可以得到一种查找字符串数组中的最长公共前缀的简单方法。依次遍历字符串数组中的每个字符串，对于每个遍历到的字符串，更新最长公共前缀，当遍历完所有的字符串以后，即可得到字符串数组中的最长公共前缀。如果在尚未遍历完所有的字符串时，最长公共前缀已经是空串，则最长公共前缀一定是空串，因此不需要继续遍历剩下的字符串，直接返回空串即可。</p>
<img alt="1.png" style="zoom:50%;" data-src="https://i.loli.net/2020/06/15/K5HnPa1dQFhOyBG.png" class="lazyload">

<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">longestCommonPrefix</span><span class="params">(self, strs: List[str])</span> -> str:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> strs:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">""</span></span><br><span class="line">        </span><br><span class="line">        prefix, count = strs[<span class="number">0</span>], len(strs)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, count):</span><br><span class="line">            prefix = self.lcp(prefix, strs[i])</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> prefix:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> prefix</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">lcp</span><span class="params">(self, str1, str2)</span>:</span></span><br><span class="line">        length, index = min(len(str1), len(str2)), <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> index < length <span class="keyword">and</span> str1[index] == str2[index]:</span><br><span class="line">            index += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> str1[:index]</span><br></pre></td></tr></tbody></table></figure>

<p>复杂度分析</p>
<ul>
<li><p>时间复杂度：O(mn)O(mn)，其中 mm 是字符串数组中的字符串的平均长度，nn 是字符串的数量。最坏情况下，字符串数组中的每个字符串的每个字符都会被比较一次。</p>
</li>
<li><p>空间复杂度：O(1)O(1)。使用的额外空间复杂度为常数。</p>
</li>
</ul>
<h4 id="方法二：纵向扫描"><a href="#方法二：纵向扫描" class="headerlink" title="方法二：纵向扫描"></a>方法二：纵向扫描</h4><p>另一种方法是纵向扫描。纵向扫描时，从前往后遍历所有字符串的每一列，比较相同列上的字符是否相同，如果相同则继续对下一列进行比较，如果不相同则当前列不再属于公共前缀，当前列之前的部分为最长公共前缀。</p>
<img alt="2.png" style="zoom:50%;" data-src="https://i.loli.net/2020/06/15/SvCD5j4XQV7HZ9u.png" class="lazyload">

<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">longestCommonPrefix</span><span class="params">(self, strs: List[str])</span> -> str:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> strs:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">""</span></span><br><span class="line">        </span><br><span class="line">        length, count = len(strs[<span class="number">0</span>]), len(strs)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(length):</span><br><span class="line">            c = strs[<span class="number">0</span>][i]</span><br><span class="line">            <span class="keyword">if</span> any(i == len(strs[j]) <span class="keyword">or</span> strs[j][i] != c <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, count)):</span><br><span class="line">                <span class="keyword">return</span> strs[<span class="number">0</span>][:i]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> strs[<span class="number">0</span>]</span><br></pre></td></tr></tbody></table></figure>

<p>复杂度分析</p>
<ul>
<li><p>时间复杂度：$O(mn)$，其中 $m$ 是字符串数组中的字符串的平均长度，$n$ 是字符串的数量。最坏情况下，字符串数组中的每个字符串的每个字符都会被比较一次。</p>
</li>
<li><p>空间复杂度：$O(1)$。使用的额外空间复杂度为常数。</p>
</li>
</ul>
<h4 id="方法三：分治"><a href="#方法三：分治" class="headerlink" title="方法三：分治"></a>方法三：分治</h4><p>注意到$LCP$的计算满足结合律，有以下结论：</p>
<p>​        $$LCP(S_1…S_n) = LCP(LCP(S_1…S_K),LCP(S_{k+1}…S_N))$$</p>
<p>其中$LCP(S_1…S_n)$是字符串$S_1…S_n$的最长公共前缀，$1<K<n$。</p>
<p>基于上述结论，可以使用分治法得到字符串数组中的最长公共前缀。对于问题$LCP(S_i…S_j)$，可以分解成两个字问题$LCP(S_i…S_{mid})$与$LCP(S_{mid+1}…S_{j})$，其中$mid$值为二分之$i+j$。对于两个子问题分别求解，然后对两个子问题的解计算最长公共前缀，即为原问题的解。</p>
<img alt="3.png" style="zoom:50%;" data-src="https://i.loli.net/2020/06/15/ybQ3iH8joIAanxJ.png" class="lazyload">

<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">longestCommonPrefix</span><span class="params">(self, strs: List[str])</span> -> str:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">lcp</span><span class="params">(start, end)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> start == end:</span><br><span class="line">                <span class="keyword">return</span> strs[start]</span><br><span class="line"></span><br><span class="line">            mid = (start + end) // <span class="number">2</span></span><br><span class="line">            lcpLeft, lcpRight = lcp(start, mid), lcp(mid + <span class="number">1</span>, end)</span><br><span class="line">            minLength = min(len(lcpLeft), len(lcpRight))</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(minLength):</span><br><span class="line">                <span class="keyword">if</span> lcpLeft[i] != lcpRight[i]:</span><br><span class="line">                    <span class="keyword">return</span> lcpLeft[:i]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> lcpLeft[:minLength]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span> <span class="keyword">if</span> <span class="keyword">not</span> strs <span class="keyword">else</span> lcp(<span class="number">0</span>, len(strs) - <span class="number">1</span>)</span><br></pre></td></tr></tbody></table></figure>

<h4 id="方法四：二分查找"><a href="#方法四：二分查找" class="headerlink" title="方法四：二分查找"></a>方法四：二分查找</h4><p>显然，最长公共前缀的长度不会超过字符串数组中的最短字符串的长度。用 $minLength$ 表示字符串数组中的最短字符串的长度，则可以在 $[0,minLength] $的范围内通过二分查找得到最长公共前缀的长度。每次取查找范围的中间值 $mid$，判断每个字符串的长度为 $mid$ 的前缀是否相同，如果相同则最长公共前缀的长度一定大于或等于 $mid$，如果不相同则最长公共前缀的长度一定小于$mid$，通过上述方式将查找范围缩小一半，直到得到最长公共前缀的长度。</p>
<img alt="4.png" style="zoom:50%;" data-src="https://i.loli.net/2020/06/15/PQH8k5yO3gEAoVd.png" class="lazyload">

<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">longestCommonPrefix</span><span class="params">(self, strs: List[str])</span> -> str:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">isCommonPrefix</span><span class="params">(length)</span>:</span></span><br><span class="line">            str0, count = strs[<span class="number">0</span>][:length], len(strs)</span><br><span class="line">            <span class="keyword">return</span> all(strs[i][:length] == str0 <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, count))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> strs:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">""</span></span><br><span class="line"></span><br><span class="line">        minLength = min(len(s) <span class="keyword">for</span> s <span class="keyword">in</span> strs)</span><br><span class="line">        low, high = <span class="number">0</span>, minLength</span><br><span class="line">        <span class="keyword">while</span> low < high:</span><br><span class="line">            mid = (high - low + <span class="number">1</span>) // <span class="number">2</span> + low</span><br><span class="line">            <span class="keyword">if</span> isCommonPrefix(mid):</span><br><span class="line">                low = mid</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                high = mid - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> strs[<span class="number">0</span>][:low]</span><br></pre></td></tr></tbody></table></figure>

</body></html>]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>The FacT: Taming Latent Factor Models for Explainability with Factorization Trees</title>
    <url>/2020/05/23/The-FacT-Taming-Latent-Factor-Models-for-Explainability-with-Factorization-Trees/</url>
    <content><![CDATA[<html><head></head><body><blockquote>
<figure class="highlight plain"><table><tbody><tr><td class="code"><pre><span class="line">@inproceedings{tao2019the,</span><br><span class="line">	title="The FacT: Taming Latent Factor Models for Explainability with Factorization Trees",</span><br><span class="line">	author="Yiyi {Tao} and Yiling {Jia} and Nan {Wang} and Aobo {Yang} and Hongning {Wang}",</span><br><span class="line">	booktitle="SIGIR 2019 : 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval",</span><br><span class="line">	year="2019"</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>

<p> <a href="https://doi.org/10.1145/3331184.3331244" target="_blank" rel="noopener">https://doi.org/10.1145/3331184.3331244</a></p>
</blockquote>
<h2>Abstract</h2>

<p>​        The FacT model aims at explaining latent factor based recommendation algorithms with rule-based explanations. It integrates <font color="red">regression trees</font> to guide the learning of latent factor models for recommendation, and uses the learned tree structure to explain the resulting latent factors. With user-generated reviews, regression trees on users and items are built respectively, and each node on the trees are asscoiated with a latent profile to represent users and items. </p>
<h2>Methodology</h2>

<p><b>Latent factor learning</b></p>
<p>​        latent factor models have been widely deployed in modern recommender systems becuase its excellent recommendation quality. This work choices matrix factorization due to its simplicity.</p>
<img alt="1.jpeg" style="zoom:50%;" data-src="https://i.loli.net/2020/05/24/fK8aYMkwTldIHUi.jpg" class="lazyload">

<p>​        (1) for point wise rating prediction loss; (2) for pairwise ranking loss.</p>
<p><b>Explanation rule induction</b></p>
<p>​        It selects the predicates among the item features extracted from user-generated reviews, and each lexicon entry takes the form of <span style="border-bottom:2px dashed red;">(feature, opinion, sentiment polarity)</span>, abbreviated as (f, o ,s), and represents the <span style="border-bottom:2px dashed red;">sentiment polarity s inferred from an opinionated text phrase o describing feature f.</span></p>
<img alt="2.jpeg" style="zoom:50%;" data-src="https://i.loli.net/2020/05/24/iXmpjxkQTIFnDzu.jpg" class="lazyload">

<img alt="3.jpeg" style="zoom:50%;" data-src="https://i.loli.net/2020/05/24/Z8JDPxUqAhgm9ej.jpg" class="lazyload">

<img alt="4.jpeg" style="zoom: 50%;" data-src="https://i.loli.net/2020/05/24/pCk8Q4oLXSz1gZ9.jpg" class="lazyload">

<p><img alt="IMG_1A6D7EF91195-1.jpeg" data-src="https://i.loli.net/2020/05/24/fACq4EtT3X82yHP.jpg" class="lazyload"></p>
<h2>Evaluation</h2>

<ul>
<li>Normalized Discounted Cumulative Gain(NDCG)</li>
</ul>
<h2>Data</h2>

<ul>
<li>Amazon <a href="http://jmcauley.ucsd.edu/data/amazon/" target="_blank" rel="noopener">http://jmcauley.ucsd.edu/data/amazon/</a></li>
<li>Yelp dataset challenge <a href="https://www.yelp.com/dataset" target="_blank" rel="noopener">https://www.yelp.com/dataset</a></li>
</ul>
<h2>Code</h2>

<ul>
<li><a href="https://github.com/yilingjia/TheFacT" target="_blank" rel="noopener">https://github.com/yilingjia/TheFacT</a></li>
</ul>
<h2>Reference</h2>

<ul>
<li><a href="https://blog.csdn.net/qq_38871942/article/details/104696210" target="_blank" rel="noopener">https://blog.csdn.net/qq_38871942/article/details/104696210</a></li>
<li><a href="https://www.cnblogs.com/pinard/p/9128682.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/9128682.html</a></li>
</ul>
</body></html>]]></content>
      <categories>
        <category>recommendation</category>
        <category>explainable recommendation</category>
      </categories>
      <tags>
        <tag>explainability</tag>
        <tag>regression tree</tag>
        <tag>recommendation</tag>
      </tags>
  </entry>
  <entry>
    <title>Heterogeneous Graph Neural Network</title>
    <url>/2020/05/20/Heterogeneous%20Graph%20Neural%20Network/</url>
    <content><![CDATA[<html><head></head><body><blockquote>
<blockquote>
<p>Zhang C , Song D , Huang C , et al. Heterogeneous Graph Neural Network[C]// the 25th ACM SIGKDD International Conference. ACM, 2019.</p>
</blockquote>
</blockquote>
<h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><h5 id="idea-of-the-article-HetGNN"><a href="#idea-of-the-article-HetGNN" class="headerlink" title="idea of the article(HetGNN)"></a>idea of the article(HetGNN)</h5><blockquote>
<p>This paper models the heterogeneous graph network and gets each nodes’ vector representation.<br>purpose: learn how to represent the vectors of each node in a heterogeneous graph.(embedding)</p>
</blockquote>
<p>First, <font color="red">a reboot-based random walk strategy is used to select neighbors for each node according to the node type</font>.</p>
<p>Then, two modules are used to aggregate the characteristics of neighbor nodes:</p>
<ul>
<li>Generate feature vectors by modeling <font color="red">the features of different types of nodes.</font></li>
<li><font color="red">Aggregate different types of neighbor nodes</font>, and assign different weights to different types of nodes by fusing attention mechanism to obtain the final vector representation.</li>
</ul>
<p>Finally, establish loss function, use mini-batch gradient descent. </p>
<p><em>the learnted vector can be used to <b>link prediction, recommendation, nodes classification, cluster, etc..</b></em></p>
<hr>
<p><img alt="HGNN_challenge.jpeg" data-src="https://i.loli.net/2020/05/21/TFq79ogQsh1xCm3.jpg" class="lazyload"></p>
<hr>
<h3>HetGNN模型结构</h3>



<p>针对上文提到的异构网络面临的三个挑战，HetGNN分成了三个部分：</p>
<ul>
<li><p>邻居采样策略：Sampling Heterogeneous Neighbors（挑战一）</p>
</li>
<li><p>特征编码：Encoding Heterogeneous Contents（挑战二）</p>
</li>
<li><p>聚合邻居：Aggregating Heterogeneous Neighbors（挑战三）</p>
</li>
</ul>
<p>最后，根据目标函数进行优化，进行训练和预测</p>
<h5>Sampling Heterogeneous Neighbors(挑战一)</h5>

<p><em>(挑战一：对异构图如何采样？)</em></p>
<p>在异构图中，直接采样邻居面临的问题：</p>
<ol>
<li><p><font color="red">不能捕捉到不同类型邻居的信息。</font>比如说，在下图的作者-论文-会议的图中，作者之间并不相连，作者会议也不相连，但他们之间的关系不可被忽视。</p>
<div align="center"><img alt="HetG_example.png" style="zoom: 50%;" data-src="https://i.loli.net/2020/05/21/kLxdo3AM9jJPlVc.png" class="lazyload">
</div></li>
<li><p><font color="red">邻居数量的影响</font>。有的作者写了很多篇论文，有的作者写的少。有的商品被很多人访问，有的商品无人问津，冷启动问题不能很好的表示。</p>
</li>
<li><p><font color="red">节点的特征类型不同（如图像，文字等），不能直接聚合。</font></p>
</li>
</ol>
<p>针对上述问题，本文采用一种<font color="red">random walk with restart(RWR)</font>方法进行采样，主要有两步：</p>
<ol>
<li><p>从节点v<span style="border-bottom:2px dashed red;">随机游走采样</span>，采样固定长度，每次以概率p访问邻居节点或返回初始节点，每种类型节点采样数固定，确保每类节点都会被采样到。</p>
</li>
<li><p>对不同类型的邻居分组，不同类型的邻居，根据采样频率返回前k个</p>
</li>
</ol>
<p>上述采样方法中：</p>
<ol>
<li><p>对于每种类型的节点都采样到了</p>
</li>
<li><p>每种类型节点数量相同，并且高频邻居被选择</p>
</li>
<li><p>同种类型的邻居放在了一起，邻居信息可以聚合</p>
</li>
</ol>
<h5>Encoding Heterogeneous Contents(挑战二)</h5>

<p><em>(挑战二：对不同类型的特征怎样进行编码？)</em></p>
<p>同一个节点，也往往有多种类型的特征，如图像，文字等，<span style="border-bottom:2px dashed red;">文章提出先对这一类特征进行预训练，如类别特征直接利用one-hot，文本特征利用par2vec，图像特征利用CNN，训练得到每类特征的向量表示后，利用Bi-LSTM进行编码后聚合</span>。模型架构如图所示：</p>
<img alt="embedding.png" style="zoom:50%;" data-src="https://i.loli.net/2020/05/22/u9vmF5WgNJ7PUjy.jpg" class="lazyload">

<blockquote>
<p>类别型特征（categorical feature）主要是指年龄，职业，血型等在有限类别内取值的特征。它的原始输入通常是字符串形式，除了决策树族的算法能直接接受类别型特征作为输入，对于支持向量机，逻辑回归等模型来说，必须对其做一定的处理，转换成可靠的数值特征才能正确运行。一般的处理方式就是<a href="https://zhuanlan.zhihu.com/p/88921408" target="_blank" rel="noopener">one-hot encoding</a></p>
</blockquote>
<blockquote>
<p><a href="https://blog.csdn.net/hero_fantao/article/details/69659457?locationNum=10&fps=1" target="_blank" rel="noopener">par2vec</a>: 是根据word2vec产生的，专门针对paragraph vector。</p>
</blockquote>
<p>在数学表达式上，节点v的向量表示$f{_1}(v)$为：</p>
<img alt="f1_v_.jpeg" style="zoom:50%;" data-src="https://i.loli.net/2020/05/22/TcMxmGuFK9ZkR2r.jpg" class="lazyload">

<h5>Aggregating Heterogeneous Neighbors(挑战三)</h5>

<p><em>(挑战三：对不同类型的节点如何聚合？)</em></p>
<p>在上一部分，得到了每个节点的特征表示，在聚合上面临两个问题：对同样类型的不同节点怎样聚合？对不同类型的节点怎样聚合？分两步解决这两个问题：</p>
<p>Same type neighbors aggregation:</p>
<p>在采样中，我们对不同类型的节点进行采样，<font color="red">通过上一步得到了每个节点的特征，这里，需要对同一类型的节点特征进行聚合，此处仍然采用Bi-LSTM方法</font>：</p>
<img alt="f2_v_.png" style="zoom:50%;" data-src="https://i.loli.net/2020/05/22/PAKxtBSkiIfaoJj.jpg" class="lazyload">

<p><span style="border-bottom:2px dashed red;">输入为采样得到的相同类型邻居的特征表示，输出为这一类型邻居的向量表示</span>，有点类似与GraphSAGE的思想，<span style="border-bottom:2px dashed red;">利用Bi-LSTM对相同类型的邻居节点进行聚合</span>，模型结构如图所示：</p>
<img alt="NN-2.png" style="zoom:50%;" data-src="https://i.loli.net/2020/05/22/j768MorHRWbhtUd.jpg" class="lazyload">

<p>Type Combination:</p>
<p>上述得到了每个类型节点的向量表示，这里，希望对这些类型的节点进行聚合，考虑到不同类型节点的邻居贡献不同，因此引入注意力机制l联合学习不同类型的邻居：</p>
<img alt="FC8680C6-BD1C-4ADE-9DBF-A0B3C255DF27.png" style="zoom:50%;" data-src="https://i.loli.net/2020/05/22/TLhV1KUcCMB6pYP.jpg" class="lazyload">



<p>模型结构如图所示：</p>
<img alt="NN-3.png" style="zoom:50%;" data-src="https://i.loli.net/2020/05/22/wVPgWO1y8K3UJ2h.jpg" class="lazyload">

<hr>
<h3>Objective function and Framework</h3>

<p>根据目标函数学习模型参数：</p>
<img alt="fn.png" style="zoom:50%;" data-src="https://i.loli.net/2020/05/22/IAXLDbB9mT82onk.jpg" class="lazyload">

<p>Framework:</p>
<img alt="Framework.png" style="zoom:50%;" data-src="https://i.loli.net/2020/05/22/E3RnvjMmPidVucN.jpg" class="lazyload">

<p><strong>分成5步：</strong></p>
<ol>
<li><p>对邻居节点进行采样，按照节点类型进行分类</p>
</li>
<li><p>NN-1：对节点不同类型特征学习</p>
</li>
<li><p>NN-2：对相同类型节点各个特征的聚合</p>
</li>
<li><p>NN-3：对不同类型节点的聚合</p>
</li>
<li><p>根据Graph Context Loss损失函数进行优化</p>
</li>
</ol>
<p>最终得到每个节点的向量表示用于下游任务</p>
<hr>
<h3>Contribution:</h3>

<ul>
<li>Define heterogeneous graph: heterogeneous of graph structure and nodes information.</li>
<li>Propose HetGNN, which can capture the heterogeneous of stucture and content at the same time. It can be applied to both direct and inductive tasks.</li>
<li>Good performance in multi-data set experiments, link prediction, node classification, clustering and other tasks.</li>
</ul>
<hr>
<p>Reference :</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/w0KhXwE3tkZtjzQcRApRow" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/w0KhXwE3tkZtjzQcRApRow</a></li>
<li><a href="https://www.jianshu.com/p/fd8355e3d5d5" target="_blank" rel="noopener">https://www.jianshu.com/p/fd8355e3d5d5</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/88921408" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/88921408</a></li>
<li><a href="https://blog.csdn.net/hero_fantao/article/details/69659457?locationNum=10&fps=1" target="_blank" rel="noopener">https://blog.csdn.net/hero_fantao/article/details/69659457?locationNum=10&fps=1</a></li>
</ul>
<p>Paper:</p>
<ul>
<li><a href="https://doi.org/10.1145/3292500.3330961" target="_blank" rel="noopener">https://doi.org/10.1145/3292500.3330961</a></li>
</ul>
<p>Code:</p>
<ul>
<li><a href="https://github.com/chuxuzhang/KDD2019_HetGNN" target="_blank" rel="noopener">https://github.com/chuxuzhang/KDD2019_HetGNN</a></li>
</ul>
</body></html>]]></content>
      <tags>
        <tag>heterogeneous GNN</tag>
      </tags>
  </entry>
  <entry>
    <title>Summary of learning resources</title>
    <url>/2020/04/17/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB/</url>
    <content><![CDATA[<blockquote>
<blockquote>
<p>Some links about graph network.</p>
<p>Learning resources about GNN \ graph embedding \ LSTM etc..</p>
</blockquote>
</blockquote>
<ol>
<li><p><a href="https://mp.weixin.qq.com/s/zmX0L6ZeCqlTBYtP9XslgA" target="_blank" rel="noopener">自监督黑马SimCLRv2来了！提出蒸馏新思路，可迁移至小模型，性能精度超越有监督</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/Z1WeYOV4Kwp6os73FqG9ew" target="_blank" rel="noopener">近期必读的五篇KDD 2020【图神经网络 (GNN) 】相关论文_Part2</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/JU11nv2lv-zFesHjp2IAQQ" target="_blank" rel="noopener">Graph: 表现再差，也不进行Pre-Training? Self-Supervised Learning真香！</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/LajTagTaCHBNoqogo1zEiw" target="_blank" rel="noopener">KDD2020推荐系统论文聚焦</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/alVFJvksR0W6NMErubTM_g" target="_blank" rel="noopener">IJCAI2020 图相关论文集</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/WslDM6Mgx7_i5yW3XFjdAw" target="_blank" rel="noopener">【DL】规范化：你确定了解我吗？</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/iECG0sBte25li-E3Z_tEIw" target="_blank" rel="noopener">用 Python 训练自己的语音识别系统，这波操作稳了！</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/yKxwqwCC_Jxse_z3mK9lAw" target="_blank" rel="noopener">【CTR】MMoE-PosBias：Youtube 多任务学习框架</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/Rqz9yuSxpx_os54FrzyMqQ" target="_blank" rel="noopener">一文尽览推荐系统模型演变史(文末可下载)</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/iMU2LPDadmVMgzUifw3-XA" target="_blank" rel="noopener">浅谈电商搜索推荐中ID类特征的统一建模：Hema Embedding解读</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/ifTNRW0W7-P_LyfNldtavQ" target="_blank" rel="noopener">多任务学习方法在推荐中的演变</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/5SWMKuKC3XLvUdbKS8qJZw" target="_blank" rel="noopener">近期必读的六篇顶会 ICML 2020【图神经网络 (GNN) 】相关论文</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/80Ze31Nstqp5nuRD7B3vKw" target="_blank" rel="noopener">图深度学习：成功，挑战以及后面的路</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/4wzbZqU4rJcl6qEZTI8m2A" target="_blank" rel="noopener">KDD2020|混合时空图卷积网络：更精准的时空预测模型</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/h61viD2JvOcb6SpHrk5PQg" target="_blank" rel="noopener">一文概览如何消除广告和推荐中的Position Bias</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/c0hPqwfbgdSKGvJwN5nX3A" target="_blank" rel="noopener">WSDM 2020关于深度推荐系统与CTR预估工业界必读的论文</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/J6GUf-pAXmI9jhFGyVyU8w" target="_blank" rel="noopener">图系列|三篇图层次化表示学习(Hierarchical GNN)：图分类以及节点分类</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/A64paLV_3Arhu1LKvKc63w" target="_blank" rel="noopener">PinSAGE | GCN 在工业级推荐系统中的应用</a><br>《Graph Convolutional Neural Networks for Web-Scale Recommender Systems》2018.</p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/ewzsURiU7bfG3gObzIP2Mw" target="_blank" rel="noopener">深度长文：图神经网络欺诈检测方法总结</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/Yj_yP6LjxrG_UJX9W3cAHQ" target="_blank" rel="noopener">腾讯 at IJCAI 2020，基于内部-环境注意力网络的推荐多队列冷启动召回</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/rjWOkwzX3IE59Kc9P9leAQ" target="_blank" rel="noopener">格“物”致知：多模态预训练再次入门</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/FtUxxRWkUJ7346gWAJqQDQ" target="_blank" rel="noopener">【KDD2020-MSU】图结构学习的鲁棒图神经网络，克服对抗攻击提升GNN防御能力</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/6OMMboBdbLVA-HsEjA3bSA" target="_blank" rel="noopener">Ctr 预估之 Calibration</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/aNx9or0-eHHT1XI1BJyB7w" target="_blank" rel="noopener">综述|73页近百篇参考文献JMLR20动态图上的表示学习</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/lbRymEKPcCrLsk9r9w1tlQ" target="_blank" rel="noopener">高性能涨点的动态卷积 DyNet 与 CondConv、DynamicConv 有什么区别联系？</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/7_cZhszsnfBKyoUKYMkQhw" target="_blank" rel="noopener">近期必读的五篇计算机视觉顶会CVPR 2020【图神经网络 (GNN) 】相关论文-Part 3</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/KHbCbzP1JMgoYMjY5cMOmA" target="_blank" rel="noopener">华为诺亚实验室开源Disout算法，直接对标谷歌申请专利的Dropout算法</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/n3CSdTdsk5erpwRpZE9Qvw" target="_blank" rel="noopener">图专题|IJCAI2020两篇多层次/多视角相关的图神经网络GNN研究论文</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/_VBO_9yiZ0qngq4yfwcxEg" target="_blank" rel="noopener">从EMD、WMD到WRD：文本向量序列的相似度计算</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/Fjr2iCbeTy9AYNAwvJr7Og" target="_blank" rel="noopener">Mila唐建博士最新《图表示学习:算法与应用》2020研究进展，附59页ppt</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/DKDlFDRwoedSlhL8cu99DA" target="_blank" rel="noopener">从 Triplet loss 看推荐系统中文章Embedding</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/aCICOsif_jYNUkOTiB_-FQ" target="_blank" rel="noopener">【Code】GraphSAGE 源码解析</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/upXLNJsyq1muoNn3uJCKJg" target="_blank" rel="noopener">入门推荐系统，这25篇综述文章足够了</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/w0KhXwE3tkZtjzQcRApRow" target="_blank" rel="noopener">KDD19开源论文 Heterogeneous Graph Neural Network</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/Xn1xMXu0V7nkolX5x_h2lw" target="_blank" rel="noopener">SDM(Sequential Deep Matching Model)的复现之路</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/WnF-fqQyr2VNqr75Jzoqsw" target="_blank" rel="noopener">【GNN】Diff Pool：网络图的层次化表达</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/jIVhXMmP95G7hJP_KtksxA" target="_blank" rel="noopener">Youtube推荐RL首弹，基于Top-K的Off-Policy矫正解决推荐中的信息茧房困境</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/yyE0ki5rymOCyGa6CNfnzQ" target="_blank" rel="noopener">《深度学习推荐系统》读书笔记之Embedding技术在推荐中的应用</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/FZoiDfM05rWIqWJQg8F-5w" target="_blank" rel="noopener">RecSys 2019最佳论文：基于深度学习的推荐系统是否真的优于传统经典方法？</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/daIWHWMulVlqAsm04qyi6w" target="_blank" rel="noopener">2019年，异质图神经网络领域有哪些值得读的顶会论文？</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/63lTwUB2wsvhikWnNKa0CQ" target="_blank" rel="noopener">「工业落地」基于异质图神经网络的异常账户检测</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/Ry8R6FmiAGSq5RBC7UqcAQ" target="_blank" rel="noopener">深入理解图注意力机制（Graph Attention Network）</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/1xVPRIVwQQJfEen0RiNYvg" target="_blank" rel="noopener">谈谈推荐系统中的用户行为序列建模最新进展</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/Jmutb3fWuxxmbpX4CWS3HA" target="_blank" rel="noopener">《深度学习推荐系统》读书笔记之推荐系统的进化之路</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/zLBq2VVSAr0AUtozK7qqVA" target="_blank" rel="noopener">浅析Faiss在推荐系统中的应用及原理</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/Fg9GKw-QLlOskEqX7pl1dA" target="_blank" rel="noopener">推荐系统之FM算法原理及实现（附代码）</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/dSa0-BydmzmqDfrICJoUBQ" target="_blank" rel="noopener">ECAI2020推荐系统论文聚焦</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/wLjBcK9PQHz6lUIsiZCjgg" target="_blank" rel="noopener">长文|三大主题全方位梳理图论与图学习中的基本概念：图搜索，最短路径，聚类系数，中心度等</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/jVG_JJN5OalJcbtTGti9AA" target="_blank" rel="noopener">「工业落地」阿里异质图神经网络推荐：19KDD IntentGC</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/3gHKD8g3tB6z88DVq5AkIw" target="_blank" rel="noopener">「工业落地」阿里异构图表示学习：19KDD GATNE</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/y94JACgjTxObvNDhmUfulA" target="_blank" rel="noopener">图神经网络入门（三）GAT图注意力网络</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/3x6FtOMzGitTlwwJs-dN9w" target="_blank" rel="noopener">从ACL 2020和ICLR 2020看知识图谱嵌入的近期研究进展</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/4CPfejPrAhJuYMgSj6aGsw" target="_blank" rel="noopener">【香港理工】生成式对抗网络(GANs)最新2020综述，41页pdf阐述GAN训练、 挑战、解决方案和未来方向</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/53zPX7N7YLlUnvD23kT6pg" target="_blank" rel="noopener">SIGIR 20 | 腾讯看点首次在推荐中应用迁移学习提出PeterRec框架，从少量行为数据中学习用户表示</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/lf9BjP9qs_cn8f9vGUECpw" target="_blank" rel="noopener">比CNN更强有力，港中文贾佳亚团队提出两类新型自注意力网络｜CVPR2020</a></p>
</li>
<li><p><a href="https://www.toutiao.com/a6825468794287161860/?tt_from=weixin&utm_campaign=client_share&wxshare_count=1&timestamp=1589325241&app=news_article&utm_source=weixin&utm_medium=toutiao_ios&req_id=202005130714000101941000345D3EC4DC&group_id=6825468794287161860" target="_blank" rel="noopener">33 个神经网络「炼丹」技巧</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/azgdVJ3-Ge1HJC1mVTRVfA" target="_blank" rel="noopener">自动化神经网络理论进展缓慢，AutoML 算法的边界到底在哪</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/SqBU40sfo3IEj_iHnb4cXQ" target="_blank" rel="noopener">高效利用无标注数据：自监督学习简述</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/0Jn79XT9A70sQEMOImzFLg" target="_blank" rel="noopener">【斯坦福谷歌】最新《图机器学习》综述论文，38页pdf阐述最新图表示学习进展</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/GCoyRPKe9ND7CnG9-zWTkA" target="_blank" rel="noopener">近期必读的5篇顶会CVPR 2020【场景图+图神经网络（SG+GNN）】相关论文</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/SOaA9XNnymLgGgJ5JNSdBg" target="_blank" rel="noopener">对比学习（Contrastive Learning）相关进展梳理</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/0jPOSvSqsTEtwjQ_nRgQ_g" target="_blank" rel="noopener">干货！2019五大顶会必读的Graph Embedding相关的论文</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/W2UTCtoTqRW739EFxCFf1w" target="_blank" rel="noopener">在深度学习顶会ICLR 2020上，Transformer模型有什么新进展？</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/z0UMPHLYFOaiTCxqt65uRg" target="_blank" rel="noopener">当深度学习遇上量化交易——图与知识图谱篇</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/wLvTo_k67VwiwrfW9dfSWg" target="_blank" rel="noopener">「工业落地」阿里在图神经网络推荐的探</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/RhnyQUnXxMDybv8B-dSj4Q" target="_blank" rel="noopener">图系列|WWW2020 图学习/图神经网络GNN相关论文速览</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/byVdEPcCmVPJOk-uIyGsbw" target="_blank" rel="noopener">【重磅】GCN大佬Thomas Kipf博士论文《深度学习图结构表示》178页pdf阐述图卷积神经网络等机制与应用</a></p>
</li>
</ol>
]]></content>
      <tags>
        <tag>recommendation</tag>
        <tag>GNN</tag>
      </tags>
  </entry>
  <entry>
    <title>HIN-Graph</title>
    <url>/2020/04/03/HIN-Graph/</url>
    <content><![CDATA[<blockquote>
<blockquote>
<p>Summarying the recent works on <strong>heterogeneous graph neural networks</strong></p>
</blockquote>
</blockquote>
<h2>Links</h2>

<p>清华朱文武老师组关于图上的深度学习综述(2018) <a href="https://arxiv.org/abs/1812.04202v1" target="_blank" rel="noopener" title="With a Title">Deep Learning on Graphs: A Survey</a>. </p>
<p>IEEE Fellow关于图神经网络综述(2019) <a href="https://arxiv.org/abs/1901.00596" target="_blank" rel="noopener" title="With a Title">A Comprehensive Survey on Graph Neural Networks</a>. </p>
<p>Ziniu Hu关于异质图Transformer的研究(2020) <a href="https://arxiv.org/abs/2003.01332" target="_blank" rel="noopener" title="With a Title">Heterogeneous Graph Transformer</a>. </p>
<p>纪厚业做的分享： <a href="https://www.bilibili.com/video/BV1HE41157GD?t=6492" target="_blank" rel="noopener" title="With a Title">异质图神经网络：模型和应用</a>. 介绍了异质图网络、以及一些模型和三个实际落地的论文</p>
<p>发表在数据库顶会VLDB上的最新异质图表示学习的综述(2020) <a href="https://arxiv.org/abs/1801.05852" target="_blank" rel="noopener" title="With a Title">Heterogeneous Network Representation Learning: Survey, Benchmark, Evaluation, and Beyond</a>.</p>
<p>图数据上的对抗攻击和防御综述(2020) <a href="https://arxiv.org/abs/1812.10528v1" target="_blank" rel="noopener" title="With a Title">Adversarial Attack and Defense on Graph Data: A Survey</a>. </p>
<p>用于构建动态图的例子《Dynamic Network Embedding by Modelling Triadic Closure Process(2018)》 <a href="https://github.com/luckiezhou/DynamicTriad" target="_blank" rel="noopener" title="With a Title">https://github.com/luckiezhou/DynamicTriad</a>. </p>
<hr>
<p>主流知识图谱表示学习算法:</p>
<ul>
<li>TransE、ComplEx、DistMult、TransR、RESCAL、RotatE </li>
</ul>
]]></content>
      <tags>
        <tag>HIN</tag>
        <tag>graph-learning</tag>
        <tag>GNN/GCN</tag>
      </tags>
  </entry>
  <entry>
    <title>metapath2vec: Scalable Representation Learning for Heterogeneous Networks</title>
    <url>/2019/12/12/metapath2vec-Scalable-Representation-Learning-for-Heterogeneous-Networks/</url>
    <content><![CDATA[<html><head></head><body><blockquote>
<blockquote>
<p>This article proposed two scalable representation learning models: <strong>metapath2vec</strong> and <strong>metapath2vec++</strong>. </p>
<p>In metapath2vec: </p>
<ul>
<li>First, they propose <strong>meta-path</strong> based <strong>random walks</strong> in heterogeneous networks to generate <strong>hetetogeneous neighborhoods</strong> with network semantics for various types of nodes.</li>
<li>Second, they extend the <strong>skip-gram</strong> model to facilitate the modeling of geographically and semantically close nodes.</li>
</ul>
<p>In metapath2vec++:</p>
<ul>
<li>they develop a <strong>heterogeneous negative sampling-based</strong> method that enables the accurate and efficient prediction of a node’s heterogeneous neighborhood.</li>
</ul>
<p><a href="http://dx.doi.org/10.1145/3097983.3098036" target="_blank" rel="noopener">http://dx.doi.org/10.1145/3097983.3098036</a></p>
</blockquote>
</blockquote>
<img alt="1.jpeg" style="zoom:50%;" data-src="https://i.loli.net/2020/05/25/Afdic56XJB1YWKT.jpg" class="lazyload">





<h2>Data</h2>

<ul>
<li>AMiner Computer Science dataset</li>
<li>Database and Information Systems dataset</li>
</ul>
<h2>Coding</h2>

<ul>
<li><a href="https://ericdongyx.github.io/metapath2vec/m2v.html" target="_blank" rel="noopener">https://ericdongyx.github.io/metapath2vec/m2v.html</a> </li>
</ul>
</body></html>]]></content>
      <categories>
        <category>Heterogeneous Networks</category>
      </categories>
      <tags>
        <tag>meta path</tag>
        <tag>heterogeneous</tag>
        <tag>skip-gram</tag>
        <tag>negative sampling-based method</tag>
      </tags>
  </entry>
  <entry>
    <title>Meta-Graph Based Recommendation Fusion over Heterogeneous Information Networks</title>
    <url>/2019/12/09/Meta-Graph-Based-Recommendation-Fusion-over-Heterogeneous-Information-Networks/</url>
    <content><![CDATA[<html><head></head><body><blockquote>
<blockquote>
<p>This article is paper about recommendation over heterogeneous information networks. Because HIN faces two problems: 1, how to represent the high-level semantics of recommendations and 2, how to fuse the heterogeneous information to make recommendations, the author first introduced the concept of <strong>meta-graph</strong> to HIN-based recommendation, and then solved the information fusion problem with a <strong>“matrix factorization(MF) + factorization machine(FM)”</strong> approach.</p>
<p>I think the author made two innovations:</p>
<p>1, use meta-graph and commute matrix to construct similarity matrix;</p>
<p>2, for each meta-graph, he calculate user-item’s similarity matrix, then factorize users and items’ feature vectors by using matrix factorization (MF). So he can obtain many different users and items vectors of many different meta-graph. for all of then, he use factorization machine(FM) to component then.</p>
<p>A user-item score prediction problem.</p>
</blockquote>
</blockquote>
<p><strong>KEYWORDS:  Recommendation system; Collaborative filtering; Heterogeneous information networks; Factorization machine.</strong></p>
<h2>Contents</h2>
> * [INTRODUCTION](#1)
> * [FRAMEWORK](#2)
>   * [Meta-graph based Similarity](#2.1)
>   * [Meta-graph based Latent Features](#2.2)
>   * [Recommendation Model](2.3)
>   * [Comparison with Previous Latent Feature based Model](2.4)
> * [MODEL OPTIMIZATION](#3)
>   * [Optimization](#3.1)
>   * [Complexity Analysis](3.2)
> * [EXPERIMENTS](4)
>   * [Datasets](4.1)
>   * [Evaluation Metric](#4.2)
>   * [Baseline Models](#4.3)



<h2 id="1">INTRODUCTION</h2>
​        Heterogeneous information networks(**HINs**) have been proposed as a general data representation for many different types of data. 

<p>​        At the beginning, HINs were used to handle entity search and similarity measure problems. Later, it was extended to handle heterogeneous entity recommendation problems. Then, the semantic relatedness constrained by the entity types can be defined by the similarities between two entities along meta-graph.</p>
<p>​        For traditional collaborative filtering, we always build a simple meta-path <em>Business –> User</em> and learn from this meta-path to make generalization. From HIN’s schema, we can build more complicated meta-paths like <em>User–>Review–>Word–>Review–>Business</em>. </p>
<p>​        There are two major challenges when applying meta-path based similarities to recommender systems: <strong>First</strong>, meta-path may not be the best way to characterize the rich semantics. So, in this paper, they used <strong>meta-graph(or meta-structure)</strong> to compute similarity between homogeneous type of entities over HINs. <strong>Second</strong>, different meta-paths or meta-graphs result in different similarities. How to assemble them in an effective way is another challenge.</p>
<p>​        So, in this paper, in order to solve the above two problems, author first used <strong>the concept of meta-graph</strong> and then applied <strong>matrix factorization(MF) + factorization machine(FM)</strong>.</p>
<pre><code>* for each meta-graph, compute the user-item similarity matrix
* Use unsupervised MF to factorize it into a set of user and item latent vectors.
* Use FM to assemble them to learn from the rating matrix. </code></pre><h2 id="2">FRAMEWORK</h2>
<h3 id="2.1">Meta-graph based Similarity</h3>
​        The definition of Meta-graph:

<p><img alt="meta-graph.png" data-src="https://i.loli.net/2019/12/09/VePI7wiHLXqt8fQ.jpg" class="lazyload"></p>
<p>​        All of the meta-graphs on Amazon and Yelp:</p>
<p><img alt="all of the meta-graphs for Amazon and Yelp" data-src="https://i.loli.net/2019/12/09/wjOSsENmKGR2yY9.jpg" class="lazyload"></p>
<p>​        The author used <strong>commuting matrices</strong> to compute the counting based similarity matrix of a meta-path. For example: if we have a meta-path:<img alt="meta-path.png" data-src="https://i.loli.net/2019/12/09/Pkmpr4n9MUfvV5s.jpg" class="lazyload"></p>
<p>And, we define a matrix $W_{A_iA_j}$ as the adjacency matrix between type $A_i$ and $A_j$. So the commuting matrix for path p is:<img alt="ABF68C6E-64CD-469A-BC2E-7D1B19ED3D31.png" data-src="https://i.loli.net/2019/12/09/esYuT4iBoKzdtrN.jpg" class="lazyload"></p>
<p><strong>Algorithm 1 Computing commuting matrix for CM9 :</strong></p>
<p><img alt="B07D1FB3-2139-4357-A000-0EA870DA1635.png" data-src="https://i.loli.net/2019/12/09/Yshg3R91MzpqZ48.jpg" class="lazyload"></p>
<h3 id="2.2">Meta-graph based Latent Features</h3>
​        By use of matrix factorization method, we can obtain user and item's low-rank matrices:![low-rank.png](https://i.loli.net/2019/12/09/z4JjWdDx5TnMNf3.jpg)

<p><img alt="702CE1AE-D96F-4C88-BA2E-B80D483F536C.png" data-src="https://i.loli.net/2019/12/09/pmxiLPM6atBTrq8.jpg" class="lazyload"></p>
<p>Which ${\lambda_u},{\lambda_b}$ are the hyper-parameters that control the influence of Frobenius norm regularization to aviod overfitting.</p>
<h3 id="2.3">Recommendation Model</h3>
​        For the L groups of user and item latent features and F ranks which used to factorize every similarity matrix, we concatenate all of the corresponding user and item features form all of the L meta-graphs:![EADE9E6C-D305-4B2C-BF96-033AC1A9BAF7.png](https://i.loli.net/2019/12/09/abOz2gnh9PDwklq.jpg)

<p>​        So the rating for the sample $x^n$ based on FM is computed as follows:</p>
<p><img alt="99A1983D-A650-43E0-95B0-C2D2E7826EA3.png" data-src="https://i.loli.net/2019/12/09/5oRL7Dfn9Sl1vGt.jpg" class="lazyload"></p>
<p>Where $w_0$ is the global bias representing the first-order weights for the features, and $V = [v_i]$ represents the second-order weights to model the interactions across different features. $<.,.>$ is the dot product of two vectors of size K.</p>
<p>​        The parameters can be learned by minimizing the mean square loss:<img alt="mean square loss.png" data-src="https://i.loli.net/2019/12/11/k63WYeTtjAdoalO.jpg" class="lazyload"></p>
<p>Where $y^n$ is an observed rating for the n-th sample, N is the number of all the observed ratings.</p>
<p><strong>There are two problems when applying the FM model to the meta-graph based latent features:</strong></p>
<ul>
<li>It may bring noise when working with many meta-graphs thus impairing the predicting capability of the model. In practice, some meta-graphs can be useless since information provided by some meta-paths can be covered by others.</li>
<li>Computational cost.</li>
</ul>
<p>​        To alleviate the above two problems, we propose a novel regularization for FM,i.e., the group lasso regularization, which is a feature selection method on a group of variables. The group lasso regularization of parameters p is defined as follows:<img alt="6F205C97-3FB4-4FE5-B2D8-9A5A8E0B7ECA.png" data-src="https://i.loli.net/2019/12/09/THbdrAQpqiRCLlI.jpg" class="lazyload"></p>
<h3 id="2.4">Comparison with Previous Latent Feature based Model</h3>
​        The previous approaches of recommendation based on HIN is not adequate, as it fails to capture the interactions between inter-meta-graph features, i.e. features across different meta-graphs, as well as the intra-meta-graph features, i.e. It may decrease the prediction ability of all of the user and item features.



<h2 id="3">MODEL OPTIMIZATION</h2>
​        The author define FM over meta-graph(FMG) model with the following objective function:

<p><img alt="1CB932AA-8575-4885-82D5-3190F9CA0128.png" data-src="https://i.loli.net/2019/12/09/FYwu3NTrWsHG5hy.jpg" class="lazyload"></p>
<p>​        We can see now the object function is non-convex and non-smooth. So author used <strong>proximal gradient (nmAPG) algorithm</strong>.</p>
<h2 id="4"> EXPERIMENTS</h2>
<h3 id="4.1">Datasets</h3>
* Yelp (https://www.yelp.com/dataset_challenge)
* Amazon Electronics (http://jmcauley.ucsd.edu/data/amazon/)

<h3 id="4.2">Evaluation Metric</h3>
​        RMSE

<h3 id="4.3">Baseline Models</h3>
* RegSVD: RegSVD is the basic matrix factorization with L2 regularization, which uses only the user-item rating matrix. (https://www.librec.net/)
* FMR: FMR is the factorization machine with only the user-item rating matrix.(http://www.libfm.org/)
* HeteRec: HeteRec method is based on meta-path based similarity between users and items.
* SemRec: SemRec is a meta-path based recommendation on weighted HIN, which is built by connecting users and items with the same ratings.(https://github.com/zzqsmall/SemRec)

</body></html>]]></content>
      <categories>
        <category>recommendation</category>
        <category>HIN</category>
      </categories>
      <tags>
        <tag>Recommendation</tag>
        <tag>Meta-graph</tag>
        <tag>Heterogeneous Information Networks</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo+Github搭建简单博客总结</title>
    <url>/2019/11/23/Hexo+Github%E6%90%AD%E5%BB%BA%E7%AE%80%E6%98%93%E5%8D%9A%E5%AE%A2%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h1 id="Hexo-Github搭建简易博客总结"><a href="#Hexo-Github搭建简易博客总结" class="headerlink" title="Hexo+Github搭建简易博客总结"></a>Hexo+Github搭建简易博客总结</h1><p><a href="https://www.zhihu.com/question/39183612" target="_blank" rel="noopener">参考知乎问题: 如何使用10个小时搭建出个人域名而又Geek的独立博客？</a></p>
<p>前言：</p>
<p>一直没有尝试自己搭建一个博客，之前本来想着自己前后端搭建，但是懒惰。。。。。hexo+github是一个傻瓜式的自助搭建博客方式，很简便，如果是稍微有点代码基础的，应该都能在几个小时之内熟悉搭建整个过程。</p>
<p>软件准备：<a href="https://nodejs.org/en/" target="_blank" rel="noopener">Node.js</a>、<a href="https://git-scm.com/downloads" target="_blank" rel="noopener">Git</a></p>
<ol>
<li><p>先安装Hexo: <code>$ npm install -g hexo-cli</code></p>
</li>
<li><p>再初始化Hexo:<code>$ npm install</code></p>
<p>Hexo的大体文件：</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">themes 存放主题的文件夹+ source 博客资源文件夹+ source/_drafts 草稿文件夹+ source/_posts 文章文件夹+ themes/landscape 默认皮肤文件夹+ themes 存放主题的文件夹+ source 博客资源文件夹+ source/_drafts 草稿文件夹+ source/_posts 文章文件夹+ themes/landscape 默认皮肤文件夹</span><br></pre></td></tr></table></figure>

<ul>
<li>_config.yml 全局配置文件。要注意的是，该文件格式要求极为严格，缺少一个空格都会导致运行错误。小提示：不要用Tab缩进，两个空格符， <strong>冒号：后面只用一个空格即可*</strong>。</li>
</ul>
<ol>
<li><p>我采用的还是最常用的<a href="https://hexo.io/themes/" target="_blank" rel="noopener">Hexo|Next主题</a>，修改了一连串配置，包括侧栏等等</p>
</li>
<li><p>Hexo部署</p>
<p>到这里，就可以启动本地服务器打开了：<code>$ hexo s</code></p>
<p>在localhost:4000端口就可以查看到效果。</p>
<p>在git上创建了一个和本地的文件夹名称一致，我都是创见的daluzi.github.io，然后把本地的文件夹朴实到远程仓库里面。</p>
<p>接下来，在站点的config.yml文件中，部署如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">deploy:  	 type: git 部署类型若有问题，其他类型自行google之  	 repository: https://github.com/daluzi/daluzi.github.io.git  	 branch: master  	 plugins: -hexo-generator-feed</span><br></pre></td></tr></table></figure>

<p>然后</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ hexo g #生成静态网页# hexo d #开始部署</span><br></pre></td></tr></table></figure>

<p>好了，在浏览器输入daluzi.github.io就能查看博客了，接下来就是挂载域名。</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>Multimodal Machine Learning:A Survey and Taxonomy</title>
    <url>/2019/11/23/Multimodal-Machine-Learning-A-Survey-and-Taxonomy/</url>
    <content><![CDATA[<blockquote>
<blockquote>
<p>This article is a survey for multimodal machine learning. This paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. They go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: <strong>representation, translation, alignment, fusion, and co-learning.</strong></p>
</blockquote>
</blockquote>
<blockquote>
<p>Index Terms:<strong>Multimodal, machine learning, introductory, survey</strong></p>
</blockquote>
]]></content>
      <tags>
        <tag>Muilimodal</tag>
        <tag>machine learning</tag>
        <tag>survey</tag>
      </tags>
  </entry>
  <entry>
    <title>Semi-supervised Classification with Graph Convolutional Networks</title>
    <url>/2019/11/14/Semi-supervised-Classification-with-Graph-Convolutional-Networks/</url>
    <content><![CDATA[<html><head></head><body><blockquote>
<blockquote>
<p>The convolution is extended to the data of the graph structure.Motivate the choice of our convolutional architecture via a localized first-order approximation of <strong>spectral graph convolutions</strong>.The model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes.</p>
<p>In a number of experiments on <strong>citation networks</strong> and on a <strong>knowledge graph dataset</strong>,This model works well in semi-supervised tasks.</p>
</blockquote>
</blockquote>
<h1 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h1><blockquote>
<ul>
<li><a href="#1">The main content of the article</a><ul>
<li><a href="#1.1">abstract</a></li>
<li><a href="#1.2">spectral graph convolutions</a></li>
<li><a href="#1.3">layer-wise linear model</a></li>
<li><a href="#1.4">semi-supervised node classifacation</a></li>
</ul>
</li>
<li><a href="#2">My learing understanding of semi-GCN</a></li>
</ul>
</blockquote>
<h2 id="1">The main content of the article</h2>

<h3 id="1.1">abstract</h3>

<p>The authors motivate the choice of their convolutional architecture via a localized first-order approximation of <strong>spectral graph convolutions</strong>.</p>
<p>By training the structural model of convolution neural network with some tagged node data in the graph structural data ,the network model can further classify the remaining untagged data.</p>
<p>So,on the whole , the way which the convolution is extended to the data of the graph structural ,can get a better data representation ,and it works well in semi-supervised tasks.</p>
<blockquote>
<p>Datasets: citation network datasets(Citeseer ,Cora ,Pubmed) ,bipartite graph dataset(NELL) </p>
</blockquote>
<h3 id="1.2">spectral graph convolutions</h3>

<p><em>In this period ,I mainly refer to this website:<a href="https://blog.csdn.net/qq_41727666/article/details/84622965" target="_blank" rel="noopener">https://blog.csdn.net/qq_41727666/article/details/84622965</a></em></p>
<p>In fact ,there are two versions of GCN:</p>
<p>1.The first generation of GCN is this formula:<strong>$$g{\theta} * x = Ug{\theta}U^Tx$$</strong>.</p>
<p>In the formula ,$x$ is the eigenvector of the graph node ,$g{\theta}=diag(\theta)$ is the convolution kernel.$U$ is the matrix of eigenvectors of the normalized graph Laplacian $L$. And the $L = I_N - D^{-1/2}AD^{-1/2} = U{\Lambda}U^T$</p>
<p><strong>But ,this is too complicated to calculate</strong></p>
<p>2.To circumvent this problem ,$g{\theta}(\Lambda)$ can be well-approximated by a truncated expansion in terms of Chebyshev polynomial $T_k(x)$ up to $K^{th}$ order:</p>
<p>$$g{\theta}^<code>{\approx} {\sum_{k=0}^{K}{\theta}^</code>T_k(\Lambda^1)}$$</p>
<p>With a rescaled $\Lambda^1 = {\frac{2}{\lambda_{max}}}\Lambda - I_N$. $\lambda_{max}$ Denotes the largest eigenvalue of $L$. $\theta^`$ is now a vector of Chebyshev coefficients. The Chebyshev polynomials are recursively defined as $T_k(x) = 2xT_{k-1}(x) - T_{k-2}(x)$, with $T_0(x) = 1$ and $T_1(x) = x$.</p>
<p>So, now the formula is:</p>
<p>$$g{\theta}^<code>* x \approx {\sum_{k=0}^{K}{\theta}_k^</code>T_k(L^1)x}$$</p>
<p>With $L^1 = {\frac{2}{\lambda_{max}}L - I_N}$ .This expression is now K-localized since it is a <strong>$K^{th}$-order ploynomial in the Laplacian</strong>.</p>
<h3 id="1.3">layer-wise linear model</h3>

<p>The authors approximate $\lambda_{max} \approx 2$, so now the formula is simplified to:</p>
<p><img alt="semi_eq5.png" data-src="https://i.loli.net/2019/11/17/LdqIwO9NfxzXWJv.png" class="lazyload"></p>
<h3 id="1.4">semi-supervised node classification</h3>

<p>In this example, they consider a two-layer GCN for semi-supervised node classification on a graph with a symmetric adjacency matrix A(binary or weighted). </p>
<p>First, calculate $A^` = D1^{-1/2}A^1D1^{-1/2}$.</p>
<p>And the forward model then takes the simple form:$$Z = f(X,A) = softmax(A^<code>{ReLU}(A^</code>XW^{(0)})W^{(1)})$$</p>
<p>Here, $W^{(0)}$ is an input-to-hidden weight matrix for a hidden layer with H feature maps.</p>
<p>$W^{(1)}$ is a hidden-to-output weight matrix.</p>
<p>The softmax activation function defined as $softmax(x_i) = \frac{1}{\sum_i{exp(x_i)}}exp(x_i)$</p>
<p>For semi-supervised multiclass classification, they then evaluate the cross-entropy error over all labeled examples :$\xi = -\sum_{l\epsilon{y_l}}\sum_{f=1}^{F}Y_{lf}lnZ_{lf}$</p>
<p>The author perform <strong>batch gradient descent</strong>. Stochasticity in the training process is in produced via <strong>dropout</strong>. </p>
<h2 id="2">My learing understanding of semi-GCN</h2>

<p><em>mainly depend on <a href="https://zhuanlan.zhihu.com/p/58178060" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/58178060</a></em></p>
</body></html>]]></content>
      <tags>
        <tag>GCN</tag>
        <tag>Deeplearning</tag>
      </tags>
  </entry>
  <entry>
    <title>Personalized Recommendation Combining User Interest and Social Circle</title>
    <url>/2019/11/13/Personalized-Recommendation-Combining-User-Interest-and-Social-Circle/</url>
    <content><![CDATA[<blockquote>
<blockquote>
<p>Interpersonal influence and interest based on circles of friends can be used to solve cold start and sparsity problem of datasets.This paper used three social factors:<strong>personal interest</strong>,<strong>interpersonal interest similarity</strong> and <strong>interpersonal influence</strong> to make personalized recommendation.</p>
</blockquote>
</blockquote>
<h1 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h1><blockquote>
<ul>
<li><a href="#1">Introduction</a></li>
<li><a href="#2">Problem formulation</a></li>
<li><a href="#3">Related work</a><ul>
<li><a href="3.1">Basic matrix factorization</a></li>
<li><a href="3.2">CircleCon model</a></li>
<li><a href="3.3">ContextMF model</a></li>
</ul>
</li>
<li><a href="4">The approach</a><ul>
<li><a href="4.1">User interest factor</a></li>
<li><a href="4.2">Personalized recommendation model</a></li>
<li><a href="4.3">Model training</a></li>
</ul>
</li>
<li><a href="5">Experiments</a><ul>
<li><a href="5.1">Datasets</a></li>
<li><a href="5.2">Performance measures</a></li>
<li><a href="5.3">Evaluation</a></li>
</ul>
</li>
</ul>
</blockquote>
<h2 id='1'>Introduction</h2>
Recommender system(RS) has been successfully exploited to solve information overload.Many methods proposed to improve the performance of RS.And there are two main problems needed to solve:**1.the problems of cold start for users(new users into the RS with little historical behavior)2.the sparsity of datasets(the proportion of rated user-item pairs in all the user-item pairs of RS)**.

<p>In this paper, three social factors,<strong>personal interest,interpersonal interest similarity,and interpersonal influence</strong>, fuse into a unified personalized recommendation model based on probabilistic matrix factorization.</p>
<h2 id='2'>Problem formulation</h2>
]]></content>
      <tags>
        <tag>RS</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2019/11/12/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
  </entry>
</search>
