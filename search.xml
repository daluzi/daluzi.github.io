<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>HIN-Graph</title>
    <url>/2020/04/03/HIN-Graph/</url>
    <content><![CDATA[<blockquote>
<blockquote>
<p>Summarying the recent works on <strong>heterogeneous graph neural networks</strong></p>
</blockquote>
</blockquote>
<h2>Links</h2>

<p>清华朱文武老师组关于图上的深度学习综述(2018) <a href="https://arxiv.org/abs/1812.04202v1" target="_blank" rel="noopener" title="With a Title">Deep Learning on Graphs: A Survey</a>. </p>
<p>IEEE Fellow关于图神经网络综述(2019) <a href="https://arxiv.org/abs/1901.00596" target="_blank" rel="noopener" title="With a Title">A Comprehensive Survey on Graph Neural Networks</a>. </p>
<p>Ziniu Hu关于异质图Transformer的研究(2020) <a href="https://arxiv.org/abs/2003.01332" target="_blank" rel="noopener" title="With a Title">Heterogeneous Graph Transformer</a>. </p>
<p>纪厚业做的分享： <a href="https://www.bilibili.com/video/BV1HE41157GD?t=6492" target="_blank" rel="noopener" title="With a Title">异质图神经网络：模型和应用</a>. 介绍了异质图网络、以及一些模型和三个实际落地的论文</p>
<p>发表在数据库顶会VLDB上的最新异质图表示学习的综述(2020) <a href="https://arxiv.org/abs/1801.05852" target="_blank" rel="noopener" title="With a Title">Heterogeneous Network Representation Learning: Survey, Benchmark, Evaluation, and Beyond</a>.</p>
<p>图数据上的对抗攻击和防御综述(2020) <a href="https://arxiv.org/abs/1812.10528v1" target="_blank" rel="noopener" title="With a Title">Adversarial Attack and Defense on Graph Data: A Survey</a>. </p>
<p>用于构建动态图的例子《Dynamic Network Embedding by Modelling Triadic Closure Process(2018)》 <a href="https://github.com/luckiezhou/DynamicTriad" target="_blank" rel="noopener" title="With a Title">https://github.com/luckiezhou/DynamicTriad</a>. </p>
<hr>
<p>主流知识图谱表示学习算法:</p>
<ul>
<li>TransE、ComplEx、DistMult、TransR、RESCAL、RotatE </li>
</ul>
]]></content>
      <tags>
        <tag>HIN</tag>
        <tag>graph-learning</tag>
        <tag>GNN/GCN</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker</title>
    <url>/2020/06/28/Docker/</url>
    <content><![CDATA[<h2 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h2><p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/06/28/hnm5d3vaqzFY9sJ.png" alt="docker.png" style="zoom: 80%;" /></p>
<h4 id="purpose："><a href="#purpose：" class="headerlink" title="purpose："></a>purpose：</h4><p>​    解决环境配置麻烦的问题。</p>
<h4 id="other-options"><a href="#other-options" class="headerlink" title="other options:"></a>other options:</h4><ul>
<li>虚拟机：资源占用多、冗余步骤多、启动慢。</li>
<li>Linux容器(LXC)：LXC不是模拟一个完整的操作系统，而是对进程进行隔离。即在正常进行的外面套了一个保护层，对于容器里面的进程来说，它接触到的各种资源都是虚拟的。从而实现与底层系统的隔离。启动快、资源占用少、体积小。</li>
</ul>
<hr>
<h4 id="Docker-install"><a href="#Docker-install" class="headerlink" title="Docker install"></a>Docker install</h4><p>​    属于LXC的一种封装，提供简单易用的容器使用接口，是目前最流行的LXC。</p>
<p>​    Docker 将应用程序与该程序的依赖，打包在一个文件里面。运行这个文件，就会生成一个虚拟容器。程序在这个虚拟容器里运行，就好像在真实的物理机上运行一样。有了 Docker，就不用担心环境问题。总体来说，Docker 的接口相当简单，用户可以方便地创建和使用容器，把自己的应用放入容器。容器还可以进行版本管理、复制、分享、修改，就像管理普通的代码一样。</p>
<p>​    Docker 需要用户具有 sudo 权限，为了避免每次命令都输入<code>sudo</code>，可以把用户加入 Docker 用户组（<a href="https://docs.docker.com/install/linux/linux-postinstall/#manage-docker-as-a-non-root-user" target="_blank" rel="noopener">官方文档</a>）。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ sudo usermod -aG docker <span class="variable">$USER</span></span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    Docker 是服务器——客户端架构。命令行运行<code>docker</code>命令的时候，需要本机有 Docker 服务。如果这项服务没有启动，可以用下面的命令启动（<a href="https://docs.docker.com/config/daemon/systemd/" target="_blank" rel="noopener">官方文档</a>）。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># service 命令的用法</span></span><br><span class="line">$ sudo service docker start</span><br><span class="line"></span><br><span class="line"><span class="comment"># systemctl 命令的用法</span></span><br><span class="line">$ sudo systemctl start docker</span><br></pre></td></tr></table></figure>
</blockquote>
<h4 id="image文件"><a href="#image文件" class="headerlink" title="image文件"></a>image文件</h4><p>​    <strong>Docker 把应用程序及其依赖，打包在 image 文件里面。</strong>只有通过这个文件，才能生成 Docker 容器。image 文件可以看作是容器的模板。Docker 根据 image 文件生成容器的实例。同一个 image 文件，可以生成多个同时运行的容器实例。</p>
<p>​    image 是二进制文件。实际开发中，一个 image 文件往往通过继承另一个 image 文件，加上一些个性化设置而生成。举例来说，你可以在 Ubuntu 的 image 基础上，往里面加入 Apache 服务器，形成你的 image。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 列出本机的所有 image 文件。</span></span><br><span class="line">$ docker image ls</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除 image 文件</span></span><br><span class="line">$ docker image rm [imageName]</span><br></pre></td></tr></table></figure>
<p>​    image 文件是通用的，一台机器的 image 文件拷贝到另一台机器，照样可以使用。一般来说，为了节省时间，我们应该尽量使用别人制作好的 image 文件，而不是自己制作。即使要定制，也应该基于别人的 image 文件进行加工，而不是从零开始制作。</p>
<p>​    为了方便共享，image 文件制作完成后，可以上传到网上的仓库。Docker 的官方仓库 <a href="https://hub.docker.com/" target="_blank" rel="noopener">Docker Hub</a> 是最重要、最常用的 image 仓库。此外，出售自己制作的 image 文件也是可以的。</p>
<h4 id="hello-world实例"><a href="#hello-world实例" class="headerlink" title="hello world实例"></a>hello world实例</h4><p>​    首先，运行下面的命令，将 image 文件从仓库抓取到本地。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker image pull library/hello-world</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    上面代码中，<code>docker image pull</code>是抓取 image 文件的命令。<code>library/hello-world</code>是 image 文件在仓库里面的位置，其中<code>library</code>是 image 文件所在的组，<code>hello-world</code>是 image 文件的名字。</p>
<p>​    由于 Docker 官方提供的 image 文件，都放在<a href="https://hub.docker.com/r/library/" target="_blank" rel="noopener"><code>library</code></a>组里面，所以它的是默认组，可以省略。因此，上面的命令可以写成下面这样。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker image pull hello-world</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    抓取成功以后，就可以在本机看到这个 image 文件了。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker image ls</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    现在，运行这个 image 文件。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker container run hello-world</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    <code>docker container run</code>命令会从 image 文件，生成一个正在运行的容器实例。</p>
<p>​    注意，<code>docker container run</code>命令具有自动抓取 image 文件的功能。如果发现本地没有指定的 image 文件，就会从仓库自动抓取。因此，前面的<code>docker image pull</code>命令并不是必需的步骤。</p>
<p>如果运行成功，你会在屏幕上读到下面的输出。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker container run hello-world</span><br><span class="line"></span><br><span class="line">Hello from Docker!</span><br><span class="line">This message shows that your installation appears to be working correctly.</span><br><span class="line"></span><br><span class="line">... ...</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    输出这段提示以后，<code>hello world</code>就会停止运行，容器自动终止。</p>
<p>​    有些容器不会自动终止，因为提供的是服务。比如，安装运行 Ubuntu 的 image，就可以在命令行体验 Ubuntu 系统。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker container run -it ubuntu bash</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    对于那些不会自动终止的容器，必须使用<a href="https://docs.docker.com/engine/reference/commandline/container_kill/" target="_blank" rel="noopener"><code>docker container kill</code></a> 命令手动终止。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker container <span class="built_in">kill</span> [containID]</span><br></pre></td></tr></table></figure>
</blockquote>
<h4 id="容器文件"><a href="#容器文件" class="headerlink" title="容器文件"></a>容器文件</h4><p>​    <strong>image 文件生成的容器实例，本身也是一个文件，称为容器文件。</strong>也就是说，一旦容器生成，就会同时存在两个文件： image 文件和容器文件。而且关闭容器并不会删除容器文件，只是容器停止运行而已。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 列出本机正在运行的容器</span></span><br><span class="line">$ docker container ls</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出本机所有容器，包括终止运行的容器</span></span><br><span class="line">$ docker container ls --all</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    上面命令的输出结果之中，包括容器的 ID。很多地方都需要提供这个 ID，比如上一节终止容器运行的<code>docker container kill</code>命令。</p>
<p>​    终止运行的容器文件，依然会占据硬盘空间，可以使用<a href="https://docs.docker.com/engine/reference/commandline/container_rm/" target="_blank" rel="noopener"><code>docker container rm</code></a>命令删除。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker container rm [containerID]</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    运行上面的命令之后，再使用<code>docker container ls --all</code>命令，就会发现被删除的容器文件已经消失了。</p>
<h4 id="Dockerfile文件"><a href="#Dockerfile文件" class="headerlink" title="Dockerfile文件"></a>Dockerfile文件</h4><p>​    学会使用 image 文件以后，接下来的问题就是，如何可以生成 image 文件？如果你要推广自己的软件，势必要自己制作 image 文件。</p>
<p>​    这就需要用到 Dockerfile 文件。它是一个文本文件，用来配置 image。Docker 根据 该文件生成二进制的 image 文件。</p>
<p>​    下面通过一个实例，演示如何编写 Dockerfile 文件。</p>
<h4 id="实例：制作自己的Docker容器"><a href="#实例：制作自己的Docker容器" class="headerlink" title="实例：制作自己的Docker容器"></a>实例：制作自己的Docker容器</h4><p>​    下面我以 <a href="http://www.ruanyifeng.com/blog/2017/08/koa.html" target="_blank" rel="noopener">koa-demos</a> 项目为例，介绍怎么写 Dockerfile 文件，实现让用户在 Docker 容器里面运行 Koa 框架。</p>
<p>​    作为准备工作，请先<a href="https://github.com/ruanyf/koa-demos/archive/master.zip" target="_blank" rel="noopener">下载源码</a>。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> https://github.com/ruanyf/koa-demos.git</span><br><span class="line">$ <span class="built_in">cd</span> koa-demos</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    <b>1 编写 Dockerfile 文件</b></p>
<p>​    首先，在项目的根目录下，新建一个文本文件<code>.dockerignore</code>，写入下面的<a href="https://github.com/ruanyf/koa-demos/blob/master/.dockerignore" target="_blank" rel="noopener">内容</a>。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">.git</span><br><span class="line">node_modules</span><br><span class="line">npm-debug.log</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    上面代码表示，这三个路径要排除，不要打包进入 image 文件。如果你没有路径要排除，这个文件可以不新建。</p>
<p>​    然后，在项目的根目录下，新建一个文本文件 Dockerfile，写入下面的<a href="https://github.com/ruanyf/koa-demos/blob/master/Dockerfile" target="_blank" rel="noopener">内容</a>。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">FROM node:8.4</span><br><span class="line">COPY . /app</span><br><span class="line">WORKDIR /app</span><br><span class="line">RUN npm install --registry=https://registry.npm.taobao.org</span><br><span class="line">EXPOSE 3000</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    上面代码一共五行，含义如下。</p>
<blockquote>
<ul>
<li><code>FROM node:8.4</code>：该 image 文件继承官方的 node image，冒号表示标签，这里标签是<code>8.4</code>，即8.4版本的 node。</li>
<li><code>COPY . /app</code>：将当前目录下的所有文件（除了<code>.dockerignore</code>排除的路径），都拷贝进入 image 文件的<code>/app</code>目录。</li>
<li><code>WORKDIR /app</code>：指定接下来的工作路径为<code>/app</code>。</li>
<li><code>RUN npm install</code>：在<code>/app</code>目录下，运行<code>npm install</code>命令安装依赖。注意，安装后所有的依赖，都将打包进入 image 文件。</li>
<li><code>EXPOSE 3000</code>：将容器 3000 端口暴露出来， 允许外部连接这个端口。</li>
</ul>
</blockquote>
<p>​    <b>10.2 创建 image 文件</b></p>
<p>​    有了 Dockerfile 文件以后，就可以使用<code>docker image build</code>命令创建 image 文件了。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker image build -t koa-demo .</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">$ docker image build -t koa-demo:0.0.1 .</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    上面代码中，<code>-t</code>参数用来指定 image 文件的名字，后面还可以用冒号指定标签。如果不指定，默认的标签就是<code>latest</code>。最后的那个点表示 Dockerfile 文件所在的路径，上例是当前路径，所以是一个点。</p>
<p>​    如果运行成功，就可以看到新生成的 image 文件<code>koa-demo</code>了。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker image ls</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    <b>10.3 生成容器</b></p>
<p>​    <code>docker container run</code>命令会从 image 文件生成容器。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker container run -p 8000:3000 -it koa-demo /bin/bash</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">$ docker container run -p 8000:3000 -it koa-demo:0.0.1 /bin/bash</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    上面命令的各个参数含义如下：</p>
<blockquote>
<ul>
<li><code>-p</code>参数：容器的 3000 端口映射到本机的 8000 端口。</li>
<li><code>-it</code>参数：容器的 Shell 映射到当前的 Shell，然后你在本机窗口输入的命令，就会传入容器。</li>
<li><code>koa-demo:0.0.1</code>：image 文件的名字（如果有标签，还需要提供标签，默认是 latest 标签）。</li>
<li><code>/bin/bash</code>：容器启动以后，内部第一个执行的命令。这里是启动 Bash，保证用户可以使用 Shell。</li>
</ul>
</blockquote>
<p>​    如果一切正常，运行上面的命令以后，就会返回一个命令行提示符。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@66d80f4aaf1e:/app<span class="comment">#</span></span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    这表示你已经在容器里面了，返回的提示符就是容器内部的 Shell 提示符。执行下面的命令。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@66d80f4aaf1e:/app<span class="comment"># node demos/01.js</span></span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    这时，Koa 框架已经运行起来了。打开本机的浏览器，访问 <a href="http://127.0.0.1:8000，网页显示&quot;Not" target="_blank" rel="noopener">http://127.0.0.1:8000，网页显示&quot;Not</a> Found”，这是因为这个 <a href="https://github.com/ruanyf/koa-demos/blob/master/demos/01.js" target="_blank" rel="noopener">demo</a> 没有写路由。</p>
<p>​    这个例子中，Node 进程运行在 Docker 容器的虚拟环境里面，进程接触到的文件系统和网络接口都是虚拟的，与本机的文件系统和网络接口是隔离的，因此需要定义容器与物理机的端口映射（map）。</p>
<p>​    现在，在容器的命令行，按下 Ctrl + c 停止 Node 进程，然后按下 Ctrl + d （或者输入 exit）退出容器。此外，也可以用<code>docker container kill</code>终止容器运行。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在本机的另一个终端窗口，查出容器的 ID</span></span><br><span class="line">$ docker container ls</span><br><span class="line"></span><br><span class="line"><span class="comment"># 停止指定的容器运行</span></span><br><span class="line">$ docker container <span class="built_in">kill</span> [containerID]</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    容器停止运行之后，并不会消失，用下面的命令删除容器文件。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查出容器的 ID</span></span><br><span class="line">$ docker container ls --all</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除指定的容器文件</span></span><br><span class="line">$ docker container rm [containerID]</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    也可以使用<code>docker container run</code>命令的<code>--rm</code>参数，在容器终止运行后自动删除容器文件。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker container run --rm -p 8000:3000 -it koa-demo /bin/bash</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    <b>10.4 CMD 命令</b></p>
<p>​    上一节的例子里面，容器启动以后，需要手动输入命令<code>node demos/01.js</code>。我们可以把这个命令写在 Dockerfile 里面，这样容器启动以后，这个命令就已经执行了，不用再手动输入了。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">FROM node:8.4</span><br><span class="line">COPY . /app</span><br><span class="line">WORKDIR /app</span><br><span class="line">RUN npm install --registry=https://registry.npm.taobao.org</span><br><span class="line">EXPOSE 3000</span><br><span class="line">CMD node demos/01.js</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    上面的 Dockerfile 里面，多了最后一行<code>CMD node demos/01.js</code>，它表示容器启动后自动执行<code>node demos/01.js</code>。</p>
<p>​    你可能会问，<code>RUN</code>命令与<code>CMD</code>命令的区别在哪里？简单说，<code>RUN</code>命令在 image 文件的构建阶段执行，执行结果都会打包进入 image 文件；<code>CMD</code>命令则是在容器启动后执行。另外，一个 Dockerfile 可以包含多个<code>RUN</code>命令，但是只能有一个<code>CMD</code>命令。</p>
<p>​    注意，指定了<code>CMD</code>命令以后，<code>docker container run</code>命令就不能附加命令了（比如前面的<code>/bin/bash</code>），否则它会覆盖<code>CMD</code>命令。现在，启动容器可以使用下面的命令。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker container run --rm -p 8000:3000 -it koa-demo:0.0.1</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    <b>10.5 发布 image 文件</b></p>
<p>​    容器运行成功后，就确认了 image 文件的有效性。这时，我们就可以考虑把 image 文件分享到网上，让其他人使用。</p>
<p>​    首先，去 <a href="https://hub.docker.com/" target="_blank" rel="noopener">hub.docker.com</a> 或 <a href="https://cloud.docker.com/" target="_blank" rel="noopener">cloud.docker.com</a> 注册一个账户。然后，用下面的命令登录。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker login</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    接着，为本地的 image 标注用户名和版本。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker image tag [imageName] [username]/[repository]:[tag]</span><br><span class="line"><span class="comment"># 实例</span></span><br><span class="line">$ docker image tag koa-demos:0.0.1 ruanyf/koa-demos:0.0.1</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    也可以不标注用户名，重新构建一下 image 文件。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker image build -t [username]/[repository]:[tag] .</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    最后，发布 image 文件。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker image push [username]/[repository]:[tag]</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    发布成功以后，登录 hub.docker.com，就可以看到已经发布的 image 文件。</p>
<h4 id="其他有用的命令"><a href="#其他有用的命令" class="headerlink" title="其他有用的命令"></a>其他有用的命令</h4><p>​    docker 的主要用法就是上面这些，此外还有几个命令，也非常有用。</p>
<p>​    <strong>（1）docker container start</strong></p>
<p>​    前面的<code>docker container run</code>命令是新建容器，每运行一次，就会新建一个容器。同样的命令运行两次，就会生成两个一模一样的容器文件。如果希望重复使用容器，就要使用<code>docker container start</code>命令，它用来启动已经生成、已经停止运行的容器文件。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker container start [containerID]</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    <strong>（2）docker container stop</strong></p>
<p>​    前面的<code>docker container kill</code>命令终止容器运行，相当于向容器里面的主进程发出 SIGKILL 信号。而<code>docker container stop</code>命令也是用来终止容器运行，相当于向容器里面的主进程发出 SIGTERM 信号，然后过一段时间再发出 SIGKILL 信号。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ bash container stop [containerID]</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    这两个信号的差别是，应用程序收到 SIGTERM 信号以后，可以自行进行收尾清理工作，但也可以不理会这个信号。如果收到 SIGKILL 信号，就会强行立即终止，那些正在进行中的操作会全部丢失。</p>
<p>​    <strong>（3）docker container logs</strong></p>
<p><code>docker container logs</code>命令用来查看 docker 容器的输出，即容器里面 Shell 的标准输出。如果<code>docker run</code>命令运行容器的时候，没有使用<code>-it</code>参数，就要用这个命令查看输出。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker container logs [containerID]</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    <strong>（4）docker container exec</strong></p>
<p>​    <code>docker container exec</code>命令用于进入一个正在运行的 docker 容器。如果<code>docker run</code>命令运行容器的时候，没有使用<code>-it</code>参数，就要用这个命令进入容器。一旦进入了容器，就可以在容器的 Shell 执行命令了。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker container <span class="built_in">exec</span> -it [containerID] /bin/bash</span><br></pre></td></tr></table></figure>
</blockquote>
<p>​    <strong>（5）docker container cp</strong></p>
<p>​    <code>docker container cp</code>命令用于从正在运行的 Docker 容器里面，将文件拷贝到本机。下面是拷贝到当前目录的写法。</p>
<blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker container cp [containID]:[/path/to/file] .</span><br></pre></td></tr></table></figure>
</blockquote>
<h2 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h2><p>​    Windows上安装了docker之后，netkeeper报如下错误：</p>
<p>​    <b>sorry,this application cannot be run under a Virtual Machine</b></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/06/29/bpL5eSUHEQGAOtT.png" alt="error.png" style="zoom:50%;" /></p>
<p>​    搜索了到如下几种参考方法(直接看第三种即可)：</p>
<p>​    1.开机时打开BIOS，关闭virtual tchnology这一选项,具体操作参考下面链接(<a href="http://www.udaxia.com/upqd/5254.html).但对于我没法尝试.(失败" target="_blank" rel="noopener">http://www.udaxia.com/upqd/5254.html).但对于我没法尝试.(失败</a>)<br>​    2.在控制面板→程序→启用或关闭 Windows 功能下,取消Hyper-V选项,重启电脑,参考下面链接（<a href="https://blog.csdn.net/sdut15110581043/article/details/53330481/).尝试无数遍,重启无数遍,均失败.(失败" target="_blank" rel="noopener">https://blog.csdn.net/sdut15110581043/article/details/53330481/).尝试无数遍,重启无数遍,均失败.(失败</a>)</p>
<p>​    <b>可忽略上面失败的解决方案</b><br>​    3.禁用Hyper-V正确姿势来了(第二种方法不行)<br>​    ①打开Windows PowerShell（管理员）</p>
<p>​    ②运行命令bcdedit /set hypervisorlaunchtype off<br>​    注意:上述命令有一个空格的存在</p>
<p>​    ③重启计算机</p>
<p>​    如果要重新开启Hyper-V，按照方法3执行bcdedit / set hypervisorlaunchtype auto 命令并重启计算机即可。</p>
<p>原文链接：<a href="https://blog.csdn.net/qq_43942195/article/details/88600624" target="_blank" rel="noopener">https://blog.csdn.net/qq_43942195/article/details/88600624</a></p>
<h4 id="reference"><a href="#reference" class="headerlink" title="reference:"></a>reference:</h4><ul>
<li><a href="https://docs.docker.com/" target="_blank" rel="noopener">Docker官网</a></li>
<li><a href="http://www.ruanyifeng.com/blog/2018/02/docker-tutorial.html" target="_blank" rel="noopener">阮一峰的Dockers入门</a></li>
</ul>
]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo+Github搭建简单博客总结</title>
    <url>/2019/11/23/Hexo+Github%E6%90%AD%E5%BB%BA%E7%AE%80%E6%98%93%E5%8D%9A%E5%AE%A2%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h1 id="Hexo-Github搭建简易博客总结"><a href="#Hexo-Github搭建简易博客总结" class="headerlink" title="Hexo+Github搭建简易博客总结"></a>Hexo+Github搭建简易博客总结</h1><p><a href="https://www.zhihu.com/question/39183612" target="_blank" rel="noopener">参考知乎问题: 如何使用10个小时搭建出个人域名而又Geek的独立博客？</a></p>
<p>前言：</p>
<p>一直没有尝试自己搭建一个博客，之前本来想着自己前后端搭建，但是懒惰。。。。。hexo+github是一个傻瓜式的自助搭建博客方式，很简便，如果是稍微有点代码基础的，应该都能在几个小时之内熟悉搭建整个过程。</p>
<p>软件准备：<a href="https://nodejs.org/en/" target="_blank" rel="noopener">Node.js</a>、<a href="https://git-scm.com/downloads" target="_blank" rel="noopener">Git</a></p>
<ol>
<li>先安装Hexo: <code>$ npm install -g hexo-cli</code></li>
<li><p>再初始化Hexo:<code>$ npm install</code></p>
<p>Hexo的大体文件：</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">themes 存放主题的文件夹+ source 博客资源文件夹+ source/_drafts 草稿文件夹+ source/_posts 文章文件夹+ themes/landscape 默认皮肤文件夹+ themes 存放主题的文件夹+ source 博客资源文件夹+ source/_drafts 草稿文件夹+ source/_posts 文章文件夹+ themes/landscape 默认皮肤文件夹</span><br></pre></td></tr></table></figure>
<ul>
<li>_config.yml 全局配置文件。要注意的是，该文件格式要求极为严格，缺少一个空格都会导致运行错误。小提示：不要用Tab缩进，两个空格符， <strong>冒号：后面只用一个空格即可*</strong>。</li>
</ul>
<ol>
<li><p>我采用的还是最常用的<a href="https://hexo.io/themes/" target="_blank" rel="noopener">Hexo|Next主题</a>，修改了一连串配置，包括侧栏等等</p>
</li>
<li><p>Hexo部署</p>
<p>到这里，就可以启动本地服务器打开了：<code>$ hexo s</code></p>
<p>在localhost:4000端口就可以查看到效果。</p>
<p>在git上创建了一个和本地的文件夹名称一致，我都是创见的daluzi.github.io，然后把本地的文件夹朴实到远程仓库里面。</p>
<p>接下来，在站点的config.yml文件中，部署如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">deploy:  	 type: git 部署类型若有问题，其他类型自行google之  	 repository: https://github.com/daluzi/daluzi.github.io.git  	 branch: master  	 plugins: -hexo-generator-feed</span><br></pre></td></tr></table></figure>
<p>然后</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ hexo g #生成静态网页# hexo d #开始部署</span><br></pre></td></tr></table></figure>
<p>好了，在浏览器输入daluzi.github.io就能查看博客了，接下来就是挂载域名。</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>KG_learning</title>
    <url>/2020/06/22/KG-learning/</url>
    <content><![CDATA[<h3>知识图谱</h3>

<hr>
<p><em>以结构化的形式描述客观世界中的概念、实体及其关系，将互联网的信息表达成更接近人类认知世界的形式，提供了一种更好的组织、管理和理解互联网海量信息的能力。</em></p>
<p><b>Google</b>首先提出。使得谷歌的搜索结果能给出更精准的答案。</p>
<p>知识图谱推理，知识图谱可视化</p>
<p>机器学习、图数据库（如Neo4j）、自然语言等技术的成熟，使得知识图谱变火。</p>
<p>语义网络-&gt;本体论-&gt;web-&gt;语义网（大规模,也即知识图谱）</p>
<p>————————————-分割线———————————</p>
<p><b>典型的知识图谱：</b></p>
<p>按照知识的主客观性：事实性知识、主观性知识</p>
<p>根据知识的变化性质：静态知识、动态</p>
<p>根据领域知识：通用知识、领域知识</p>
<p>因此常见的知识图谱：</p>
<ul>
<li>基于专家知识/人工构建：1. Cyc；2.WordNet</li>
<li>基于众包数据和其他知识图谱：1.ConceptNet; 2.YAGO; 3.Wikidata; 4.BDpedia; 5.Freebase; 6.BabelNet</li>
<li>基于机器学习：1.NELL; 2.Knowledge Vault; 3. WOE; 4.ReVerb</li>
<li>企业知识图谱：1.Google KG; 2.百度知心; 3.搜狗知立方</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">graph LR</span><br><span class="line">A[常见的知识图谱] --&gt; B&#123;基于专家知识/人工构建&#125; </span><br><span class="line">A[常见的知识图谱] --&gt; B1&#123;基于众包数据和其他知识图谱&#125;</span><br><span class="line">A[常见的知识图谱] --&gt; B2&#123;基于机器学习&#125; </span><br><span class="line">A[常见的知识图谱] --&gt; B3&#123;企业知识图谱&#125; </span><br><span class="line">		B --&gt;  C[Cyc]</span><br><span class="line">		B --&gt;  D[WordNet]</span><br><span class="line">		B1 --&gt; C1[ConceptNet]</span><br><span class="line">		B1 --&gt; C2[YAGO]		</span><br><span class="line">		B1 --&gt; C3[Wikidata]		</span><br><span class="line">		B1 --&gt; C4[BDpedia]		</span><br><span class="line">		B1 --&gt; C5[Freebase]		</span><br><span class="line">		B1 --&gt; C6[BabelNet]</span><br><span class="line">    B2 --&gt; C7[NELL]</span><br><span class="line">    B2 --&gt; C8[Knowledge Vault]    </span><br><span class="line">    B2 --&gt; C9[WOE]    </span><br><span class="line">    B2 --&gt; C10[Reverb]</span><br><span class="line">    B3 --&gt; C11[Google KG]</span><br><span class="line">    B3 --&gt; C12[百度知心]</span><br><span class="line">    B3 --&gt; C13[搜狗知立方]</span><br></pre></td></tr></table></figure>
<p><b>知识图谱的主要技术</b></p>
<ul>
<li><p>知识问答</p>
</li>
<li><p>语义搜索</p>
</li>
<li><p>可视化</p>
</li>
<li><p>知识链接</p>
</li>
<li><p>知识推理：知识补全、自动问答系统</p>
</li>
<li><p>知识众包</p>
</li>
<li><p>知识融合：对不同来源、不同语言或结构的知识进行融合，从而对已有知识图谱进行补充、更新或去重。</p>
</li>
<li><p>知识抽取</p>
</li>
<li><p>知识表示</p>
</li>
<li><p>知识存储：研究采用何种方式将已有的知识图谱进行存储。基于图的数据结构，存储方式主要是两种形式：1，RDF格式存储—-Apache Jena；2，图数据库———Neo4j</p>
</li>
</ul>
<p>————————————-分割线———————————</p>
<p><b>知识表示和建模</b></p>
<p>一阶谓词逻辑、产生式系统、框架表示法、语义网络</p>
<p><em>（语义网络不等价于语义网，语义网络中的弧表示各种语义联系，指明它所连接的节点间某种语义关系。语义网络的推理规则不十分明了。<b>语义网络重在知识的表示，对实体间的关系等不太能表示，所以和知识图谱不同，大的语义网即为知识图谱</b>）</em></p>
<p>RDF（Resource Description Framework 资源描述框架）本质上是一组表示知识的语法，由若干个三元组的组合表达。</p>
<p>RDF序列化(即存储)：方式主要有：RDF/XML, N-Triples, Turtle, RDFa, JSON-LD等</p>
<p>RDF和RDFS</p>
<p>OWL（网络本体语言）：OWL Lite  OWL DL  OWL FULL</p>
<p>SPARQL语言、cypher QL语言</p>
<p>————————————-分割线———————————</p>
<p><b>知识图谱数据存储</b></p>
<p>方式：</p>
<ul>
<li><p>基于关系型数据库的存储方式</p>
</li>
<li><p>面向RDF的存储方式</p>
</li>
<li><p>图数据库存储方式</p>
</li>
</ul>
<p>————————————-分割线———————————</p>
<p><b>PageRank</b></p>
<p>一个网页i的重要度可以使用指向网页i的其他网页j的重要度加权得到。</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/06/22/iEIjbymsGqJtgCp.jpg" alt="98A3E36B-2A01-42C4-9435-C658C3CA5294.png" style="zoom:50%;" /></p>
<p><b>TextRank</b></p>
<p>将PageRank的“词”改成“句子”</p>
<p><b>BRAT</b></p>
<p><a href="http://brat.nlplab.org" target="_blank" rel="noopener">http://brat.nlplab.org</a></p>
<p>用于人工做实体标注的工具</p>
<p><b>word embedding</b></p>
<p><b>word2vec</b></p>
<p><b>Skipgram</b></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/06/22/iWensqMk6bSpEog.jpg" alt="skipgram.png"></p>
]]></content>
      <categories>
        <category>KG</category>
      </categories>
      <tags>
        <tag>KG</tag>
      </tags>
  </entry>
  <entry>
    <title>Heterogeneous Graph Neural Network</title>
    <url>/2020/05/20/Heterogeneous%20Graph%20Neural%20Network/</url>
    <content><![CDATA[<blockquote>
<blockquote>
<p>Zhang C , Song D , Huang C , et al. Heterogeneous Graph Neural Network[C]// the 25th ACM SIGKDD International Conference. ACM, 2019.</p>
</blockquote>
</blockquote>
<h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><h5 id="idea-of-the-article-HetGNN"><a href="#idea-of-the-article-HetGNN" class="headerlink" title="idea of the article(HetGNN)"></a>idea of the article(HetGNN)</h5><blockquote>
<p>This paper models the heterogeneous graph network and gets each nodes’ vector representation.<br>purpose: learn how to represent the vectors of each node in a heterogeneous graph.(embedding)</p>
</blockquote>
<p>First, <font color='red'>a reboot-based random walk strategy is used to select neighbors for each node according to the node type</font>.</p>
<p>Then, two modules are used to aggregate the characteristics of neighbor nodes:</p>
<ul>
<li>Generate feature vectors by modeling <font color='red'>the features of different types of nodes.</font></li>
<li><font color='red'>Aggregate different types of neighbor nodes</font>, and assign different weights to different types of nodes by fusing attention mechanism to obtain the final vector representation.</li>
</ul>
<p>Finally, establish loss function, use mini-batch gradient descent. </p>
<p><em>the learnted vector can be used to <b>link prediction, recommendation, nodes classification, cluster, etc..</b></em></p>
<hr>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/05/21/TFq79ogQsh1xCm3.jpg" alt="HGNN_challenge.jpeg"></p>
<hr>
<h3>HetGNN模型结构</h3>



<p>针对上文提到的异构网络面临的三个挑战，HetGNN分成了三个部分：</p>
<ul>
<li><p>邻居采样策略：Sampling Heterogeneous Neighbors（挑战一）</p>
</li>
<li><p>特征编码：Encoding Heterogeneous Contents（挑战二）</p>
</li>
<li><p>聚合邻居：Aggregating Heterogeneous Neighbors（挑战三）</p>
</li>
</ul>
<p>最后，根据目标函数进行优化，进行训练和预测</p>
<h5>Sampling Heterogeneous Neighbors(挑战一)</h5>

<p><em>(挑战一：对异构图如何采样？)</em></p>
<p>在异构图中，直接采样邻居面临的问题：</p>
<ol>
<li><p><font color='red'>不能捕捉到不同类型邻居的信息。</font>比如说，在下图的作者-论文-会议的图中，作者之间并不相连，作者会议也不相连，但他们之间的关系不可被忽视。</p>
<p><div align=center><img src= "img/loading.gif" data-src="https://i.loli.net/2020/05/21/kLxdo3AM9jJPlVc.png" alt="HetG_example.png" style="zoom: 50%;"/></p>
</li>
<li><p><font color='red'>邻居数量的影响</font>。有的作者写了很多篇论文，有的作者写的少。有的商品被很多人访问，有的商品无人问津，冷启动问题不能很好的表示。</p>
</li>
<li><font color='red'>节点的特征类型不同（如图像，文字等），不能直接聚合。</font>

</li>
</ol>
<p>针对上述问题，本文采用一种<font color='red'>random walk with restart(RWR)</font>方法进行采样，主要有两步：</p>
<ol>
<li><p>从节点v<span style="border-bottom:2px dashed red;">随机游走采样</span>，采样固定长度，每次以概率p访问邻居节点或返回初始节点，每种类型节点采样数固定，确保每类节点都会被采样到。</p>
</li>
<li><p>对不同类型的邻居分组，不同类型的邻居，根据采样频率返回前k个</p>
</li>
</ol>
<p>上述采样方法中：</p>
<ol>
<li><p>对于每种类型的节点都采样到了</p>
</li>
<li><p>每种类型节点数量相同，并且高频邻居被选择</p>
</li>
<li><p>同种类型的邻居放在了一起，邻居信息可以聚合</p>
</li>
</ol>
<h5>Encoding Heterogeneous Contents(挑战二)</h5>

<p><em>(挑战二：对不同类型的特征怎样进行编码？)</em></p>
<p>同一个节点，也往往有多种类型的特征，如图像，文字等，<span style="border-bottom:2px dashed red;">文章提出先对这一类特征进行预训练，如类别特征直接利用one-hot，文本特征利用par2vec，图像特征利用CNN，训练得到每类特征的向量表示后，利用Bi-LSTM进行编码后聚合</span>。模型架构如图所示：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/05/22/u9vmF5WgNJ7PUjy.jpg" alt="embedding.png" style="zoom:50%;" /></p>
<blockquote>
<p>类别型特征（categorical feature）主要是指年龄，职业，血型等在有限类别内取值的特征。它的原始输入通常是字符串形式，除了决策树族的算法能直接接受类别型特征作为输入，对于支持向量机，逻辑回归等模型来说，必须对其做一定的处理，转换成可靠的数值特征才能正确运行。一般的处理方式就是<a href="https://zhuanlan.zhihu.com/p/88921408" target="_blank" rel="noopener">one-hot encoding</a></p>
<p><a href="https://blog.csdn.net/hero_fantao/article/details/69659457?locationNum=10&amp;fps=1" target="_blank" rel="noopener">par2vec</a>: 是根据word2vec产生的，专门针对paragraph vector。</p>
</blockquote>
<p>在数学表达式上，节点v的向量表示$f{_1}(v)$为：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/05/22/TcMxmGuFK9ZkR2r.jpg" alt="f1_v_.jpeg" style="zoom:50%;" /></p>
<h5>Aggregating Heterogeneous Neighbors(挑战三)</h5>

<p><em>(挑战三：对不同类型的节点如何聚合？)</em></p>
<p>在上一部分，得到了每个节点的特征表示，在聚合上面临两个问题：对同样类型的不同节点怎样聚合？对不同类型的节点怎样聚合？分两步解决这两个问题：</p>
<p>Same type neighbors aggregation:</p>
<p>在采样中，我们对不同类型的节点进行采样，<font color='red'>通过上一步得到了每个节点的特征，这里，需要对同一类型的节点特征进行聚合，此处仍然采用Bi-LSTM方法</font>：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/05/22/PAKxtBSkiIfaoJj.jpg" alt="f2_v_.png" style="zoom:50%;" /></p>
<p><span style="border-bottom:2px dashed red;">输入为采样得到的相同类型邻居的特征表示，输出为这一类型邻居的向量表示</span>，有点类似与GraphSAGE的思想，<span style="border-bottom:2px dashed red;">利用Bi-LSTM对相同类型的邻居节点进行聚合</span>，模型结构如图所示：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/05/22/j768MorHRWbhtUd.jpg" alt="NN-2.png" style="zoom:50%;" /></p>
<p>Type Combination:</p>
<p>上述得到了每个类型节点的向量表示，这里，希望对这些类型的节点进行聚合，考虑到不同类型节点的邻居贡献不同，因此引入注意力机制l联合学习不同类型的邻居：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/05/22/TLhV1KUcCMB6pYP.jpg" alt="FC8680C6-BD1C-4ADE-9DBF-A0B3C255DF27.png" style="zoom:50%;" /></p>
<p>模型结构如图所示：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/05/22/wVPgWO1y8K3UJ2h.jpg" alt="NN-3.png" style="zoom:50%;" /></p>
<hr>
<h3>Objective function and Framework</h3>

<p>根据目标函数学习模型参数：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/05/22/IAXLDbB9mT82onk.jpg" alt="fn.png" style="zoom:50%;" /></p>
<p>Framework:</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/05/22/E3RnvjMmPidVucN.jpg" alt="Framework.png" style="zoom:50%;" /></p>
<p><strong>分成5步：</strong></p>
<ol>
<li><p>对邻居节点进行采样，按照节点类型进行分类</p>
</li>
<li><p>NN-1：对节点不同类型特征学习</p>
</li>
<li><p>NN-2：对相同类型节点各个特征的聚合</p>
</li>
<li><p>NN-3：对不同类型节点的聚合</p>
</li>
<li><p>根据Graph Context Loss损失函数进行优化</p>
</li>
</ol>
<p>最终得到每个节点的向量表示用于下游任务</p>
<hr>
<h3>Contribution:</h3>

<ul>
<li>Define heterogeneous graph: heterogeneous of graph structure and nodes information.</li>
<li>Propose HetGNN, which can capture the heterogeneous of stucture and content at the same time. It can be applied to both direct and inductive tasks.</li>
<li>Good performance in multi-data set experiments, link prediction, node classification, clustering and other tasks.</li>
</ul>
<hr>
<p>Reference :</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/w0KhXwE3tkZtjzQcRApRow" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/w0KhXwE3tkZtjzQcRApRow</a></li>
<li><a href="https://www.jianshu.com/p/fd8355e3d5d5" target="_blank" rel="noopener">https://www.jianshu.com/p/fd8355e3d5d5</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/88921408" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/88921408</a></li>
<li><a href="https://blog.csdn.net/hero_fantao/article/details/69659457?locationNum=10&amp;fps=1" target="_blank" rel="noopener">https://blog.csdn.net/hero_fantao/article/details/69659457?locationNum=10&amp;fps=1</a></li>
</ul>
<p>Paper:</p>
<ul>
<li><a href="https://doi.org/10.1145/3292500.3330961" target="_blank" rel="noopener">https://doi.org/10.1145/3292500.3330961</a></li>
</ul>
<p>Code:</p>
<ul>
<li><a href="https://github.com/chuxuzhang/KDD2019_HetGNN" target="_blank" rel="noopener">https://github.com/chuxuzhang/KDD2019_HetGNN</a></li>
</ul>
]]></content>
      <tags>
        <tag>heterogeneous GNN</tag>
      </tags>
  </entry>
  <entry>
    <title>Meta-Graph Based Recommendation Fusion over Heterogeneous Information Networks</title>
    <url>/2019/12/09/Meta-Graph-Based-Recommendation-Fusion-over-Heterogeneous-Information-Networks/</url>
    <content><![CDATA[<blockquote>
<blockquote>
<p>This article is paper about recommendation over heterogeneous information networks. Because HIN faces two problems: 1, how to represent the high-level semantics of recommendations and 2, how to fuse the heterogeneous information to make recommendations, the author first introduced the concept of <strong>meta-graph</strong> to HIN-based recommendation, and then solved the information fusion problem with a <strong>“matrix factorization(MF) + factorization machine(FM)”</strong> approach.</p>
<p>I think the author made two innovations:</p>
<p>1, use meta-graph and commute matrix to construct similarity matrix;</p>
<p>2, for each meta-graph, he calculate user-item’s similarity matrix, then factorize users and items’ feature vectors by using matrix factorization (MF). So he can obtain many different users and items vectors of many different meta-graph. for all of then, he use factorization machine(FM) to component then.</p>
<p>A user-item score prediction problem.</p>
</blockquote>
</blockquote>
<p><strong>KEYWORDS:  Recommendation system; Collaborative filtering; Heterogeneous information networks; Factorization machine.</strong></p>
<p><h2>Contents</h2></p>
<blockquote>
<ul>
<li><a href="#1">INTRODUCTION</a></li>
<li><a href="#2">FRAMEWORK</a><ul>
<li><a href="#2.1">Meta-graph based Similarity</a></li>
<li><a href="#2.2">Meta-graph based Latent Features</a></li>
<li><a href="2.3">Recommendation Model</a></li>
<li><a href="2.4">Comparison with Previous Latent Feature based Model</a></li>
</ul>
</li>
<li><a href="#3">MODEL OPTIMIZATION</a><ul>
<li><a href="#3.1">Optimization</a></li>
<li><a href="3.2">Complexity Analysis</a></li>
</ul>
</li>
<li><a href="4">EXPERIMENTS</a><ul>
<li><a href="4.1">Datasets</a></li>
<li><a href="#4.2">Evaluation Metric</a></li>
<li><a href="#4.3">Baseline Models</a></li>
</ul>
</li>
</ul>
</blockquote>
<p><h2 id='1'>INTRODUCTION</h2><br>​        Heterogeneous information networks(<strong>HINs</strong>) have been proposed as a general data representation for many different types of data. </p>
<p>​        At the beginning, HINs were used to handle entity search and similarity measure problems. Later, it was extended to handle heterogeneous entity recommendation problems. Then, the semantic relatedness constrained by the entity types can be defined by the similarities between two entities along meta-graph.</p>
<p>​        For traditional collaborative filtering, we always build a simple meta-path <em>Business —&gt; User</em> and learn from this meta-path to make generalization. From HIN’s schema, we can build more complicated meta-paths like <em>User—&gt;Review—&gt;Word—&gt;Review—&gt;Business</em>. </p>
<p>​        There are two major challenges when applying meta-path based similarities to recommender systems: <strong>First</strong>, meta-path may not be the best way to characterize the rich semantics. So, in this paper, they used <strong>meta-graph(or meta-structure)</strong> to compute similarity between homogeneous type of entities over HINs. <strong>Second</strong>, different meta-paths or meta-graphs result in different similarities. How to assemble them in an effective way is another challenge.</p>
<p>​        So, in this paper, in order to solve the above two problems, author first used <strong>the concept of meta-graph</strong> and then applied <strong>matrix factorization(MF) + factorization machine(FM)</strong>.</p>
<pre><code>    * for each meta-graph, compute the user-item similarity matrix
    * Use unsupervised MF to factorize it into a set of user and item latent vectors.
    * Use FM to assemble them to learn from the rating matrix. 
</code></pre><p><h2 id='2'>FRAMEWORK</h2></p>
<p><h3 id='2.1'>Meta-graph based Similarity</h3><br>​        The definition of Meta-graph:</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2019/12/09/VePI7wiHLXqt8fQ.jpg" alt="meta-graph.png"></p>
<p>​        All of the meta-graphs on Amazon and Yelp:</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2019/12/09/wjOSsENmKGR2yY9.jpg" alt="all of the meta-graphs for Amazon and Yelp"></p>
<p>​        The author used <strong>commuting matrices</strong> to compute the counting based similarity matrix of a meta-path. For example: if we have a meta-path:<img src= "img/loading.gif" data-src="https://i.loli.net/2019/12/09/Pkmpr4n9MUfvV5s.jpg" alt="meta-path.png"></p>
<p>And, we define a matrix $W_{A_iA_j}$ as the adjacency matrix between type $A_i$ and $A_j$. So the commuting matrix for path p is:<img src= "img/loading.gif" data-src="https://i.loli.net/2019/12/09/esYuT4iBoKzdtrN.jpg" alt="ABF68C6E-64CD-469A-BC2E-7D1B19ED3D31.png"></p>
<p><strong>Algorithm 1 Computing commuting matrix for CM9 :</strong></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2019/12/09/Yshg3R91MzpqZ48.jpg" alt="B07D1FB3-2139-4357-A000-0EA870DA1635.png"></p>
<p><h3 id='2.2'>Meta-graph based Latent Features</h3><br>​        By use of matrix factorization method, we can obtain user and item’s low-rank matrices:<img src= "img/loading.gif" data-src="https://i.loli.net/2019/12/09/z4JjWdDx5TnMNf3.jpg" alt="low-rank.png"></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2019/12/09/pmxiLPM6atBTrq8.jpg" alt="702CE1AE-D96F-4C88-BA2E-B80D483F536C.png"></p>
<p>Which ${\lambda_u},{\lambda_b}$ are the hyper-parameters that control the influence of Frobenius norm regularization to aviod overfitting.</p>
<p><h3 id='2.3'>Recommendation Model</h3><br>​        For the L groups of user and item latent features and F ranks which used to factorize every similarity matrix, we concatenate all of the corresponding user and item features form all of the L meta-graphs:<img src= "img/loading.gif" data-src="https://i.loli.net/2019/12/09/abOz2gnh9PDwklq.jpg" alt="EADE9E6C-D305-4B2C-BF96-033AC1A9BAF7.png"></p>
<p>​        So the rating for the sample $x^n$ based on FM is computed as follows:</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2019/12/09/5oRL7Dfn9Sl1vGt.jpg" alt="99A1983D-A650-43E0-95B0-C2D2E7826EA3.png"></p>
<p>Where $w_0$ is the global bias representing the first-order weights for the features, and $V = [v_i]$ represents the second-order weights to model the interactions across different features. $&lt;.,.&gt;$ is the dot product of two vectors of size K.</p>
<p>​        The parameters can be learned by minimizing the mean square loss:<img src= "img/loading.gif" data-src="https://i.loli.net/2019/12/11/k63WYeTtjAdoalO.jpg" alt="mean square loss.png"></p>
<p>Where $y^n$ is an observed rating for the n-th sample, N is the number of all the observed ratings.</p>
<p><strong>There are two problems when applying the FM model to the meta-graph based latent features:</strong></p>
<ul>
<li>It may bring noise when working with many meta-graphs thus impairing the predicting capability of the model. In practice, some meta-graphs can be useless since information provided by some meta-paths can be covered by others.</li>
<li>Computational cost.</li>
</ul>
<p>​        To alleviate the above two problems, we propose a novel regularization for FM,i.e., the group lasso regularization, which is a feature selection method on a group of variables. The group lasso regularization of parameters p is defined as follows:<img src= "img/loading.gif" data-src="https://i.loli.net/2019/12/09/THbdrAQpqiRCLlI.jpg" alt="6F205C97-3FB4-4FE5-B2D8-9A5A8E0B7ECA.png"></p>
<p><h3 id='2.4'>Comparison with Previous Latent Feature based Model</h3><br>​        The previous approaches of recommendation based on HIN is not adequate, as it fails to capture the interactions between inter-meta-graph features, i.e. features across different meta-graphs, as well as the intra-meta-graph features, i.e. It may decrease the prediction ability of all of the user and item features.</p>
<p><h2 id='3'>MODEL OPTIMIZATION</h2><br>​        The author define FM over meta-graph(FMG) model with the following objective function:</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2019/12/09/FYwu3NTrWsHG5hy.jpg" alt="1CB932AA-8575-4885-82D5-3190F9CA0128.png"></p>
<p>​        We can see now the object function is non-convex and non-smooth. So author used <strong>proximal gradient (nmAPG) algorithm</strong>.</p>
<p><h2 id='4'> EXPERIMENTS</h2></p>
<p><h3 id='4.1'>Datasets</h3></p>
<ul>
<li>Yelp (<a href="https://www.yelp.com/dataset_challenge" target="_blank" rel="noopener">https://www.yelp.com/dataset_challenge</a>)</li>
<li>Amazon Electronics (<a href="http://jmcauley.ucsd.edu/data/amazon/" target="_blank" rel="noopener">http://jmcauley.ucsd.edu/data/amazon/</a>)</li>
</ul>
<p><h3 id='4.2'>Evaluation Metric</h3><br>​        RMSE</p>
<p><h3 id='4.3'>Baseline Models</h3></p>
<ul>
<li>RegSVD: RegSVD is the basic matrix factorization with L2 regularization, which uses only the user-item rating matrix. (<a href="https://www.librec.net/" target="_blank" rel="noopener">https://www.librec.net/</a>)</li>
<li>FMR: FMR is the factorization machine with only the user-item rating matrix.(<a href="http://www.libfm.org/" target="_blank" rel="noopener">http://www.libfm.org/</a>)</li>
<li>HeteRec: HeteRec method is based on meta-path based similarity between users and items.</li>
<li>SemRec: SemRec is a meta-path based recommendation on weighted HIN, which is built by connecting users and items with the same ratings.(<a href="https://github.com/zzqsmall/SemRec" target="_blank" rel="noopener">https://github.com/zzqsmall/SemRec</a>)</li>
</ul>
]]></content>
      <categories>
        <category>recommendation</category>
        <category>HIN</category>
      </categories>
      <tags>
        <tag>Recommendation</tag>
        <tag>Meta-graph</tag>
        <tag>Heterogeneous Information Networks</tag>
      </tags>
  </entry>
  <entry>
    <title>The FacT: Taming Latent Factor Models for Explainability with Factorization Trees</title>
    <url>/2020/05/23/The-FacT-Taming-Latent-Factor-Models-for-Explainability-with-Factorization-Trees/</url>
    <content><![CDATA[<blockquote>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">@inproceedings&#123;tao2019the,</span><br><span class="line">	title=&quot;The FacT: Taming Latent Factor Models for Explainability with Factorization Trees&quot;,</span><br><span class="line">	author=&quot;Yiyi &#123;Tao&#125; and Yiling &#123;Jia&#125; and Nan &#123;Wang&#125; and Aobo &#123;Yang&#125; and Hongning &#123;Wang&#125;&quot;,</span><br><span class="line">	booktitle=&quot;SIGIR 2019 : 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval&quot;,</span><br><span class="line">	year=&quot;2019&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p> <a href="https://doi.org/10.1145/3331184.3331244" target="_blank" rel="noopener">https://doi.org/10.1145/3331184.3331244</a></p>
</blockquote>
<h2>Abstract</h2>

<p>​        The FacT model aims at explaining latent factor based recommendation algorithms with rule-based explanations. It integrates <font color='red'>regression trees</font> to guide the learning of latent factor models for recommendation, and uses the learned tree structure to explain the resulting latent factors. With user-generated reviews, regression trees on users and items are built respectively, and each node on the trees are asscoiated with a latent profile to represent users and items. </p>
<h2>Methodology</h2>

<p><b>Latent factor learning</b></p>
<p>​        latent factor models have been widely deployed in modern recommender systems becuase its excellent recommendation quality. This work choices matrix factorization due to its simplicity.</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/05/24/fK8aYMkwTldIHUi.jpg" alt="1.jpeg" style="zoom:50%;" /></p>
<p>​        (1) for point wise rating prediction loss; (2) for pairwise ranking loss.</p>
<p><b>Explanation rule induction</b></p>
<p>​        It selects the predicates among the item features extracted from user-generated reviews, and each lexicon entry takes the form of <span style="border-bottom:2px dashed red;">(feature, opinion, sentiment polarity)</span>, abbreviated as (f, o ,s), and represents the <span style="border-bottom:2px dashed red;">sentiment polarity s inferred from an opinionated text phrase o describing feature f.</span></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/05/24/iXmpjxkQTIFnDzu.jpg" alt="2.jpeg" style="zoom:50%;" /></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/05/24/Z8JDPxUqAhgm9ej.jpg" alt="3.jpeg" style="zoom:50%;" /></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/05/24/pCk8Q4oLXSz1gZ9.jpg" alt="4.jpeg" style="zoom: 50%;" /></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/05/24/fACq4EtT3X82yHP.jpg" alt="IMG_1A6D7EF91195-1.jpeg"></p>
<h2>Evaluation</h2>

<ul>
<li>Normalized Discounted Cumulative Gain(NDCG)</li>
</ul>
<h2>Data</h2>

<ul>
<li>Amazon <a href="http://jmcauley.ucsd.edu/data/amazon/" target="_blank" rel="noopener">http://jmcauley.ucsd.edu/data/amazon/</a></li>
<li>Yelp dataset challenge <a href="https://www.yelp.com/dataset" target="_blank" rel="noopener">https://www.yelp.com/dataset</a></li>
</ul>
<h2>Code</h2>

<ul>
<li><a href="https://github.com/yilingjia/TheFacT" target="_blank" rel="noopener">https://github.com/yilingjia/TheFacT</a></li>
</ul>
<h2>Reference</h2>

<ul>
<li><a href="https://blog.csdn.net/qq_38871942/article/details/104696210" target="_blank" rel="noopener">https://blog.csdn.net/qq_38871942/article/details/104696210</a></li>
<li><a href="https://www.cnblogs.com/pinard/p/9128682.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/9128682.html</a></li>
</ul>
]]></content>
      <categories>
        <category>recommendation</category>
        <category>explainable recommendation</category>
      </categories>
      <tags>
        <tag>explainability</tag>
        <tag>regression tree</tag>
        <tag>recommendation</tag>
      </tags>
  </entry>
  <entry>
    <title>Personalized Recommendation Combining User Interest and Social Circle</title>
    <url>/2019/11/13/Personalized-Recommendation-Combining-User-Interest-and-Social-Circle/</url>
    <content><![CDATA[<blockquote>
<blockquote>
<p>Interpersonal influence and interest based on circles of friends can be used to solve cold start and sparsity problem of datasets.This paper used three social factors:<strong>personal interest</strong>,<strong>interpersonal interest similarity</strong> and <strong>interpersonal influence</strong> to make personalized recommendation.</p>
</blockquote>
</blockquote>
<h1 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h1><blockquote>
<ul>
<li><a href="#1">Introduction</a></li>
<li><a href="#2">Problem formulation</a></li>
<li><a href="#3">Related work</a><ul>
<li><a href="3.1">Basic matrix factorization</a></li>
<li><a href="3.2">CircleCon model</a></li>
<li><a href="3.3">ContextMF model</a></li>
</ul>
</li>
<li><a href="4">The approach</a><ul>
<li><a href="4.1">User interest factor</a></li>
<li><a href="4.2">Personalized recommendation model</a></li>
<li><a href="4.3">Model training</a></li>
</ul>
</li>
<li><a href="5">Experiments</a><ul>
<li><a href="5.1">Datasets</a></li>
<li><a href="5.2">Performance measures</a></li>
<li><a href="5.3">Evaluation</a></li>
</ul>
</li>
</ul>
</blockquote>
<h2 id='1'>Introduction</h2>
Recommender system(RS) has been successfully exploited to solve information overload.Many methods proposed to improve the performance of RS.And there are two main problems needed to solve:**1.the problems of cold start for users(new users into the RS with little historical behavior)2.the sparsity of datasets(the proportion of rated user-item pairs in all the user-item pairs of RS)**.

In this paper, three social factors,**personal interest,interpersonal interest similarity,and interpersonal influence**, fuse into a unified personalized recommendation model based on probabilistic matrix factorization.

<h2 id='2'>Problem formulation</h2>
]]></content>
      <tags>
        <tag>RS</tag>
      </tags>
  </entry>
  <entry>
    <title>Multimodal Machine Learning:A Survey and Taxonomy</title>
    <url>/2019/11/23/Multimodal-Machine-Learning-A-Survey-and-Taxonomy/</url>
    <content><![CDATA[<blockquote>
<blockquote>
<p>This article is a survey for multimodal machine learning. This paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. They go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: <strong>representation, translation, alignment, fusion, and co-learning.</strong></p>
</blockquote>
<p>Index Terms:<strong>Multimodal, machine learning, introductory, survey</strong></p>
</blockquote>
]]></content>
      <tags>
        <tag>Muilimodal</tag>
        <tag>machine learning</tag>
        <tag>survey</tag>
      </tags>
  </entry>
  <entry>
    <title>Semi-supervised Classification with Graph Convolutional Networks</title>
    <url>/2019/11/14/Semi-supervised-Classification-with-Graph-Convolutional-Networks/</url>
    <content><![CDATA[<blockquote>
<blockquote>
<p>The convolution is extended to the data of the graph structure.Motivate the choice of our convolutional architecture via a localized first-order approximation of <strong>spectral graph convolutions</strong>.The model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes.</p>
<p>In a number of experiments on <strong>citation networks</strong> and on a <strong>knowledge graph dataset</strong>,This model works well in semi-supervised tasks.</p>
</blockquote>
</blockquote>
<h1 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h1><blockquote>
<ul>
<li><a href="#1">The main content of the article</a><ul>
<li><a href="#1.1">abstract</a></li>
<li><a href="#1.2">spectral graph convolutions</a></li>
<li><a href="#1.3">layer-wise linear model</a></li>
<li><a href="#1.4">semi-supervised node classifacation</a></li>
</ul>
</li>
<li><a href="#2">My learing understanding of semi-GCN</a></li>
</ul>
</blockquote>
<h2 id='1'>The main content of the article</h2>

<h3 id='1.1'>abstract</h3>

<p>The authors motivate the choice of their convolutional architecture via a localized first-order approximation of <strong>spectral graph convolutions</strong>.</p>
<p>By training the structural model of convolution neural network with some tagged node data in the graph structural data ,the network model can further classify the remaining untagged data.</p>
<p>So,on the whole , the way which the convolution is extended to the data of the graph structural ,can get a better data representation ,and it works well in semi-supervised tasks.</p>
<blockquote>
<p>Datasets: citation network datasets(Citeseer ,Cora ,Pubmed) ,bipartite graph dataset(NELL) </p>
</blockquote>
<h3 id='1.2'>spectral graph convolutions</h3>

<p><em>In this period ,I mainly refer to this website:<a href="https://blog.csdn.net/qq_41727666/article/details/84622965" target="_blank" rel="noopener">https://blog.csdn.net/qq_41727666/article/details/84622965</a></em></p>
<p>In fact ,there are two versions of GCN:</p>
<p>1.The first generation of GCN is this formula:<strong><script type="math/tex">g{\theta} * x = Ug{\theta}U^Tx</script></strong>.</p>
<p>In the formula ,$x$ is the eigenvector of the graph node ,$g{\theta}=diag(\theta)$ is the convolution kernel.$U$ is the matrix of eigenvectors of the normalized graph Laplacian $L$. And the $L = I_N - D^{-1/2}AD^{-1/2} = U{\Lambda}U^T$</p>
<p><strong>But ,this is too complicated to calculate</strong></p>
<p>2.To circumvent this problem ,$g{\theta}(\Lambda)$ can be well-approximated by a truncated expansion in terms of Chebyshev polynomial $T_k(x)$ up to $K^{th}$ order:</p>
<script type="math/tex; mode=display">g{\theta}^` {\approx} {\sum_{k=0}^{K}{\theta}^`T_k(\Lambda^1)}</script><p>With a rescaled $\Lambda^1 = {\frac{2}{\lambda_{max}}}\Lambda - I_N$. $\lambda_{max}$ Denotes the largest eigenvalue of $L$. $\theta^`$ is now a vector of Chebyshev coefficients. The Chebyshev polynomials are recursively defined as $T_k(x) = 2xT_{k-1}(x) - T_{k-2}(x)$, with $T_0(x) = 1$ and $T_1(x) = x$.</p>
<p>So, now the formula is:</p>
<script type="math/tex; mode=display">g{\theta}^` * x \approx {\sum_{k=0}^{K}{\theta}_k^`T_k(L^1)x}</script><p>With $L^1 = {\frac{2}{\lambda_{max}}L - I_N}$ .This expression is now K-localized since it is a <strong>$K^{th}$-order ploynomial in the Laplacian</strong>.</p>
<h3 id='1.3'>layer-wise linear model</h3>

<p>The authors approximate $\lambda_{max} \approx 2$, so now the formula is simplified to:</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2019/11/17/LdqIwO9NfxzXWJv.png" alt="semi_eq5.png"></p>
<h3 id='1.4'>semi-supervised node classification</h3>

<p>In this example, they consider a two-layer GCN for semi-supervised node classification on a graph with a symmetric adjacency matrix A(binary or weighted). </p>
<p>First, calculate $A^` = D1^{-1/2}A^1D1^{-1/2}$.</p>
<p>And the forward model then takes the simple form:<script type="math/tex">Z = f(X,A) = softmax(A^`{ReLU}(A^`XW^{(0)})W^{(1)})</script></p>
<p>Here, $W^{(0)}$ is an input-to-hidden weight matrix for a hidden layer with H feature maps.</p>
<p>$W^{(1)}$ is a hidden-to-output weight matrix.</p>
<p>The softmax activation function defined as $softmax(x_i) = \frac{1}{\sum_i{exp(x_i)}}exp(x_i)$</p>
<p>For semi-supervised multiclass classification, they then evaluate the cross-entropy error over all labeled examples :$\xi = -\sum_{l\epsilon{y_l}}\sum_{f=1}^{F}Y_{lf}lnZ_{lf}$</p>
<p>The author perform <strong>batch gradient descent</strong>. Stochasticity in the training process is in produced via <strong>dropout</strong>. </p>
<h2 id='2'>My learing understanding of semi-GCN</h2>

<p><em>mainly depend on <a href="https://zhuanlan.zhihu.com/p/58178060" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/58178060</a></em></p>
]]></content>
      <tags>
        <tag>GCN</tag>
        <tag>Deeplearning</tag>
      </tags>
  </entry>
  <entry>
    <title>leetcode record</title>
    <url>/2020/06/01/leetcode-record/</url>
    <content><![CDATA[<h4 id="974-和可被K整除的子数组-中等"><a href="#974-和可被K整除的子数组-中等" class="headerlink" title="974.和可被K整除的子数组(中等)"></a>974.和可被K整除的子数组(中等)</h4><hr>
<p>给定一个整数数组 <code>A</code>，返回其中元素之和可被 <code>K</code> 整除的（连续、非空）子数组的数目。</p>
<p>示例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入：A = [4,5,0,-2,-3,1], K = 5</span><br><span class="line">输出：7</span><br><span class="line">解释：</span><br><span class="line">有 7 个子数组满足其元素之和可被 K = 5 整除：</span><br><span class="line">[4, 5, 0, -2, -3, 1], [5], [5, 0], [5, 0, -2, -3], [0], [0, -2, -3], [-2, -3]</span><br></pre></td></tr></table></figure>
<p>思路：</p>
<p> 通常，涉及连续子数组问题的时候，使用前缀和来解决。</p>
<p> 我们令 P[i]=A[0]+A[1]+…+A[i] 。那么每个连续子数组的和sum(i,j) 就可以写成P[j]−P<a href="0&lt;i&lt;j">i</a> 的形式。此时，判断子数组的和能否被K整除就等价于判断(P[j]−P[i−1])modK==0，根据同余定理，只要P[j]modK==P[i−1]modK就行。</p>
<p> 因此我们可以考虑对数组进行遍历，在遍历同时统计答案。当我们遍历到第 i个元素时，我们统计以 i 结尾的符合条件的子数组个数。我们可以维护一个以前缀和模 K 的值为键，出现次数为值的哈希表 record，在遍历的同时进行更新。这样在计算以 i结尾的符合条件的子数组个数时，根据上面的分析，答案即为[0..i−1] 中前缀和模K也为P[i]modK的位置个数，即record[P[i]modK]。</p>
<p> 最后的答案即为以每一个位置为数尾的符合条件的子数组个数之和。需要注意的一个边界条件是，我们需要对哈希表初始化，记录record[0]=1，这样就考虑了前缀和本身被 K 整除的情况。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def subarraysDivByK(self, A: List[int], K: int) -&gt; int:</span><br><span class="line">        record = &#123;0: 1&#125;</span><br><span class="line">        total, ans = 0, 0</span><br><span class="line">        for elem in A:</span><br><span class="line">            total += elem</span><br><span class="line">            modulus = total % K</span><br><span class="line">            same = record.get(modulus, 0)</span><br><span class="line">            ans += same</span><br><span class="line">            record[modulus] = same + 1</span><br><span class="line">        return ans</span><br></pre></td></tr></table></figure>
<p>复杂度分析</p>
<ul>
<li>时间复杂度：O(N)，其中 N 是数组 A 的长度。我们只需要从前往后遍历一次数组，在遍历数组的过程中，维护哈希表的各个操作均为 O(1)，因此总时间复杂度为 O(N)。</li>
<li>空间复杂度：O(min(N,K))，即哈希表需要的空间。当 N≤K 时，最多有N 个前缀和，因此哈希表中最多有N+1个键值对；当 N&gt;K 时，最多有K 个不同的余数，因此哈希表中最多有 K 个键值对。也就是说，哈希表需要的空间取决于N 和 K中的较小值。</li>
</ul>
<h4 id="394-字符串解码-中等"><a href="#394-字符串解码-中等" class="headerlink" title="394.字符串解码(中等)"></a>394.字符串解码(中等)</h4><hr>
<p>给定一个经过编码的字符串，返回它解码后的字符串。</p>
<p>编码规则为:k[encodedstring]，表示其中方括号内部的 encodedstring 正好重复 k 次。注意 k 保证为正整数。你可以认为输入字符串总是有效的；输入字符串中没有额外的空格，且输入的方括号总是符合格式要求的。此外，你可以认为原始数据不包含数字，所有的数字只表示重复的次数 k ，例如不会出现像 3a 或 2[4]的输入。</p>
<p>示例:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">s = &quot;3[a]2[bc]&quot;, 返回 &quot;aaabcbc&quot;.</span><br><span class="line">s = &quot;3[a2[c]]&quot;, 返回 &quot;accaccacc&quot;.</span><br><span class="line">s = &quot;2[abc]3[cd]ef&quot;, 返回 &quot;abcabccdcdcdef&quot;.</span><br></pre></td></tr></table></figure>
<p>用栈的思想：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def decodeString(self, s: str) -&gt; str:</span><br><span class="line">        stack = [[&apos;&apos;, 1]]</span><br><span class="line">        num = &apos;&apos;</span><br><span class="line">        for c in s:</span><br><span class="line">            if c.isdigit():</span><br><span class="line">                num += c</span><br><span class="line">            elif c == &apos;[&apos;:</span><br><span class="line">                stack.append([&apos;&apos;, int(num)])</span><br><span class="line">                num = &apos;&apos;</span><br><span class="line">            elif c == &apos;]&apos;:</span><br><span class="line">                subs, k = stack.pop()</span><br><span class="line">                stack[-1][0] += subs * k</span><br><span class="line">            else:</span><br><span class="line">                stack[-1][0] += c</span><br><span class="line">        return stack[0][0] * stack[0][1]</span><br></pre></td></tr></table></figure>
<h4 id="198-打家劫舍-中等"><a href="#198-打家劫舍-中等" class="headerlink" title="198.打家劫舍(中等)"></a>198.打家劫舍(中等)</h4><hr>
<p>你是一个专业的小偷，计划偷窃沿街的房屋。每间房内都藏有一定的现金，影响你偷窃的唯一制约因素就是相邻的房屋装有相互连通的防盗系统，如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警。给定一个代表每个房屋存放金额的非负整数数组，计算你 不触动警报装置的情况下 ，一夜之内能够偷窃到的最高金额。</p>
<p>示例 1:</p>
<p>输入: [1,2,3,1] 输出: 4 解释: 偷窃 1 号房屋 (金额 = 1) ，然后偷窃 3 号房屋 (金额 = 3)。 偷窃到的最高金额 = 1 + 3 = 4 。 示例 2:</p>
<p>输入: [2,7,9,3,1] 输出: 12 解释: 偷窃 1 号房屋 (金额 = 2), 偷窃 3 号房屋 (金额 = 9)，接着偷窃 5 号房屋 (金额 = 1)。 偷窃到的最高金额 = 2 + 9 + 1 = 12 。</p>
<p><strong>思路：</strong></p>
<p>动态规划+滚动数组：</p>
<p>首先考虑最简单的情况：如果只有一个房间，则偷窃该房屋，可以偷窃到最高总金额。如果只有两个房间，则由于两间房屋相邻，不能同时偷窃，只能偷窃其中的一间屋子，因此选择其中金额较高的房屋进行偷窃，可以偷窃到最高总金额。</p>
<p>如果房屋数量大于两间，应该如何计算能够偷窃到的最高总金额呢？对于第 k(k&gt;2) 间房屋，有两个选项：</p>
<p>偷窃第 k 间房屋，那么就不能偷窃第 k−1 间房屋，偷窃总金额为前 k−2 间房屋的最高总金额与第 k 间房屋的金额之和。</p>
<p>不偷窃第k 间房屋，偷窃总金额为前 k−1间房屋的最高总金额。</p>
<p>在两个选项中选择偷窃总金额较大的选项，该选项对应的偷窃总金额即为前 k 间房屋能偷窃到的最高总金额。</p>
<p>用 dp[i]表示前 i间房屋能偷窃到的最高总金额，那么就有如下的状态转移方程：</p>
<p>dp[i]=max(dp[i−2]+nums[i],dp[i−1])</p>
<p>边界条件为：</p>
<p>{dp[0]=nums[0]只有一间房屋，则偷窃该房屋 dp[1]=max(nums[0],nums[1])只有两间房屋，选择其中金额较高的房屋进行偷窃</p>
<p>只有一间房屋，则偷窃该房屋 只有两间房屋，选择其中金额较高的房屋进行偷窃 最终的答案即为 dp[n−1]，其中 n 是数组的长度。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def rob(self, nums: List[int]) -&gt; int:</span><br><span class="line">        if not nums:</span><br><span class="line">            return 0</span><br><span class="line"></span><br><span class="line">        size = len(nums)</span><br><span class="line">        if size == 1:</span><br><span class="line">            return nums[0]</span><br><span class="line">        </span><br><span class="line">        dp = [0] * size</span><br><span class="line">        dp[0] = nums[0]</span><br><span class="line">        dp[1] = max(nums[0], nums[1])</span><br><span class="line">        for i in range(2, size):</span><br><span class="line">            dp[i] = max(dp[i - 2] + nums[i], dp[i - 1])</span><br><span class="line">        </span><br><span class="line">        return dp[size - 1]</span><br></pre></td></tr></table></figure>
<p>上述方法使用了数组存储结果。考虑到每间房屋的最高总金额只和该房屋的前两间房屋的最高总金额相关，因此可以使用滚动数组，在每个时刻只需要存储前两间房屋的最高总金额。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def rob(self, nums: List[int]) -&gt; int:</span><br><span class="line">        if not nums:</span><br><span class="line">            return 0</span><br><span class="line"></span><br><span class="line">        size = len(nums)</span><br><span class="line">        if size == 1:</span><br><span class="line">            return nums[0]</span><br><span class="line">        </span><br><span class="line">        first, second = nums[0], max(nums[0], nums[1])</span><br><span class="line">        for i in range(2, size):</span><br><span class="line">            first, second = second, max(first + nums[i], second)</span><br><span class="line">        </span><br><span class="line">        return second</span><br></pre></td></tr></table></figure>
<p>复杂度分析</p>
<p>时间复杂度：O(n)，其中 n 是数组长度。只需要对数组遍历一次。</p>
<p>空间复杂度：O(1)。使用滚动数组，可以只存储前两间房屋的最高总金额，而不需要存储整个数组的结果，因此空间复杂度是 O(1)。</p>
<h4>1431.拥有最多糖果的孩子(简单)</h4>

<hr>
<p>给你一个数组 candies 和一个整数 extraCandies ，其中 candies[i] 代表第 i 个孩子拥有的糖果数目。</p>
<p>对每一个孩子，检查是否存在一种方案，将额外的 extraCandies 个糖果分配给孩子们之后，此孩子有 最多 的糖果。注意，允许有多个孩子同时拥有 最多 的糖果数目。</p>
<p><strong>示例 1：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入：candies = [2,3,5,1,3], extraCandies = 3</span><br><span class="line">输出：[true,true,true,false,true] </span><br><span class="line">解释：</span><br><span class="line">孩子 1 有 2 个糖果，如果他得到所有额外的糖果（3个），那么他总共有 5 个糖果，他将成为拥有最多糖果的孩子。</span><br><span class="line">孩子 2 有 3 个糖果，如果他得到至少 2 个额外糖果，那么他将成为拥有最多糖果的孩子。</span><br><span class="line">孩子 3 有 5 个糖果，他已经是拥有最多糖果的孩子。</span><br><span class="line">孩子 4 有 1 个糖果，即使他得到所有额外的糖果，他也只有 4 个糖果，无法成为拥有糖果最多的孩子。</span><br><span class="line">孩子 5 有 3 个糖果，如果他得到至少 2 个额外糖果，那么他将成为拥有最多糖果的孩子。</span><br></pre></td></tr></table></figure>
<p><strong>示例 2：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入：candies = [4,2,1,1,2], extraCandies = 1</span><br><span class="line">输出：[true,false,false,false,false] </span><br><span class="line">解释：只有 1 个额外糖果，所以不管额外糖果给谁，只有孩子 1 可以成为拥有糖果最多的孩子。</span><br></pre></td></tr></table></figure>
<p><strong>示例 3：</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入：candies = [12,1,12], extraCandies = 10</span><br><span class="line">输出：[true,false,true]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">kidsWithCandies</span><span class="params">(self, candies: List[int], extraCandies: int)</span> -&gt; List[bool]:</span></span><br><span class="line">        m = max(candies)</span><br><span class="line">        a = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> candies:</span><br><span class="line">            <span class="keyword">if</span> (i + extraCandies) &gt;= m:</span><br><span class="line">                a.append(<span class="literal">True</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                a.append(<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> a</span><br></pre></td></tr></table></figure>
<h4>101.对称二叉树(简单)</h4>

<hr>
<p>给定一个二叉树，检查它是否是镜像对称的。</p>
<p>例如，二叉树 [1,2,2,3,4,4,3] 是对称的。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">    1</span><br><span class="line">   / \</span><br><span class="line">  2   2</span><br><span class="line"> / \ / \</span><br><span class="line">3  4 4  3</span><br></pre></td></tr></table></figure>
<p>但是下面这个 <code>[1,2,2,null,3,null,3]</code> 则不是镜像对称的:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">  1</span><br><span class="line"> / \</span><br><span class="line">2   2</span><br><span class="line"> \   \</span><br><span class="line"> 3    3</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isSymmetric</span><span class="params">(self, root: TreeNode)</span> -&gt; bool:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root: <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">return</span> self.isSTree(root.left,root.right)</span><br><span class="line">    <span class="comment">#     global r</span></span><br><span class="line">    <span class="comment">#     r =  []</span></span><br><span class="line">    <span class="comment">#     f = 0</span></span><br><span class="line">    <span class="comment">#     self.mid(root, f)</span></span><br><span class="line">    <span class="comment">#     print(r)</span></span><br><span class="line">    <span class="comment">#     if r == r[::-1]:</span></span><br><span class="line">    <span class="comment">#         return True</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># def mid(self, root, f):</span></span><br><span class="line">    <span class="comment">#     f += 1</span></span><br><span class="line">    <span class="comment">#     if root == None:</span></span><br><span class="line">    <span class="comment">#         r.append(f)</span></span><br><span class="line">    <span class="comment">#     else:</span></span><br><span class="line">    <span class="comment">#         self.mid(root.left, f)</span></span><br><span class="line">    <span class="comment">#         r.append(root.val)</span></span><br><span class="line">    <span class="comment">#         self.mid(root.right, f)</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isSTree</span><span class="params">(self,left,right)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> left <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">and</span> right <span class="keyword">is</span> <span class="literal">None</span>: <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">if</span> left <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> right <span class="keyword">is</span> <span class="literal">None</span>: <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">if</span> left.val != right.val: <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> self.isSTree(left.left,right.right) <span class="keyword">and</span> self.isSTree(left.right, right.left)</span><br></pre></td></tr></table></figure>
<p>我采用了中序遍历和递归两种方式，注释掉的部分为中序遍历。递归判断时想清楚递归的终止情况与返回值：（1）左右子树都为空 → True （2）左右子树一个为空 → False （3）左右子树都不空，但是值不相等 → False （4）若上述情况都不满足， 检查 左左&amp;右右， 左右&amp;右左。</p>
<h4>84.柱状图中最大的矩形(困难)</h4>

<hr>
<p>给定 <em>n</em> 个非负整数，用来表示柱状图中各个柱子的高度。每个柱子彼此相邻，且宽度为 1 。</p>
<p>求在该柱状图中，能够勾勒出来的矩形的最大面积。</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/06/01/gPviFIYRh2xmOjf.jpg" alt="1.png" style="zoom:50%;" /></p>
<p>以上是柱状图的示例，其中每个柱子的宽度为 1，给定的高度为 <code>[2,1,5,6,2,3]</code>。</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/06/01/whXgI2TFcaU7rNm.jpg" alt="2.png" style="zoom:50%;" /></p>
<p>图中阴影部分为所能勾勒出的最大矩形面积，其面积为 <code>10</code> 个单位。</p>
<p><strong>示例:</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入: [2,1,5,6,2,3]</span><br><span class="line">输出: 10</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">largestRectangleArea</span><span class="params">(self, heights: List[int])</span> -&gt; int:</span></span><br><span class="line">        n = len(heights)</span><br><span class="line">        left, right = [<span class="number">0</span>] * n, [<span class="number">0</span>] * n</span><br><span class="line"></span><br><span class="line">        mono_stack = list()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="keyword">while</span> mono_stack <span class="keyword">and</span> heights[mono_stack[<span class="number">-1</span>]] &gt;= heights[i]:</span><br><span class="line">                mono_stack.pop()</span><br><span class="line">            left[i] = mono_stack[<span class="number">-1</span>] <span class="keyword">if</span> mono_stack <span class="keyword">else</span> <span class="number">-1</span></span><br><span class="line">            mono_stack.append(i)</span><br><span class="line">        </span><br><span class="line">        mono_stack = list()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n - <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">            <span class="keyword">while</span> mono_stack <span class="keyword">and</span> heights[mono_stack[<span class="number">-1</span>]] &gt;= heights[i]:</span><br><span class="line">                mono_stack.pop()</span><br><span class="line">            right[i] = mono_stack[<span class="number">-1</span>] <span class="keyword">if</span> mono_stack <span class="keyword">else</span> n</span><br><span class="line">            mono_stack.append(i)</span><br><span class="line">        </span><br><span class="line">        ans = max((right[i] - left[i] - <span class="number">1</span>) * heights[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(n)) <span class="keyword">if</span> n &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure>
<p>采用的是单调栈。</p>
<h4>14.最长公共前缀(简单)</h4>

<hr>
<p>编写一个函数来查找字符串数组中的最长公共前缀。</p>
<p>如果不存在公共前缀，返回空字符串 <code>&quot;&quot;</code>。</p>
<p><strong>示例 1:</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入: [&quot;flower&quot;,&quot;flow&quot;,&quot;flight&quot;]</span><br><span class="line">输出: &quot;fl&quot;</span><br></pre></td></tr></table></figure>
<p><strong>示例 2:</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">输入: [&quot;dog&quot;,&quot;racecar&quot;,&quot;car&quot;]</span><br><span class="line">输出: &quot;&quot;</span><br><span class="line">解释: 输入不存在公共前缀。</span><br></pre></td></tr></table></figure>
<p>我写得很简陋，就是暴力，有点像官方给的横向扫描，但是不够简洁：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">longestCommonPrefix</span><span class="params">(self, strs: List[str])</span> -&gt; str:</span></span><br><span class="line">        length = len(strs)</span><br><span class="line">        <span class="keyword">if</span> length == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">''</span></span><br><span class="line">        <span class="keyword">if</span> length == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> strs[<span class="number">0</span>]</span><br><span class="line">        L = len(strs[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">if</span> L == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">''</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            R = <span class="string">''</span></span><br><span class="line">            R += self.cons(strs[<span class="number">0</span>], strs[<span class="number">1</span>])</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, length - <span class="number">1</span>):</span><br><span class="line">                R = self.cons(R, strs[i + <span class="number">1</span>])</span><br><span class="line">            <span class="keyword">return</span> R</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cons</span><span class="params">(self, a, b)</span>:</span></span><br><span class="line">        r = <span class="string">''</span></span><br><span class="line">        <span class="keyword">if</span> len(a) == <span class="number">0</span> <span class="keyword">or</span> len(b) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> r</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(a)):</span><br><span class="line">            <span class="keyword">if</span> i &gt; len(b) - <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">if</span> a[i] == b[i]:</span><br><span class="line">                r += a[i]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">return</span> r</span><br></pre></td></tr></table></figure>
<p>官方解析给了四种思路：</p>
<h4 id="方法一：横向扫描"><a href="#方法一：横向扫描" class="headerlink" title="方法一：横向扫描"></a>方法一：横向扫描</h4><p>用$LCP(S_1…S_n)$表示字符串$S_1…S_n$的最长公共前缀。可以得出以下结论：</p>
<p>​         <script type="math/tex">LCP(S_1...S_n) = LCP(LCP(LCP(S_1,S_2),S_3),...S_n)</script></p>
<p>基于该结论，可以得到一种查找字符串数组中的最长公共前缀的简单方法。依次遍历字符串数组中的每个字符串，对于每个遍历到的字符串，更新最长公共前缀，当遍历完所有的字符串以后，即可得到字符串数组中的最长公共前缀。如果在尚未遍历完所有的字符串时，最长公共前缀已经是空串，则最长公共前缀一定是空串，因此不需要继续遍历剩下的字符串，直接返回空串即可。</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/06/15/K5HnPa1dQFhOyBG.png" alt="1.png" style="zoom:50%;" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">longestCommonPrefix</span><span class="params">(self, strs: List[str])</span> -&gt; str:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> strs:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">""</span></span><br><span class="line">        </span><br><span class="line">        prefix, count = strs[<span class="number">0</span>], len(strs)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, count):</span><br><span class="line">            prefix = self.lcp(prefix, strs[i])</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> prefix:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> prefix</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">lcp</span><span class="params">(self, str1, str2)</span>:</span></span><br><span class="line">        length, index = min(len(str1), len(str2)), <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> index &lt; length <span class="keyword">and</span> str1[index] == str2[index]:</span><br><span class="line">            index += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> str1[:index]</span><br></pre></td></tr></table></figure>
<p>复杂度分析</p>
<ul>
<li><p>时间复杂度：O(mn)O(mn)，其中 mm 是字符串数组中的字符串的平均长度，nn 是字符串的数量。最坏情况下，字符串数组中的每个字符串的每个字符都会被比较一次。</p>
</li>
<li><p>空间复杂度：O(1)O(1)。使用的额外空间复杂度为常数。</p>
</li>
</ul>
<h4 id="方法二：纵向扫描"><a href="#方法二：纵向扫描" class="headerlink" title="方法二：纵向扫描"></a>方法二：纵向扫描</h4><p>另一种方法是纵向扫描。纵向扫描时，从前往后遍历所有字符串的每一列，比较相同列上的字符是否相同，如果相同则继续对下一列进行比较，如果不相同则当前列不再属于公共前缀，当前列之前的部分为最长公共前缀。</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/06/15/SvCD5j4XQV7HZ9u.png" alt="2.png" style="zoom:50%;" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">longestCommonPrefix</span><span class="params">(self, strs: List[str])</span> -&gt; str:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> strs:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">""</span></span><br><span class="line">        </span><br><span class="line">        length, count = len(strs[<span class="number">0</span>]), len(strs)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(length):</span><br><span class="line">            c = strs[<span class="number">0</span>][i]</span><br><span class="line">            <span class="keyword">if</span> any(i == len(strs[j]) <span class="keyword">or</span> strs[j][i] != c <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, count)):</span><br><span class="line">                <span class="keyword">return</span> strs[<span class="number">0</span>][:i]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> strs[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>复杂度分析</p>
<ul>
<li><p>时间复杂度：$O(mn)$，其中 $m$ 是字符串数组中的字符串的平均长度，$n$ 是字符串的数量。最坏情况下，字符串数组中的每个字符串的每个字符都会被比较一次。</p>
</li>
<li><p>空间复杂度：$O(1)$。使用的额外空间复杂度为常数。</p>
</li>
</ul>
<h4 id="方法三：分治"><a href="#方法三：分治" class="headerlink" title="方法三：分治"></a>方法三：分治</h4><p>注意到$LCP$的计算满足结合律，有以下结论：</p>
<p>​        <script type="math/tex">LCP(S_1...S_n) = LCP(LCP(S_1...S_K),LCP(S_{k+1}...S_N))</script></p>
<p>其中$LCP(S_1…S_n)$是字符串$S_1…S_n$的最长公共前缀，$1&lt;K&lt;n$。</p>
<p>基于上述结论，可以使用分治法得到字符串数组中的最长公共前缀。对于问题$LCP(S_i…S_j)$，可以分解成两个字问题$LCP(S_i…S_{mid})$与$LCP(S_{mid+1}…S_{j})$，其中$mid$值为二分之$i+j$。对于两个子问题分别求解，然后对两个子问题的解计算最长公共前缀，即为原问题的解。</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/06/15/ybQ3iH8joIAanxJ.png" alt="3.png" style="zoom:50%;" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">longestCommonPrefix</span><span class="params">(self, strs: List[str])</span> -&gt; str:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">lcp</span><span class="params">(start, end)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> start == end:</span><br><span class="line">                <span class="keyword">return</span> strs[start]</span><br><span class="line"></span><br><span class="line">            mid = (start + end) // <span class="number">2</span></span><br><span class="line">            lcpLeft, lcpRight = lcp(start, mid), lcp(mid + <span class="number">1</span>, end)</span><br><span class="line">            minLength = min(len(lcpLeft), len(lcpRight))</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(minLength):</span><br><span class="line">                <span class="keyword">if</span> lcpLeft[i] != lcpRight[i]:</span><br><span class="line">                    <span class="keyword">return</span> lcpLeft[:i]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> lcpLeft[:minLength]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span> <span class="keyword">if</span> <span class="keyword">not</span> strs <span class="keyword">else</span> lcp(<span class="number">0</span>, len(strs) - <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h4 id="方法四：二分查找"><a href="#方法四：二分查找" class="headerlink" title="方法四：二分查找"></a>方法四：二分查找</h4><p>显然，最长公共前缀的长度不会超过字符串数组中的最短字符串的长度。用 $minLength$ 表示字符串数组中的最短字符串的长度，则可以在 $[0,minLength] $的范围内通过二分查找得到最长公共前缀的长度。每次取查找范围的中间值 $mid$，判断每个字符串的长度为 $mid$ 的前缀是否相同，如果相同则最长公共前缀的长度一定大于或等于 $mid$，如果不相同则最长公共前缀的长度一定小于$mid$，通过上述方式将查找范围缩小一半，直到得到最长公共前缀的长度。</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/06/15/PQH8k5yO3gEAoVd.png" alt="4.png" style="zoom:50%;" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">longestCommonPrefix</span><span class="params">(self, strs: List[str])</span> -&gt; str:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">isCommonPrefix</span><span class="params">(length)</span>:</span></span><br><span class="line">            str0, count = strs[<span class="number">0</span>][:length], len(strs)</span><br><span class="line">            <span class="keyword">return</span> all(strs[i][:length] == str0 <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, count))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> strs:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">""</span></span><br><span class="line"></span><br><span class="line">        minLength = min(len(s) <span class="keyword">for</span> s <span class="keyword">in</span> strs)</span><br><span class="line">        low, high = <span class="number">0</span>, minLength</span><br><span class="line">        <span class="keyword">while</span> low &lt; high:</span><br><span class="line">            mid = (high - low + <span class="number">1</span>) // <span class="number">2</span> + low</span><br><span class="line">            <span class="keyword">if</span> isCommonPrefix(mid):</span><br><span class="line">                low = mid</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                high = mid - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> strs[<span class="number">0</span>][:low]</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>Summary of learning resources</title>
    <url>/2020/04/17/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB/</url>
    <content><![CDATA[<blockquote>
<blockquote>
<p>Some links about graph network.</p>
<p>Learning resources about GNN  graph embedding  LSTM etc..</p>
</blockquote>
</blockquote>
<ol>
<li><p><a href="https://mp.weixin.qq.com/s/zmX0L6ZeCqlTBYtP9XslgA" target="_blank" rel="noopener">自监督黑马SimCLRv2来了！提出蒸馏新思路，可迁移至小模型，性能精度超越有监督</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/Z1WeYOV4Kwp6os73FqG9ew" target="_blank" rel="noopener">近期必读的五篇KDD 2020【图神经网络 (GNN) 】相关论文_Part2</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/JU11nv2lv-zFesHjp2IAQQ" target="_blank" rel="noopener">Graph: 表现再差，也不进行Pre-Training? Self-Supervised Learning真香！</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/LajTagTaCHBNoqogo1zEiw" target="_blank" rel="noopener">KDD2020推荐系统论文聚焦</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/alVFJvksR0W6NMErubTM_g" target="_blank" rel="noopener">IJCAI2020 图相关论文集</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/WslDM6Mgx7_i5yW3XFjdAw" target="_blank" rel="noopener">【DL】规范化：你确定了解我吗？</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/iECG0sBte25li-E3Z_tEIw" target="_blank" rel="noopener">用 Python 训练自己的语音识别系统，这波操作稳了！</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/yKxwqwCC_Jxse_z3mK9lAw" target="_blank" rel="noopener">【CTR】MMoE-PosBias：Youtube 多任务学习框架</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/Rqz9yuSxpx_os54FrzyMqQ" target="_blank" rel="noopener">一文尽览推荐系统模型演变史(文末可下载)</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/iMU2LPDadmVMgzUifw3-XA" target="_blank" rel="noopener">浅谈电商搜索推荐中ID类特征的统一建模：Hema Embedding解读</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/ifTNRW0W7-P_LyfNldtavQ" target="_blank" rel="noopener">多任务学习方法在推荐中的演变</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/5SWMKuKC3XLvUdbKS8qJZw" target="_blank" rel="noopener">近期必读的六篇顶会 ICML 2020【图神经网络 (GNN) 】相关论文</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/80Ze31Nstqp5nuRD7B3vKw" target="_blank" rel="noopener">图深度学习：成功，挑战以及后面的路</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/4wzbZqU4rJcl6qEZTI8m2A" target="_blank" rel="noopener">KDD2020|混合时空图卷积网络：更精准的时空预测模型</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/h61viD2JvOcb6SpHrk5PQg" target="_blank" rel="noopener">一文概览如何消除广告和推荐中的Position Bias</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/c0hPqwfbgdSKGvJwN5nX3A" target="_blank" rel="noopener">WSDM 2020关于深度推荐系统与CTR预估工业界必读的论文</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/J6GUf-pAXmI9jhFGyVyU8w" target="_blank" rel="noopener">图系列|三篇图层次化表示学习(Hierarchical GNN)：图分类以及节点分类</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/A64paLV_3Arhu1LKvKc63w" target="_blank" rel="noopener">PinSAGE | GCN 在工业级推荐系统中的应用</a><br>《Graph Convolutional Neural Networks for Web-Scale Recommender Systems》2018.</p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/ewzsURiU7bfG3gObzIP2Mw" target="_blank" rel="noopener">深度长文：图神经网络欺诈检测方法总结</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/Yj_yP6LjxrG_UJX9W3cAHQ" target="_blank" rel="noopener">腾讯 at IJCAI 2020，基于内部-环境注意力网络的推荐多队列冷启动召回</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/rjWOkwzX3IE59Kc9P9leAQ" target="_blank" rel="noopener">格“物”致知：多模态预训练再次入门</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/FtUxxRWkUJ7346gWAJqQDQ" target="_blank" rel="noopener">【KDD2020-MSU】图结构学习的鲁棒图神经网络，克服对抗攻击提升GNN防御能力</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/6OMMboBdbLVA-HsEjA3bSA" target="_blank" rel="noopener">Ctr 预估之 Calibration</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/aNx9or0-eHHT1XI1BJyB7w" target="_blank" rel="noopener">综述|73页近百篇参考文献JMLR20动态图上的表示学习</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/lbRymEKPcCrLsk9r9w1tlQ" target="_blank" rel="noopener">高性能涨点的动态卷积 DyNet 与 CondConv、DynamicConv 有什么区别联系？</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/7_cZhszsnfBKyoUKYMkQhw" target="_blank" rel="noopener">近期必读的五篇计算机视觉顶会CVPR 2020【图神经网络 (GNN) 】相关论文-Part 3</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/KHbCbzP1JMgoYMjY5cMOmA" target="_blank" rel="noopener">华为诺亚实验室开源Disout算法，直接对标谷歌申请专利的Dropout算法</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/n3CSdTdsk5erpwRpZE9Qvw" target="_blank" rel="noopener">图专题|IJCAI2020两篇多层次/多视角相关的图神经网络GNN研究论文</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/_VBO_9yiZ0qngq4yfwcxEg" target="_blank" rel="noopener">从EMD、WMD到WRD：文本向量序列的相似度计算</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/Fjr2iCbeTy9AYNAwvJr7Og" target="_blank" rel="noopener">Mila唐建博士最新《图表示学习:算法与应用》2020研究进展，附59页ppt</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/DKDlFDRwoedSlhL8cu99DA" target="_blank" rel="noopener">从 Triplet loss 看推荐系统中文章Embedding</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/aCICOsif_jYNUkOTiB_-FQ" target="_blank" rel="noopener">【Code】GraphSAGE 源码解析</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/upXLNJsyq1muoNn3uJCKJg" target="_blank" rel="noopener">入门推荐系统，这25篇综述文章足够了</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/w0KhXwE3tkZtjzQcRApRow" target="_blank" rel="noopener">KDD19开源论文 Heterogeneous Graph Neural Network</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/Xn1xMXu0V7nkolX5x_h2lw" target="_blank" rel="noopener">SDM(Sequential Deep Matching Model)的复现之路</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/WnF-fqQyr2VNqr75Jzoqsw" target="_blank" rel="noopener">【GNN】Diff Pool：网络图的层次化表达</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/jIVhXMmP95G7hJP_KtksxA" target="_blank" rel="noopener">Youtube推荐RL首弹，基于Top-K的Off-Policy矫正解决推荐中的信息茧房困境</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/yyE0ki5rymOCyGa6CNfnzQ" target="_blank" rel="noopener">《深度学习推荐系统》读书笔记之Embedding技术在推荐中的应用</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/FZoiDfM05rWIqWJQg8F-5w" target="_blank" rel="noopener">RecSys 2019最佳论文：基于深度学习的推荐系统是否真的优于传统经典方法？</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/daIWHWMulVlqAsm04qyi6w" target="_blank" rel="noopener">2019年，异质图神经网络领域有哪些值得读的顶会论文？</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/63lTwUB2wsvhikWnNKa0CQ" target="_blank" rel="noopener">「工业落地」基于异质图神经网络的异常账户检测</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/Ry8R6FmiAGSq5RBC7UqcAQ" target="_blank" rel="noopener">深入理解图注意力机制（Graph Attention Network）</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/1xVPRIVwQQJfEen0RiNYvg" target="_blank" rel="noopener">谈谈推荐系统中的用户行为序列建模最新进展</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/Jmutb3fWuxxmbpX4CWS3HA" target="_blank" rel="noopener">《深度学习推荐系统》读书笔记之推荐系统的进化之路</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/zLBq2VVSAr0AUtozK7qqVA" target="_blank" rel="noopener">浅析Faiss在推荐系统中的应用及原理</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/Fg9GKw-QLlOskEqX7pl1dA" target="_blank" rel="noopener">推荐系统之FM算法原理及实现（附代码）</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/dSa0-BydmzmqDfrICJoUBQ" target="_blank" rel="noopener">ECAI2020推荐系统论文聚焦</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/wLjBcK9PQHz6lUIsiZCjgg" target="_blank" rel="noopener">长文|三大主题全方位梳理图论与图学习中的基本概念：图搜索，最短路径，聚类系数，中心度等</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/jVG_JJN5OalJcbtTGti9AA" target="_blank" rel="noopener">「工业落地」阿里异质图神经网络推荐：19KDD IntentGC</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/3gHKD8g3tB6z88DVq5AkIw" target="_blank" rel="noopener">「工业落地」阿里异构图表示学习：19KDD GATNE</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/y94JACgjTxObvNDhmUfulA" target="_blank" rel="noopener">图神经网络入门（三）GAT图注意力网络</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/3x6FtOMzGitTlwwJs-dN9w" target="_blank" rel="noopener">从ACL 2020和ICLR 2020看知识图谱嵌入的近期研究进展</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/4CPfejPrAhJuYMgSj6aGsw" target="_blank" rel="noopener">【香港理工】生成式对抗网络(GANs)最新2020综述，41页pdf阐述GAN训练、 挑战、解决方案和未来方向</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/53zPX7N7YLlUnvD23kT6pg" target="_blank" rel="noopener">SIGIR 20 | 腾讯看点首次在推荐中应用迁移学习提出PeterRec框架，从少量行为数据中学习用户表示</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/lf9BjP9qs_cn8f9vGUECpw" target="_blank" rel="noopener">比CNN更强有力，港中文贾佳亚团队提出两类新型自注意力网络｜CVPR2020</a></p>
</li>
<li><p><a href="https://www.toutiao.com/a6825468794287161860/?tt_from=weixin&amp;utm_campaign=client_share&amp;wxshare_count=1&amp;timestamp=1589325241&amp;app=news_article&amp;utm_source=weixin&amp;utm_medium=toutiao_ios&amp;req_id=202005130714000101941000345D3EC4DC&amp;group_id=6825468794287161860" target="_blank" rel="noopener">33 个神经网络「炼丹」技巧</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/azgdVJ3-Ge1HJC1mVTRVfA" target="_blank" rel="noopener">自动化神经网络理论进展缓慢，AutoML 算法的边界到底在哪</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/SqBU40sfo3IEj_iHnb4cXQ" target="_blank" rel="noopener">高效利用无标注数据：自监督学习简述</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/0Jn79XT9A70sQEMOImzFLg" target="_blank" rel="noopener">【斯坦福谷歌】最新《图机器学习》综述论文，38页pdf阐述最新图表示学习进展</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/GCoyRPKe9ND7CnG9-zWTkA" target="_blank" rel="noopener">近期必读的5篇顶会CVPR 2020【场景图+图神经网络（SG+GNN）】相关论文</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/SOaA9XNnymLgGgJ5JNSdBg" target="_blank" rel="noopener">对比学习（Contrastive Learning）相关进展梳理</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/0jPOSvSqsTEtwjQ_nRgQ_g" target="_blank" rel="noopener">干货！2019五大顶会必读的Graph Embedding相关的论文</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/W2UTCtoTqRW739EFxCFf1w" target="_blank" rel="noopener">在深度学习顶会ICLR 2020上，Transformer模型有什么新进展？</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/z0UMPHLYFOaiTCxqt65uRg" target="_blank" rel="noopener">当深度学习遇上量化交易——图与知识图谱篇</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/wLvTo_k67VwiwrfW9dfSWg" target="_blank" rel="noopener">「工业落地」阿里在图神经网络推荐的探</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/RhnyQUnXxMDybv8B-dSj4Q" target="_blank" rel="noopener">图系列|WWW2020 图学习/图神经网络GNN相关论文速览</a></p>
</li>
<li><p><a href="https://mp.weixin.qq.com/s/byVdEPcCmVPJOk-uIyGsbw" target="_blank" rel="noopener">【重磅】GCN大佬Thomas Kipf博士论文《深度学习图结构表示》178页pdf阐述图卷积神经网络等机制与应用</a></p>
</li>
</ol>
]]></content>
      <tags>
        <tag>recommendation</tag>
        <tag>GNN</tag>
      </tags>
  </entry>
  <entry>
    <title>第三章</title>
    <url>/2020/08/16/%E7%AC%AC%E4%B8%89%E7%AB%A0/</url>
    <content><![CDATA[<h1>一、深度学习推荐模型的演化关系图</h1>

<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/08/17/wEVPFOIKdDo1Rj6.jpg" alt="BBA75C44-476D-4786-B990-7EEE2FB4DF45.png" style="zoom:50%;" /></p>
<p>整体来说，主要是以<font color='red'>多层感知机(Multi-Layer Perceptron，MLP)</font>为核心，通过改变神经网络的结构来构建各异的模型，比如：</p>
<ul>
<li>改变神经网络的复杂程度</li>
<li>改变特征交叉方式</li>
<li>组合多种模型</li>
<li>FM模型的深度学习演化版本</li>
<li>注意力机制与推荐模型的结合</li>
<li>序列模型与推荐模型的结合</li>
<li>强化学习与推荐模型的结合</li>
<li>等</li>
</ul>
<h1>二、AutoRec——单隐层神经网络推荐模型</h1>

<p><em>2015 澳大利亚国立大学提出。</em></p>
<p>它将<font color='red'>自编码器（AutoEncoder）的思想和协同过滤</font>结合，提出了一种单隐层神经网络推荐模型。</p>
<p><b>原理：</b></p>
<p>​        利用协同过滤中的共现矩阵，完成物品向量或用户向量的自编码。再利用自编码的结果得到用户对物品的预估评分，进而进行推荐排序。</p>
<blockquote>
<p>自编码器：假设其数据向量为r，自编码器的作用是将向量r作为输入，通过自编码器后，得到的输出向量尽量接近其本身。</p>
<p>假设自编码器的重建函数为$ h(r;\theta) $，那么自编码器的目标函数为：</p>
<script type="math/tex; mode=display">min_\theta\sum_{r\in S}||r - h(r;\theta)||_2^2</script><p>其中，S是所有数据向量的集合。</p>
<p>一般来说，重建函数的参数数量远小于输入向量的维度数量，因此自编码器相当于完成了<b>数据压缩和降维</b>的工作。</p>
</blockquote>
<p>AutoRec使用单隐层神经网络的结构来解决构建重建函数的问题。模型结构图如下：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/08/23/wo71GfOnrbxZcI6.jpg" alt="84CCD42D-718C-4122-8F08-7A8DCD155589.png" style="zoom:50%;" /></p>
<p>网络的输入层是物品的评分向量<code>r</code>，输出层是一个多分类层。图中蓝色的神经元代表模型的<code>k</code>维单隐层，其中<code>k&lt;&lt;m</code>。图中的<code>V</code>和<code>W</code>分别代表输入层到隐层，以及隐层到输出层的参数矩阵。该模型结构代表的重建函数的具体形式如：</p>
<script type="math/tex; mode=display">h(r;\theta)=f(W·g(V_r+\mu)+b)</script><p>其中，<code>f(.)</code>，<code>g(.)</code>分别为输出层神经元和隐层神经元的激活函数。</p>
<p><b>为防止过拟合，在加入<code>L2</code>正则化后，</b>AutoRec目标函数的具体形式为：</p>
<script type="math/tex; mode=display">min_\theta\sum_{i=1}^n||r^{(i)} - h(r^{(i)};\theta)||_2^2+\lambda/2·(||W||_F^2+||V||_F^2)</script><p>模型的训练利用梯度反向传播即可完成。</p>
<h1>三、Deep Crossing模型</h1>

<p><em>2016年，微软提出Deep Crossing模型，一次深度学习架构在推荐系统中的完整应用。</em></p>
<p><b>应用场景</b>：微软搜索引擎Bing中的搜索广告推荐场景。</p>
<p><b>目标：</b>用户搜索关键词后，搜索引擎除了返回相关结果，还会返回与搜索词相关的广告，因此要尽可能地<font color='red'>增加搜索广告的点击率，准确地预测广告点击率。</font></p>
<p>该模型完整的解决了从<b>特征工程、稀疏向量稠密化、多层神经网络进行优化目标拟合</b>等一系列深度学习在推荐系统中的应用问题。</p>
<p>基于此，微软使用的特征分成了三类：</p>
<ul>
<li>可以被处理成one-hot或者multi-hot向量的<b>类别型特征</b>：用户搜索词（query）、广告关键词（keyword）、广告标题（title）、落地页（landing page）、匹配类型（match type）；</li>
<li><b>数值型特征</b>：点击率、预估点击率（click prediction）；</li>
<li><b>需要进一步处理的特征</b>：广告计划（campaign）、曝光样例（impression）、点击样例（click）等，由于这些是一个特征的组别，就要把这些具体的部分拆开来分别处理。</li>
</ul>
<p><b>解决的问题：</b></p>
<ul>
<li>离散类特征编码后过于稀疏，不利于直接输入神经网络进行训练，<font color='blue'>如何解决稀疏特征向量稠密化的问题</font>（Embedding层、Stacking层）；</li>
<li><font color='blue'>如何解决特征自动交叉组合的问题</font>（Multiple Residual Units层）</li>
<li><font color='blue'>如何在输出层中达成问题设定的优化目标</font>（Scoring层）</li>
</ul>
<p><b>网络结构如下：</b></p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/08/25/Y2rRUmLtgyBz3ih.jpg" alt="4EB1E155-2663-49C3-AD1E-A07C00A48211.png" style="zoom:50%;" /></p>
<p>包括4层：Embedding层、stacking层、Multiple Residual Units层和Scoring层。各层作用如下：</p>
<ol>
<li>Embedding层：作用是将稀疏的类别型特征转换成稠密的Embedding向量，具体的策略包括Word2vec、Graph Embedding等；</li>
<li>Stacking层：作用是把不同的Embedding特征和数值型特征拼接在一起，形成新的包含全部特征的特征向量；</li>
<li>Multiple Residual Units层：作用是对特征向量各个维度进行充分的交叉组合。主要结构是多层感知机，该模型采用了<b>多层残差网络</b>作为MLP的具体实现。</li>
<li>Scoring层：作为输出层，作用是为了拟合优化目标存在的，对于CTR预估这类二分类问题，Scoring层往往使用逻辑回归模型；而对于图像分类等多分类问题，Scoring层往往采用softmax模型。</li>
</ol>
<blockquote>
<p><b>残差神经网络：</b></p>
<p>残差神经网络就是由残差单元组成的神经网络，具体结构如下：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/08/25/fQyPaNArvOxdo9T.jpg" alt="E7D0AD35-937A-4A65-B49E-F8D6CCBF8D26.png" style="zoom:33%;" /></p>
<p>特点：</p>
<ol>
<li>输入经过两层以ReLU为激活函数的全连接层后，生成输出向量；</li>
<li>输入可以通过一个短路通路直接与输出向量进行元素加操作，生成最终的输出向量</li>
</ol>
<p>在这样的结构下，残差单元中的两层ReLU网络其实拟合的是<font color='red'>输出和输入之间的残差</font>（$x^0-x^i$），这就是为什么要叫做残差神经网络。</p>
<p>残差神经网络的诞生主要是为了解决两个问题：</p>
<ul>
<li>神经网络是不是越深越好？对于传统的基于感知机的神经网络，当网络加深之后，往往存在过拟合现象，即网络越深，在测试集上的表现越差。而在残差网络中，由于有输入向量短路的存在，很多时候可以越过两层ReLU网络，减少过拟合现象的发生。</li>
<li>当神经网络足够深时，往往存在严重的梯度消失现象。（梯度消失现象是指在梯度反向传播过程中，越靠近输入端，梯度的幅度越小，参数收敛的速度越慢。）为了解决这个问题，残差单元使用了ReLU激活函数取代原来sigmoid激活函数。此外，输入向量短路相当于直接把梯度毫无变化地传递到下一层，这也使残差网络的收敛速度更快。</li>
</ul>
</blockquote>
<h1>四、NeuralCF模型</h1>

<p><em>2017年，新加坡国立大学提出基于深度学习的协同过滤模型NeuralCF。</em></p>
<p>下图为传统矩阵分解的网络化形式表示和NeuralCF模型的对比：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/08/25/EF7geXozukZxjhY.jpg" alt="944F9428-2FAE-4623-A4D9-1D7F4255B2E3.png" style="zoom:50%;" /></p>
<p>图的左半部分是传统MF的网络化形式表示，其中用户隐向量和物品隐向量都可以看作是Embedding层。可以看出，NeuralCF模型用“多层神经网络+输出层”的结构替代了矩阵分解模型中简单的内积操作。优点是：</p>
<ol>
<li>让用户向量和物品向量做更充分的交叉，得到更多有价值的特征组合信息；</li>
<li>引入更多的非线性特征让模型的表达能力更强。</li>
</ol>
<p><b>广义矩阵分解模型（Generalized Matrix Factorization）：</b>用任意的互操作形式代替用户和物品向量的互操作层。</p>
<p>基于此，该论文还提出了一种混合模型，整合了原始NeuralCF模型和以元素积为互操作的广义矩阵分解模型，如下：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/08/25/Xw4WhbKeuCJMVLY.jpg" alt="45B2031A-5ADC-4C51-95D2-27D2A3C38AB0.png" style="zoom:50%;" /></p>
<blockquote>
<p><b>softmax函数：</b></p>
<p>目前很多深度模型的输出层都使用softmax函数，<font color='red'>解决多分类问题的目标拟合问题。</font></p>
<p>数学形式：</p>
<script type="math/tex; mode=display">\sigma(X)_i=\frac{exp(x_i)}{\sum^n_{j=1}exp(x_i)},当i=1,...,n且X=[x_1,...,x_n]^T\in\mathbb{R}</script><p>可以看出，softmax函数解决了从一个原始的n维向量，向一个n维的概率分布映射问题。</p>
<p>在分类问题上，softmax函数往往和交叉熵（cross-entropy）损失函数一起使用：</p>
<script type="math/tex; mode=display">LOSS_{Cross Entropy}=-\sum_iy_iln(\sigma(x)_i)</script><p>其中$y_i$是第i个分类的真实标签值，$\sigma(x)_i$代表softmax函数对第i个分类的预测值。</p>
<p>因为softmax函数把分类输出标准化成了多个分类的概率分布，而交叉熵正好刻画了预测分类和真实结果之间的相似度，所以softmax函数往往与交叉熵搭配使用。</p>
<p>softmax函数的导数形式为：</p>
<script type="math/tex; mode=display">{\frac{\partial\sigma(x)_i}{\partial x_j}}=\begin{cases}
   \sigma(x)_i(1-\sigma(x)_j), & i=j \\
   -\sigma(x)_i·\sigma(x)_j & i\neq j
   \end{cases}</script><p>基于链式法则，交叉熵函数到softmax函数第j维输入$x_j$的导数形式为：</p>
<script type="math/tex; mode=display">\frac{\partial Loss}{\partial x_j}=\frac{\partial Loss}{\partial \sigma(x)}·\frac{\partial \sigma(x)}{\partial x_j}</script><p>在多分类问题中，真实值中只有一个维度是1，其余维度都为0，假设第k维是1，即$y_k=1$，那么交叉熵损失函数可以简化成如下形式：</p>
<script type="math/tex; mode=display">LOSS_{Cross Entropy}=-\sum_iy_iln(\sigma(x)_i)=-y_k·ln(\sigma(x)_k)=-ln(\sigma(x)_k)</script><p>则有：</p>
<script type="math/tex; mode=display">\frac{\partial Loss}{\partial x_j}=\frac{\partial(-ln(\sigma(x)_k))}{\partial\sigma(x)_k}·\frac{\partial\sigma(x)_k}{\partial x_j}=-\frac{1}{\sigma(x)_k}·\frac{\partial\sigma(x)_k}{\partial x_j}=\begin{cases}\sigma(x)_j-1, & j=k \\
   \sigma(x)_j, & j\neq k
   \end{cases}</script><p>即$j=k$时，结果为算出的值减一，$j\neq k$时，为算出来的值。</p>
<p>可以看出，softmax函数与交叉熵的配合，不仅在数学含义上完美统一，而且在梯度形式上也非常简洁。基于上式的梯度形式，通过梯度反向传播的方法，即可完成整个神经网路权重的更新。</p>
</blockquote>
<h1>五、PNN模型</h1>

<p><em>2016年，上海交大提出PNN模型，给出了特征交互方式的几种设计思路。</em></p>
]]></content>
      <categories>
        <category>《深度学习推荐系统》</category>
      </categories>
      <tags>
        <tag>recommendation</tag>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title>metapath2vec: Scalable Representation Learning for Heterogeneous Networks</title>
    <url>/2019/12/12/metapath2vec-Scalable-Representation-Learning-for-Heterogeneous-Networks/</url>
    <content><![CDATA[<blockquote>
<blockquote>
<p>This article proposed two scalable representation learning models: <strong>metapath2vec</strong> and <strong>metapath2vec++</strong>. </p>
<p>In metapath2vec: </p>
<ul>
<li>First, they propose <strong>meta-path</strong> based <strong>random walks</strong> in heterogeneous networks to generate <strong>hetetogeneous neighborhoods</strong> with network semantics for various types of nodes.</li>
<li>Second, they extend the <strong>skip-gram</strong> model to facilitate the modeling of geographically and semantically close nodes.</li>
</ul>
<p>In metapath2vec++:</p>
<ul>
<li>they develop a <strong>heterogeneous negative sampling-based</strong> method that enables the accurate and efficient prediction of a node’s heterogeneous neighborhood.</li>
</ul>
<p><a href="http://dx.doi.org/10.1145/3097983.3098036" target="_blank" rel="noopener">http://dx.doi.org/10.1145/3097983.3098036</a></p>
</blockquote>
</blockquote>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/05/25/Afdic56XJB1YWKT.jpg" alt="1.jpeg" style="zoom:50%;" /></p>
<h2>Data</h2>

<ul>
<li>AMiner Computer Science dataset</li>
<li>Database and Information Systems dataset</li>
</ul>
<h2>Coding</h2>

<ul>
<li><a href="https://ericdongyx.github.io/metapath2vec/m2v.html" target="_blank" rel="noopener">https://ericdongyx.github.io/metapath2vec/m2v.html</a> </li>
</ul>
]]></content>
      <categories>
        <category>Heterogeneous Networks</category>
      </categories>
      <tags>
        <tag>meta path</tag>
        <tag>heterogeneous</tag>
        <tag>skip-gram</tag>
        <tag>negative sampling-based method</tag>
      </tags>
  </entry>
  <entry>
    <title>第一章</title>
    <url>/2020/06/27/%E7%AC%AC%E4%B8%80%E7%AB%A0/</url>
    <content><![CDATA[<h2>第一章、互联网的增长引擎---推荐系统</h2>

<hr>
<h4>1.1 为什么推荐系统是互联网的增长引擎</h4>

<p>用户角度：推荐系统解决在“信息过载”的情况下，用户如何高效获得感兴趣信息的问题。从用户需求层面看，推荐系统是在用户需求并不十分明确的情况下，进行信息的过滤。更多的是利用用户的各类历史信息“猜测”其可能喜欢的内容。</p>
<p>公司角度：推荐系统解决产品能够最大限度地吸引用户、留存用户、增加用户黏性、提高用户转化率的问题。</p>
<h4>1.2 推荐系统的架构</h4>

<p>两个部分：</p>
<ul>
<li>“数据和信息”部分逐渐发展为推荐系统中融合了数据离线批处理、实时流处理的数据流框架；</li>
<li>“算法和模型”部分则进一步细化为推荐系统中集训练（training）、评估（evaluation）、部署（deployment）、线上推断（online inference）为一体的模型框架。<b>召回层、排序层、补充策略于算法层</b></li>
</ul>
]]></content>
      <categories>
        <category>《深度学习推荐系统》</category>
      </categories>
      <tags>
        <tag>recommendation</tag>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title>第二章：推荐系统的进化之路</title>
    <url>/2020/06/27/%E7%AC%AC%E4%BA%8C%E7%AB%A0/</url>
    <content><![CDATA[<h2>一、整体框架</h2>

<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/07/11/vndB6QLsTbFJNGg.jpg" alt="1.png"></p>
<p>传统推荐系统的发展脉络主要由以下四个部分组成：</p>
<ul>
<li><b>协同过滤算法族：</b>物品协同过滤（ItemCF）、用户协同过滤（UserCF）、矩阵分解模型（Matrix Factorization）及各分支模型；</li>
<li><b>逻辑回归模型族：</b>逻辑回归能够利用和融合更多用户、物品和上下文特征。LR模型、LS-PLM模型等；</li>
<li><b>因子分解机模型族：</b>在传统逻辑回归的基础上，加入了二阶部分，使模型具备了进行<font color='red'>特征组合</font>的能力。FM模型、FFM模型；</li>
<li><b>组合模型：</b>为了融合多个模型的优点，将不同模型组合使用是构建推荐模型常用的方法。GBDT+LR等。</li>
</ul>
<h2>二、协同过滤</h2>

<p><em>协同过滤的提出：2003年Amazon发表文章<a href="https://ieeexplore.ieee.org/document/1167344" target="_blank" rel="noopener">Amazon.com recommendations: item-to-item collaborative filtering</a></em></p>
<h4 id="2-1-定义"><a href="#2-1-定义" class="headerlink" title="2.1 定义"></a>2.1 定义</h4><p>协同大家的反馈、评价和意见一起对海量的信息进行过滤，从中筛选出目标用户可能感兴趣的信息的推荐过程。</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/07/11/Xjb4Q8YEUoDknJ3.jpg" alt="366139DF-46C7-47B9-A03F-7237B62B0EEA.png"></p>
<p><b>共现矩阵：</b>以上图为例，将有向图转成矩阵的形式，用户作为矩阵行坐标，商品作为列坐标，将“点赞”和“踩”的用户行为数据转换为矩阵中对应的元素值。</p>
<h4 id="2-2-用户相似度计算"><a href="#2-2-用户相似度计算" class="headerlink" title="2.2 用户相似度计算"></a>2.2 用户相似度计算</h4><p>对于用户向量<code>i</code>和用户向量<code>j</code></p>
<ol>
<li>余弦相似度：$sim(i,j)=cos(i,j)=\frac{i·j}{||i||·||j||}$</li>
<li>皮尔逊相关系数：<script type="math/tex; mode=display">sim(i,j)=\frac{\Sigma_{p\in P}(R_{i,P} - \hat{R}_i)(R_{j,P}-\hat{R}_j)}{\sqrt[]{\Sigma_{p\in P}(R_{i,P}- \hat{R}_i)^2}\sqrt[]{\Sigma_{p\in P}(R_{j,P}- \hat{R}_j)^2}}</script>其中，<code>R_{i,p}</code>代表用户<code>i</code>对物品<code>p</code>的评分，$\hat{R}_i$是代表用户<code>i</code>对所有物品的平均评分，<code>p</code>代表所有物品的集合。</li>
<li>基于皮尔逊系数的思路，还可以通过引入物品平均分的方式，<b>减少物品评分偏置对结果的影响:</b><script type="math/tex; mode=display">sim(i,j)=\frac{\Sigma_{p\in P}(R_{i,P} - \hat{R}_P)(R_{j,P}-\hat{R}_P)}{\sqrt[]{\Sigma_{p\in P}(R_{i,P}- \hat{R}_P)^2}\sqrt[]{\Sigma_{p\in P}(R_{j,P}- \hat{R}_P)^2}}</script>其中，$\hat{R}_p$代表物品<code>p</code>得到所有评分的平均分。</li>
</ol>
<h4>2.3 最终结果的排序</h4>

<p>假设“目标用户与其相似用户的喜好是相似的”</p>
<p>最常用的方式是<b>利用用户相似度和相似用户的评价的加权平均获得目标用户的评分预测</b>，如下：</p>
<script type="math/tex; mode=display">R_{u,p}=\frac{\Sigma_{s\in S}(W_{u,s}·R_{s,p})}{\Sigma_{s\in S}W_{u,s}}</script><p>其中，权重$W_{u,s}$是用户<code>u</code>和用户<code>s</code>的相似度，$R_{s,P}$是用户<code>s</code>对物品<code>p</code>的评分。</p>
<h4>2.4 ItemCF</h4>

<p>ItemCF是基于物品相似度进行推荐的协同过滤算法。通过计算共现矩阵中物品列向量的相似度得到物品之间的相似矩阵，再找到用户的历史正反馈物品的相似物品进行进一步排序和推荐。</p>
<ol>
<li>基于历史数据，构建以用户（假设用户总数为m）为行坐标，物品（物品总数为n）为列坐标的<code>m * n</code>维的共现矩阵。</li>
<li>计算共现矩阵两两列向量间的相似性（相似度的计算方式与用户相似度的计算方式相同），构建<code>n * n</code>维的物品相似度矩阵。</li>
<li>获得用户历史行为数据中的正反馈物品列表。</li>
<li>利用物品相似度矩阵，针对目标用户历史行为中的正反馈物品，找出相似的Top k个物品，组成相似物品集合。</li>
<li>对相似物品集合中的物品，利用相似度分值进行排序，生成最终的推荐列表。</li>
</ol>
<h4 id="2-5-CF的缺点"><a href="#2-5-CF的缺点" class="headerlink" title="2.5 CF的缺点"></a>2.5 CF的缺点</h4><ul>
<li>CF的天然缺陷：推荐结果的头部效应较明显，处理稀疏向量的能力弱。</li>
<li>CF仅利用用户和物品的交互信息，无法有效地引入用户年龄、性别、商品描述、商品分类、当前时间等一系列用户特征、物品特征和上下文特征。</li>
</ul>
<h2>三、矩阵分解</h2>

<p>矩阵分解在CF中“共现矩阵”的基础上，加入了<b>隐向量</b>的概念，加强了模型处理稀疏矩阵的能力。</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/07/11/bFJ4WoyBGTvUDpg.jpg" alt="A9DA0BC6-B77A-485E-8A5D-DE44BE271712.png" style="zoom:50%;" /></p>
<h4>3.1矩阵分解的求解过程</h4>

<p>主要方法有三种：</p>
<ul>
<li>特征值分解（Eigen Decomposition）：只能作用于方阵</li>
<li>奇异值分解（singular Value Decomposition, SVD）：<img src= "img/loading.gif" data-src="https://i.loli.net/2020/07/11/Uzw2Id8ER1t3Prx.jpg" alt="6812192D-0640-4957-90E1-9A131EF5B6A9.png"></li>
<li>梯度下降（Gradient Descent）:<br>求解矩阵分解的目标函数：<script type="math/tex; mode=display">min_{q^*,p^*}\Sigma_{(u,i)\in K}(r_{u,i}-q_i^Tp_u)^2+\lambda(||q_i||+||p_u||)^2</script></li>
</ul>
<p>在矩阵分解算法中，由于隐向量的存在，使人意的用户和物品之间都可以得到预测分值。而隐向量的生成过程其实是对共现矩阵进行全局拟合的过程，因此隐向量其实是利用全局信息生成的，有更强的泛化能力。</p>
<h4 id="3-2-MF优缺点"><a href="#3-2-MF优缺点" class="headerlink" title="3.2 MF优缺点"></a>3.2 MF优缺点</h4><ul>
<li><p>优点：</p>
<ol>
<li>泛化能力强</li>
<li>空间复杂度低：(n+m)·k</li>
<li>更好的扩展性和灵活性</li>
</ol>
</li>
<li><p>缺点：</p>
<ol>
<li>MF同样不方便加入用户、物品和上下文相关的特征</li>
<li>在缺乏用户历史行为时，无法进行有效的推荐</li>
</ol>
</li>
</ul>
<h2>四、逻辑回归</h2>

<p>逻辑回归将推荐问题看成一个<font color='red'>分类</font>问题，<b>通过预测正样本的概率对物品进行排序。</b></p>
<p>即为<font color='red'>点击率（Click Through Rate，CTR）</font>预测问题。</p>
<h4>4.1 流程：</h4>

<ol>
<li>将用户年龄、性别、物品属性、物品描述、当前时间、当前地点等特征转换成数值型特征向量；</li>
<li>确定逻辑回归模型的优化目标（以优化“点击率”为例），利用已有样本数据对逻辑回归模型进行训练，确定逻辑回归模型的内部参数；</li>
<li>在模型服务阶段，将特征向量输入逻辑回归模型，经过逻辑回归模型的推断，得到用户“点击”（这里用点击作为推荐系统正反馈行为的例子）物品的概率；</li>
<li>利用“点击”概率对所有候选物品进行排序，得到推荐列表。</li>
</ol>
<h4>4.2 数学形式</h4>

<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/08/01/JqcASsOK8b2Qjz3.jpg" alt="6FA14D81-4CD7-40A6-8302-2CCE421AA3F7.png" style="zoom:50%;" /></p>
<p>由上图所示，逻辑回归模型将特征向量$x=(x_1,x_2,…,x_n)$作为模型的输入，分别乘上相应的权重系数后再想加，最终，将得到的$x^Tw$输入到sigmoid函数中，使之映射到0-1的区间，得到最终的点击率，因此，最终的数学形式为：</p>
<script type="math/tex; mode=display">f(x)=\frac{1}{1+e^{-(w·x+b)}}</script><p>由数学形式可以看出，LR模型要确定的参数就是相应的权重向量w，常见的训练方法是<b>梯度下降法、牛顿法、拟牛顿法等。</b></p>
<h4>4.3 LR的优缺点</h4>

<p>优点：</p>
<ol>
<li>。数学含义上的支撑（假设因变量y服从伯努利分布）；</li>
<li>可解释性强；</li>
<li>简单、直观、易用，工程化的需要。</li>
</ol>
<p>缺点：</p>
<ul>
<li>表达能力不强。</li>
</ul>
<h2>五、自动特征交叉的解决方案</h2>

<h4>5.1 “辛普森悖论”</h4>

<p>​    LR模型表达能力不强的问题，会不可避免地造成有效信息的损失。在仅利用单一特征而非交叉特征进行判断的情况下，有时不仅是信息损失的问题，甚至会得出错误的结论。</p>
<p>​    定义：<font color=#2196F3 face="宋体">在对样本集合进行分组研究时，在分组比较中都占优势的一方，在总评中有时反而是失势的一方，这种有悖常理的现象，被称为“辛普森悖论”。</font></p>
<p>比如，在视频应用点击中：</p>
<p>男性用户：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>视频</th>
<th>点击（次）</th>
<th>曝光（次）</th>
<th>点击率</th>
</tr>
</thead>
<tbody>
<tr>
<td>视频A</td>
<td>8</td>
<td>530</td>
<td>1.51%</td>
</tr>
<tr>
<td>视频B</td>
<td>51</td>
<td>1520</td>
<td>3.36%</td>
</tr>
</tbody>
</table>
</div>
<p>女性用户：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>视频</th>
<th>点击（次）</th>
<th>曝光（次）</th>
<th>点击率</th>
</tr>
</thead>
<tbody>
<tr>
<td>视频A</td>
<td>201</td>
<td>2510</td>
<td>8.01%</td>
</tr>
<tr>
<td>视频B</td>
<td>92</td>
<td>1010</td>
<td>9.11%</td>
</tr>
</tbody>
</table>
</div>
<p>从上面两表看，无论男性还是女性用户，对视频B的点击率都高于视频A，显然推荐系统应该优先考虑向用户推荐视频B。</p>
<p>但如果<b>忽略性别</b>这个维度，将数据汇总为下表：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>视频</th>
<th>点击（次）</th>
<th>曝光（次）</th>
<th>点击率</th>
</tr>
</thead>
<tbody>
<tr>
<td>视频A</td>
<td>209</td>
<td>3040</td>
<td>6.88%</td>
</tr>
<tr>
<td>视频B</td>
<td>143</td>
<td>2530</td>
<td>5.65%</td>
</tr>
</tbody>
</table>
</div>
<p>从汇总的表看，视频A的点击率居然比视频B高，如果据此进行推荐，将得出与之前的结果完全相反的结果，这就是“辛普森悖论”。</p>
<h4>5.2 POLY2模型</h4>

<p>数学形式：</p>
<script type="math/tex; mode=display">\phi POLY2(w,x)=\sum_{j_1=1}^n\sum_{j_2=j_1+1}^nw_{h(j_1,j_2)}x_{j_1}x_{j_2}</script><p><em>$x_{j_1}$和$x_{j_2}$是两个特征，$w_{h(j_1,j_2)}$是赋予的对应特征组合的权重。</em></p>
<p><b>缺点：</b>输入数据本来就很稀疏，POLY2进行无选择的特征交叉会使特征向量更加稀疏，无法收敛；而且会使权重参数的数量从n直接上升到$n^2$。</p>
<h4>5.3 FM模型</h4>

<p><em>2016年，由Rendle提出</em></p>
<p>数学形式：</p>
<script type="math/tex; mode=display">\phi FM(w,x)=\sum_{j_1=1}^n\sum_{j_2=j_1+1}^n(w_{j_1}·w_{j_2})x_{j_1}x_{j_2}</script><p>与POLY2不同，FM模型用<span style="border-bottom:2px dashed red;">两个向量的内积</span>/$(w_{j_1}·w_{j_2})$<span style="border-bottom:2px dashed red;">取代了单一的权重系数</span>$w_{h(j_1,j_2)}$。该模型会为每个特征学习一个隐权重向量。</p>
<p><b>优点：</b></p>
<ol>
<li>通过引入特征隐向量，直接把POLY2模型$n^2$级别的权重参数数量减少到nk(k为隐向量维度)；</li>
<li>隐向量的引入使FM模型能更好的解决数据稀疏性的问题。</li>
</ol>
<h4>5.4 FFM模型</h4>

<p><em>2015年，基于FM提出的FFM在多项CTR预测大赛中夺魁，并被Criteo、美团等公司深度应用在推荐系统、CTR预测等领域。</em></p>
<p>主要是引入了<font color=#2196F3 face="宋体">特征域感知（field-aware）</font>概念。</p>
<p>数学形式：</p>
<script type="math/tex; mode=display">\phi FM(w,x)=\sum_{j_1=1}^n\sum_{j_2=j_1+1}^n(w_{j_1，f_2}·w_{j_2,f_1})x_{j_1}x_{j_2}</script><p><span style="border-bottom:2px dashed red;">与FM的区别在于w不同，这意味着每个特征对应的不是唯一一个隐向量，而是一组隐向量。</span></p>
<p><b>FFM模型只能做二阶的特征交叉，如果继续提高特征交叉的维度，会不可避免的产生组合爆炸和计算复杂度过高的问题。</b></p>
<p><b>做个小结：</b></p>
<ul>
<li><span style="border-bottom:2px dashed red;">POLY2直接学习每个交叉特征权重；</span></li>
<li><span style="border-bottom:2px dashed red;">FM学习特征的k维隐向量，交叉特征由对应向量的隐向量内积得到；</span></li>
<li><span style="border-bottom:2px dashed red;">FFM每个特征选择与对方域对应的隐向量做内积。</span></li>
</ul>
<h2>六、GBDT+LR</h2>

<p><em>2014年Facebook提出基于GBDT+LR（梯度下降树+逻辑回归）组合模型。</em></p>
<p>简而言之，就是利用GBDT自动进行特征筛选和组合，进而生成新的离散特征向量，再把该特征向量当作LR模型的输入，预估CTR的模型结构，图例如下：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/08/02/FcrzyjT7ACWRgMx.jpg" alt="D508E64D-9B22-459F-BE02-3F3248AF6D54.png" style="zoom: 33%;" /></p>
<p>（GBDT和LR两个过程是独立训练的）</p>
<h2>七、LS-PLM</h2>

<p><em>大规模分段线性模型（Large Scale Piece-wise Linear Model），阿里巴巴早在2012年就主流的推荐模型，2017年公之于众。</em></p>
<p><span style="border-bottom:2px dashed red;">LS-PLM，又称为MLR（Mixed Logistic Regression，混合逻辑回归），它在逻辑回归的基础上采用分而治之的思路，先对样本进行分片，再在样本分片中应用逻辑回归进行CTR预估。</span></p>
<p><b>主要是加入了聚类的思想</b></p>
<p>数学形式：</p>
<script type="math/tex; mode=display">f(x)=\sum_{i=1}^{m}\pi_i(x)·\eta_i(x)=\sum_{i=1}^{m}\frac{e^{\mu_i·x}}{\sum_{j=1}^me^{\mu_j·x}}·\frac{1}{1+e^{-w_i·x}}</script><p><em>其中超参数“分片数”m可以较好地平衡模型的拟合与推广能力</em></p>
<p>思路是：首先用<span style="border-bottom:2px dashed red;">聚类函数</span>$\pi$对样本进行分类（这里$\pi$采用了softmax函数对样本进行多分类），再用LR模型计算样本在分片中具体的CTR，然后将二者相乘之后求和。</p>
<p>优势：</p>
<ol>
<li>端到端的非线性学习能力；</li>
<li>模型的稀疏性强：LS-PLM在建模时引入了L1和L2,1范数，可以使最终训练出来的模型具有较高的稀疏度，使模型的部署更加轻量级。</li>
</ol>
<h2>八、总结</h2>

<div class="table-container">
<table>
<thead>
<tr>
<th>模型名称</th>
<th>基本原理</th>
<th>特点</th>
<th>局限性</th>
</tr>
</thead>
<tbody>
<tr>
<td>协同过滤</td>
<td>根据用户的行为历史生成用户-物品共现矩阵，利用用户相似性和物品相似性进行推荐</td>
<td>原理简单、直接，应用广泛</td>
<td>泛化能力差，处理稀疏矩阵的能力差，推荐结果的头部效应较明显</td>
</tr>
<tr>
<td>矩阵分解</td>
<td>将协同过滤算法中的共现矩阵分解为用户矩阵和物品矩阵，利用用户隐向量和物品隐向量的内积进行排序并推荐</td>
<td>相较协同过滤，泛化能力有所加强，对稀疏矩阵的处理能力有所加强</td>
<td>除了用户历史行为数据，难以利用其他用户、物品特征及上下文特征</td>
</tr>
<tr>
<td>逻辑回归</td>
<td>将推荐问题转换成类似CTR预估的二分类问题，将用户、物品、上下文等不同特征转换成特征向量，输入逻辑回归模型得到CTR，再按照预估CTR进行排序并推荐</td>
<td>能够融合多种类型的不同特征</td>
<td>模型不具备特征组合的能力，表达能力较差</td>
</tr>
<tr>
<td>FM</td>
<td>在逻辑回归的基础上，在模型中加入二阶特征交叉部分，为每一维特征训练得到相应特征隐向量，通过隐向量间的内积运算得到交叉特征权重</td>
<td>相比逻辑回归，具备了二阶特征交叉能力，模型的表达能力增强</td>
<td>由于组合爆炸问题的限制，模型不易扩展到三阶特征交叉阶段</td>
</tr>
<tr>
<td>FFM</td>
<td>在FM模型的基础上，加入“特征域”的概念，使每个特征在与不同域的特征交叉时采用不同的隐向量</td>
<td>相比FM，进一步加强了特征交叉的能力</td>
<td>模型的训练开销达到了$0(n^2)$</td>
</tr>
<tr>
<td>GBDT+LR</td>
<td>利用GBDT进行“自动化”的特征组合，将原始特征向量转化成离散型特征向量，并输入逻辑回归模型，进行最终的CTR预估</td>
<td>特征工程模型化，使模型具备了更高阶特征组合的能力</td>
<td>GBDT无法进行完全并行的训练，更新所需的训练时长较长</td>
</tr>
<tr>
<td>LS-PLM</td>
<td>首先对样本进行“分片”，在每个“分片”内部构建逻辑回归模型，将每个样本的各“分片”概率与逻辑回归的得分进行加权平均，得到最终的预估值</td>
<td>模型结构类似三层神经网络，具备了较强的表达能力</td>
<td>模型结构相比深度学习模型仍较为简单，有进一步提高的空间</td>
</tr>
</tbody>
</table>
</div>
<hr>
<ul>
<li>[x] 《深度学习推荐系统第二章》</li>
</ul>
]]></content>
      <categories>
        <category>《深度学习推荐系统》</category>
      </categories>
      <tags>
        <tag>recommendation</tag>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title>Python</title>
    <url>/2020/09/02/Python/</url>
    <content><![CDATA[<h2>Python</h2>

<h4>一. 数据结构</h4>

<ul>
<li>python中的<code>list</code>、<code>tuple</code>、<code>dict</code>、<code>set</code>   </li>
<li><b>list    </b><ol>
<li>构造<code>list</code>，使用<font color='red'><code>[]</code></font>把<code>list</code>的所有元素都括起来就是一个列表，用变量<code>L</code>表示。其中的元素<font color='red'>不要求是同一种数据类型</font>。  </li>
<li>列表中元素是<font color='red'>有序</font>的。可以通过索引来访问。</li>
<li>访问<code>list</code>的元素，<code>L[0]</code>：表示列表中的第一个元素，<code>L[-1]</code>：表示列表中的最后一个元素。<code>L[起始索引:终止索引]</code>：表示访问列表中的多个元素，包含头，不包含尾。</li>
<li>列表是<font color='red'>可以被修改的</font>，包括添加元素，删除元素，替换元素。<ul>
<li>添加元素：<code>L.append(元素)</code>、<code>L.insert(元素插入到列表中的位置，元素)</code> </li>
<li>删除元素：<code>L.pop()&lt;==&gt;L.pop(-1)</code>，<code>L.pop(要删除元素在列表中的位置)</code>、<code>L.remove(要删除元素)</code>，删除列表中第一次匹配到的元素</li>
<li>替换元素：<code>L[要替换元素在列表中的位置]=新的元素值</code>  </li>
</ul>
</li>
<li>计算列表的长度：<code>len(L)</code></li>
<li>计算列表中某一个元素在列表中出现的次数：<code>L.count(要统计的元素)</code></li>
<li>列表拼接，用<code>+</code>，<code>L1=[1,2],L2=[3,4],L1+L2=[1,2,3,4]</code></li>
<li>列表中元素复制，用<em>，`L= [2,3],L </em> 3 = [2,3,2,3,2,3] `</li>
</ol>
</li>
</ul>
<ul>
<li><p><b>tuple   </b></p>
<ol>
<li>构造<code>tuple</code>，使用<font color='red'><code>()</code></font>把<code>tuple</code>的所有元素都括起来就是一个元组，用变量<code>T</code>表示。python规定只有单个元素的元组应表示成<code>（元素，）</code>，避   免歧义。其中的元素<font color='red'>不要求是同一种数据类型</font>。   </li>
<li>元组中元素是<font color='red'>有序</font>的。可以通过索引访问。</li>
<li>访问<code>tuple</code>的元素，<code>T[0]</code>：表示元组中的第一个元素，<code>T[-1]</code>：表示元组中的最后一个元素。   </li>
<li>元组是不可以被修改的，因此它没有<code>append()</code>、<code>insert()</code>、<code>pop()</code>等方法。   注意：元组中可以包含列表，如<code>T= （1，2，[4，5]）</code>，  <code>T[2] = [ ]</code>,错误  <code>T[2] [0] =5</code>,正确   </li>
</ol>
</li>
<li><p><b>dict   </b></p>
<ol>
<li>构造<code>dict</code>，使用<font color='red'><code>{}</code></font>把<code>dict</code>所有元素都括起来就是一个字典，用<code>D</code>表示。<code>D</code>中元素的形式是<code>key：value</code>的形式。其中的<code>key</code>或者<code>value</code>都<font color='red'>不要求是同一种数据类型</font>。   </li>
<li>字典中元素是<font color='red'>无序的。不可以通过索引访问</font>。</li>
<li>访问<code>dict</code>的元素，使用<code>D[key]</code>来查找对应的value。如果key不存在，则会报错。为避免报错：  <ul>
<li>访问前先判断 <code>if key in D：print(D[key] )</code></li>
<li>使用get()方法<code>print(D.get[key])</code>，key不存在会输出None</li>
</ul>
</li>
<li>字典是可以被修改的。<code>D[key] = 新的value</code>，如果key不存在，则会想字典中添加这一键值对。  注意：  字典的一个最明显的特点是查找速度快，无论字典中有多少个数据，查找的速度都是一样的，因为它是按照key来查找的。但是它占用的内存大。典型的以空间换时间的思想。  </li>
</ol>
</li>
<li><p><b>set</b></p>
<ol>
<li>构建<code>set</code>，<code>S = set(传入一个列表)</code>，<code>或者S = {元素1，元素2，....}</code>。其中的元素<font color='red'>不要求是同一种数据类型</font>。如：  <code>S= set（[1,2,3,3,4,5]）  print(S) #{1,2,3,4,5}</code>  注意：创建空集合只能使用<code>S = set()</code>，而不能使用<code>S = { }</code>，这是用来创建空字典的。  </li>
<li>集合中元素是<font color='red'>无序的。不可以通过索引来访问元素</font>。  </li>
<li>集合是可以被修改的。   </li>
<li>添加元素和删除元素。 <ul>
<li><code>S.add(元素)</code>。</li>
<li><code>S.remove(元素)</code>。删除之前要进行判断：<code>if 元素 in S： S.remove(元素)</code>，否则会报错。</li>
</ul>
</li>
</ol>
</li>
</ul>
<h4>二.open方法打开文件</h4>

<ol>
<li><code>b</code> 二进制模式   </li>
<li><code>r</code> 只读，指针将会放在文件的开头    </li>
<li><code>rb</code> 二进制只读，指针将会放在文件的开头   </li>
<li><code>r+</code> 读写，指针将会放在文件的开头    </li>
<li><code>rb+</code> 二进制读写，指针将会放在文件的开头    </li>
<li><code>w</code> 写入， 如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件   </li>
<li><code>wb</code> 二进制写入，如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。一般用于非文本文件如图片等    </li>
<li><code>w+</code> 读写，如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。 </li>
<li><code>wb+</code> 二进制读写，如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。一般用于非文本文件如图片等</li>
<li><code>a</code> 追加，如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入   </li>
<li><code>ab</code> 二进制追加，如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入    </li>
<li><code>a+</code> 读写，如果该文件已存在，文件指针将会放在文件的结尾。文件打开时会是追加模式。如果该文件不存在，创建新文件用于读写。    </li>
<li><code>ab+</code> 二进制读写，如果该文件已存在，文件指针将会放在文件的结尾。如果该文件不存在，创建新文件用于读写。</li>
</ol>
<h4>三.标识符</h4>

<ol>
<li>第一个字符必须是字母表中字母或下划线 <code>_</code> 。</li>
<li>标识符的其他的部分由字母、数字和下划线组成。</li>
<li>标识符对大小写敏感。</li>
<li>不可以是python中的关键字，如<code>False</code>、<code>True</code>、<code>None</code>、<code>class</code>等。   </li>
<li>注意：<code>self</code>不是python中的关键字。类中参数self也可以用其他名称命名，但是为了规范和便于读者理解，推荐使用<code>self</code>。</li>
</ol>
<h4>四.原始字符串标识符r</h4>

<p>Python 中字符串的前导 <code>r</code> 代表原始字符串标识符，该字符串中的特殊符号不会被转义，适用于正则表达式中繁杂的特殊符号表示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">"\\n"</span>)</span><br><span class="line">print(<span class="string">r"\n"</span>)</span><br></pre></td></tr></table></figure>
<p>注意前导标识符 <code>r</code>不会被输出，只起标记作用。</p>
<h4>五.复数</h4>

<ol>
<li>虚数不能单独存在，它们总是和一个值为<code>0.0</code>的实数部分一起来构成一个复数。</li>
<li>复数由实数部分和虚数部分构成</li>
<li>表示虚数的语法： <code>real+imagj</code></li>
<li>实数部分和虚数部分都是浮点数</li>
<li>虚数部分必须有后缀 <code>j</code> 或 <code>J</code></li>
<li><font color='red'>python2和python3都不支持复数比较大小。</font>

</li>
</ol>
<h4>六.拷贝、赋值</h4>

<p>以一个例子说明：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line">a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,[<span class="string">'a'</span>,<span class="string">'b'</span>]]</span><br><span class="line">b = a</span><br><span class="line">c = copy.copy(a)</span><br><span class="line">d = copy.deepcopy(a)</span><br><span class="line">a.append(<span class="number">5</span>)</span><br><span class="line">a[<span class="number">4</span>].append(<span class="string">'c'</span>)</span><br></pre></td></tr></table></figure>
<p>用图例说明下<code>a</code>这个<code>list</code>在电脑里实际的存储情况：</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/08/24/GBmb29YSzDfNWau.jpg" alt="IMG_C2E327060C3C-1.jpeg" style="zoom: 67%;" /></p>
<p>首先看看<code>b</code>的情况，<code>b</code>实际上和<code>a</code>指向的是同一个值，就好比人的大名和小名，只是叫法不同，但还是同一个人。</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/08/24/XOSsrRLUaEPkoAz.jpg" alt="IMG_4800E953B5BB-1.jpeg" style="zoom: 67%;" /></p>
<p>接下来再看看<code>c</code>的情况，<code>c</code>的情况和<code>a.copy()</code>的情况是一样的，都是所谓的浅拷贝（浅复制），浅拷贝只会拷贝父对象，不会拷贝子对象，通俗的说就是只会拷贝到第二层。</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/08/24/f9eTgXdcsnFNZuW.jpg" alt="IMG_71969EEBDF15-1.jpeg" style="zoom:67%;" /></p>
<p>若父对象发生变化，<code>c</code>不会变化，因为它已经复制的所有父对象，假如子对象发生变化则<code>c</code>会变，比如<code>c[4]</code>和<code>a[4]</code>实际都是一个变量<code>list</code>，他们都指向子对象，若子对象发生变化，他们必然都变化，比如变成<code>[&quot;a&quot;,&quot;d]</code>，那它们指向的值就会变成<code>a</code>，<code>d</code>。</p>
<p><img src= "img/loading.gif" data-src="https://i.loli.net/2020/08/24/5sZJ9SCFjGANgDw.jpg" alt="IMG_53A30C2E2E9B-1.jpeg" style="zoom:67%;" /></p>
<p>再看看<code>d</code>的情况，这就是我们所说的深复制，不管<code>a</code>进行什么操作，都不会改变<code>d</code>了，他们已经指向不同的值。</p>
<h4>七.逻辑运算符</h4>

<ul>
<li><b><code>and</code>：</b><code>x and y</code>，<font color='red'>当表达式中所有值都为真，Python会选择第二个值作为结果</font>，有值为<code>False</code>时就返回<code>False</code>；</li>
<li><b><code>or</code>：</b><font color='red'>当表达式所有值都为真，Python会选择第一个值作为结果。</font>简单的记法就是看第一个值，第一个值为真，就返回第一个值，如果为假，再看第二个值。</li>
</ul>
<h4>八.函数返回值</h4>

<p>Python没有<code>Null</code>，return没有返回值时，自动返回<code>None</code>。</p>
<h4>九.命令</h4>

<p>python中主要存在四种命名方式：       </p>
<ol>
<li><code>object</code>  #公用方法     </li>
<li><code>_object</code>  #半保护，#被看作是“protect”，意思是只有类对象和子类对象自己能访问到这些变量，                      在模块或类外不可以使用，不能用<code>from module import *</code>导入。 </li>
<li><code>_ _ object</code>  #全私有，全保护。<code>_ _ object</code> 是为了避免与子类的方法名称冲突， 对于该标识符描述的方法，父类的方法不能轻易地被子类的方法覆盖，他们的名字实际上是   <code>_ _ classname _ _methodname</code>。私有成员“private”，意思是只有类对象自己能访问，连子类对象也不能访                              问到这个数据，不能用<code>from module import *</code>导入。     </li>
<li><code>_ _ object_ _</code>   #内建方法，用户不要这样定义</li>
</ol>
]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2019/11/12/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
  </entry>
</search>
