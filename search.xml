<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>HIN-Graph</title>
    <url>/2020/04/03/HIN-Graph/</url>
    <content><![CDATA[<blockquote>
<blockquote>
<p>Summarying the recent works on <strong>heterogeneous graph neural networks</strong></p>
</blockquote>
</blockquote>
<h2>Links</h2>

<p>清华朱文武老师组关于图上的深度学习综述(2018) <a href="https://arxiv.org/abs/1812.04202v1" target="_blank" rel="noopener" title="With a Title">Deep Learning on Graphs: A Survey</a>. </p>
<p>IEEE Fellow关于图神经网络综述(2019) <a href="https://arxiv.org/abs/1901.00596" target="_blank" rel="noopener" title="With a Title">A Comprehensive Survey on Graph Neural Networks</a>. </p>
<p>Ziniu Hu关于异质图Transformer的研究(2020) <a href="https://arxiv.org/abs/2003.01332" target="_blank" rel="noopener" title="With a Title">Heterogeneous Graph Transformer</a>. </p>
<p>纪厚业做的分享： <a href="https://www.bilibili.com/video/BV1HE41157GD?t=6492" target="_blank" rel="noopener" title="With a Title">异质图神经网络：模型和应用</a>. 介绍了异质图网络、以及一些模型和三个实际落地的论文</p>
<p>发表在数据库顶会VLDB上的最新异质图表示学习的综述(2020) <a href="https://arxiv.org/abs/1801.05852" target="_blank" rel="noopener" title="With a Title">Heterogeneous Network Representation Learning: Survey, Benchmark, Evaluation, and Beyond</a>.</p>
<p>图数据上的对抗攻击和防御综述(2020) <a href="https://arxiv.org/abs/1812.10528v1" target="_blank" rel="noopener" title="With a Title">Adversarial Attack and Defense on Graph Data: A Survey</a>. </p>
<p>用于构建动态图的例子《Dynamic Network Embedding by Modelling Triadic Closure Process(2018)》 <a href="https://github.com/luckiezhou/DynamicTriad" target="_blank" rel="noopener" title="With a Title">https://github.com/luckiezhou/DynamicTriad</a>. </p>
<hr>
<p>主流知识图谱表示学习算法:</p>
<ul>
<li>TransE、ComplEx、DistMult、TransR、RESCAL、RotatE </li>
</ul>
]]></content>
      <tags>
        <tag>HIN</tag>
        <tag>graph-learning</tag>
        <tag>GNN/GCN</tag>
      </tags>
  </entry>
  <entry>
    <title>Multimodal Machine Learning:A Survey and Taxonomy</title>
    <url>/2019/11/23/Multimodal-Machine-Learning-A-Survey-and-Taxonomy/</url>
    <content><![CDATA[<blockquote>
<blockquote>
<p>This article is a survey for multimodal machine learning. This paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. They go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: <strong>representation, translation, alignment, fusion, and co-learning.</strong></p>
</blockquote>
</blockquote>
<blockquote>
<p>Index Terms:<strong>Multimodal, machine learning, introductory, survey</strong></p>
</blockquote>
]]></content>
      <tags>
        <tag>Muilimodal</tag>
        <tag>machine learning</tag>
        <tag>survey</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo+Github搭建简单博客总结</title>
    <url>/2019/11/23/Hexo+Github%E6%90%AD%E5%BB%BA%E7%AE%80%E6%98%93%E5%8D%9A%E5%AE%A2%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h1 id="Hexo-Github搭建简易博客总结"><a href="#Hexo-Github搭建简易博客总结" class="headerlink" title="Hexo+Github搭建简易博客总结"></a>Hexo+Github搭建简易博客总结</h1><p><a href="https://www.zhihu.com/question/39183612" target="_blank" rel="noopener">参考知乎问题: 如何使用10个小时搭建出个人域名而又Geek的独立博客？</a></p>
<p>前言：</p>
<p>一直没有尝试自己搭建一个博客，之前本来想着自己前后端搭建，但是懒惰。。。。。hexo+github是一个傻瓜式的自助搭建博客方式，很简便，如果是稍微有点代码基础的，应该都能在几个小时之内熟悉搭建整个过程。</p>
<p>软件准备：<a href="https://nodejs.org/en/" target="_blank" rel="noopener">Node.js</a>、<a href="https://git-scm.com/downloads" target="_blank" rel="noopener">Git</a></p>
<ol>
<li><p>先安装Hexo: <code>$ npm install -g hexo-cli</code></p>
</li>
<li><p>再初始化Hexo:<code>$ npm install</code></p>
<p>Hexo的大体文件：</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">themes 存放主题的文件夹+ source 博客资源文件夹+ source/_drafts 草稿文件夹+ source/_posts 文章文件夹+ themes/landscape 默认皮肤文件夹+ themes 存放主题的文件夹+ source 博客资源文件夹+ source/_drafts 草稿文件夹+ source/_posts 文章文件夹+ themes/landscape 默认皮肤文件夹</span><br></pre></td></tr></table></figure>

<ul>
<li>_config.yml 全局配置文件。要注意的是，该文件格式要求极为严格，缺少一个空格都会导致运行错误。小提示：不要用Tab缩进，两个空格符， <strong>冒号：后面只用一个空格即可*</strong>。</li>
</ul>
<ol>
<li><p>我采用的还是最常用的<a href="https://hexo.io/themes/" target="_blank" rel="noopener">Hexo|Next主题</a>，修改了一连串配置，包括侧栏等等</p>
</li>
<li><p>Hexo部署</p>
<p>到这里，就可以启动本地服务器打开了：<code>$ hexo s</code></p>
<p>在localhost:4000端口就可以查看到效果。</p>
<p>在git上创建了一个和本地的文件夹名称一致，我都是创见的daluzi.github.io，然后把本地的文件夹朴实到远程仓库里面。</p>
<p>接下来，在站点的config.yml文件中，部署如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">deploy:  	 type: git 部署类型若有问题，其他类型自行google之  	 repository: https://github.com/daluzi/daluzi.github.io.git  	 branch: master  	 plugins: -hexo-generator-feed</span><br></pre></td></tr></table></figure>

<p>然后</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ hexo g #生成静态网页# hexo d #开始部署</span><br></pre></td></tr></table></figure>

<p>好了，在浏览器输入daluzi.github.io就能查看博客了，接下来就是挂载域名。</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>Meta-Graph Based Recommendation Fusion over Heterogeneous Information Networks</title>
    <url>/2019/12/09/Meta-Graph-Based-Recommendation-Fusion-over-Heterogeneous-Information-Networks/</url>
    <content><![CDATA[<html><head></head><body><blockquote>
<blockquote>
<p>This article is paper about recommendation over heterogeneous information networks. Because HIN faces two problems: 1, how to represent the high-level semantics of recommendations and 2, how to fuse the heterogeneous information to make recommendations, the author first introduced the concept of <strong>meta-graph</strong> to HIN-based recommendation, and then solved the information fusion problem with a <strong>“matrix factorization(MF) + factorization machine(FM)”</strong> approach.</p>
<p>I think the author made two innovations:</p>
<p>1, use meta-graph and commute matrix to construct similarity matrix;</p>
<p>2, for each meta-graph, he calculate user-item’s similarity matrix, then factorize users and items’ feature vectors by using matrix factorization (MF). So he can obtain many different users and items vectors of many different meta-graph. for all of then, he use factorization machine(FM) to component then.</p>
<p>A user-item score prediction problem.</p>
</blockquote>
</blockquote>
<p><strong>KEYWORDS:  Recommendation system; Collaborative filtering; Heterogeneous information networks; Factorization machine.</strong></p>
<h2>Contents</h2>
> * [INTRODUCTION](#1)
> * [FRAMEWORK](#2)
>   * [Meta-graph based Similarity](#2.1)
>   * [Meta-graph based Latent Features](#2.2)
>   * [Recommendation Model](2.3)
>   * [Comparison with Previous Latent Feature based Model](2.4)
> * [MODEL OPTIMIZATION](#3)
>   * [Optimization](#3.1)
>   * [Complexity Analysis](3.2)
> * [EXPERIMENTS](4)
>   * [Datasets](4.1)
>   * [Evaluation Metric](#4.2)
>   * [Baseline Models](#4.3)



<h2 id="1">INTRODUCTION</h2>
​        Heterogeneous information networks(**HINs**) have been proposed as a general data representation for many different types of data. 

<p>​        At the beginning, HINs were used to handle entity search and similarity measure problems. Later, it was extended to handle heterogeneous entity recommendation problems. Then, the semantic relatedness constrained by the entity types can be defined by the similarities between two entities along meta-graph.</p>
<p>​        For traditional collaborative filtering, we always build a simple meta-path <em>Business –> User</em> and learn from this meta-path to make generalization. From HIN’s schema, we can build more complicated meta-paths like <em>User–>Review–>Word–>Review–>Business</em>. </p>
<p>​        There are two major challenges when applying meta-path based similarities to recommender systems: <strong>First</strong>, meta-path may not be the best way to characterize the rich semantics. So, in this paper, they used <strong>meta-graph(or meta-structure)</strong> to compute similarity between homogeneous type of entities over HINs. <strong>Second</strong>, different meta-paths or meta-graphs result in different similarities. How to assemble them in an effective way is another challenge.</p>
<p>​        So, in this paper, in order to solve the above two problems, author first used <strong>the concept of meta-graph</strong> and then applied <strong>matrix factorization(MF) + factorization machine(FM)</strong>.</p>
<pre><code>* for each meta-graph, compute the user-item similarity matrix
* Use unsupervised MF to factorize it into a set of user and item latent vectors.
* Use FM to assemble them to learn from the rating matrix. </code></pre><h2 id="2">FRAMEWORK</h2>
<h3 id="2.1">Meta-graph based Similarity</h3>
​        The definition of Meta-graph:

<p><img alt="meta-graph.png" data-src="https://i.loli.net/2019/12/09/VePI7wiHLXqt8fQ.jpg" class="lazyload"></p>
<p>​        All of the meta-graphs on Amazon and Yelp:</p>
<p><img alt="all of the meta-graphs for Amazon and Yelp" data-src="https://i.loli.net/2019/12/09/wjOSsENmKGR2yY9.jpg" class="lazyload"></p>
<p>​        The author used <strong>commuting matrices</strong> to compute the counting based similarity matrix of a meta-path. For example: if we have a meta-path:<img alt="meta-path.png" data-src="https://i.loli.net/2019/12/09/Pkmpr4n9MUfvV5s.jpg" class="lazyload"></p>
<p>And, we define a matrix $W_{A_iA_j}$ as the adjacency matrix between type $A_i$ and $A_j$. So the commuting matrix for path p is:<img alt="ABF68C6E-64CD-469A-BC2E-7D1B19ED3D31.png" data-src="https://i.loli.net/2019/12/09/esYuT4iBoKzdtrN.jpg" class="lazyload"></p>
<p><strong>Algorithm 1 Computing commuting matrix for CM9 :</strong></p>
<p><img alt="B07D1FB3-2139-4357-A000-0EA870DA1635.png" data-src="https://i.loli.net/2019/12/09/Yshg3R91MzpqZ48.jpg" class="lazyload"></p>
<h3 id="2.2">Meta-graph based Latent Features</h3>
​        By use of matrix factorization method, we can obtain user and item's low-rank matrices:![low-rank.png](https://i.loli.net/2019/12/09/z4JjWdDx5TnMNf3.jpg)

<p><img alt="702CE1AE-D96F-4C88-BA2E-B80D483F536C.png" data-src="https://i.loli.net/2019/12/09/pmxiLPM6atBTrq8.jpg" class="lazyload"></p>
<p>Which ${\lambda_u},{\lambda_b}$ are the hyper-parameters that control the influence of Frobenius norm regularization to aviod overfitting.</p>
<h3 id="2.3">Recommendation Model</h3>
​        For the L groups of user and item latent features and F ranks which used to factorize every similarity matrix, we concatenate all of the corresponding user and item features form all of the L meta-graphs:![EADE9E6C-D305-4B2C-BF96-033AC1A9BAF7.png](https://i.loli.net/2019/12/09/abOz2gnh9PDwklq.jpg)

<p>​        So the rating for the sample $x^n$ based on FM is computed as follows:</p>
<p><img alt="99A1983D-A650-43E0-95B0-C2D2E7826EA3.png" data-src="https://i.loli.net/2019/12/09/5oRL7Dfn9Sl1vGt.jpg" class="lazyload"></p>
<p>Where $w_0$ is the global bias representing the first-order weights for the features, and $V = [v_i]$ represents the second-order weights to model the interactions across different features. $<.,.>$ is the dot product of two vectors of size K.</p>
<p>​        The parameters can be learned by minimizing the mean square loss:<img alt="mean square loss.png" data-src="https://i.loli.net/2019/12/11/k63WYeTtjAdoalO.jpg" class="lazyload"></p>
<p>Where $y^n$ is an observed rating for the n-th sample, N is the number of all the observed ratings.</p>
<p><strong>There are two problems when applying the FM model to the meta-graph based latent features:</strong></p>
<ul>
<li>It may bring noise when working with many meta-graphs thus impairing the predicting capability of the model. In practice, some meta-graphs can be useless since information provided by some meta-paths can be covered by others.</li>
<li>Computational cost.</li>
</ul>
<p>​        To alleviate the above two problems, we propose a novel regularization for FM,i.e., the group lasso regularization, which is a feature selection method on a group of variables. The group lasso regularization of parameters p is defined as follows:<img alt="6F205C97-3FB4-4FE5-B2D8-9A5A8E0B7ECA.png" data-src="https://i.loli.net/2019/12/09/THbdrAQpqiRCLlI.jpg" class="lazyload"></p>
<h3 id="2.4">Comparison with Previous Latent Feature based Model</h3>
​        The previous approaches of recommendation based on HIN is not adequate, as it fails to capture the interactions between inter-meta-graph features, i.e. features across different meta-graphs, as well as the intra-meta-graph features, i.e. It may decrease the prediction ability of all of the user and item features.



<h2 id="3">MODEL OPTIMIZATION</h2>
​        The author define FM over meta-graph(FMG) model with the following objective function:

<p><img alt="1CB932AA-8575-4885-82D5-3190F9CA0128.png" data-src="https://i.loli.net/2019/12/09/FYwu3NTrWsHG5hy.jpg" class="lazyload"></p>
<p>​        We can see now the object function is non-convex and non-smooth. So author used <strong>proximal gradient (nmAPG) algorithm</strong>.</p>
<h2 id="4"> EXPERIMENTS</h2>
<h3 id="4.1">Datasets</h3>
* Yelp (https://www.yelp.com/dataset_challenge)
* Amazon Electronics (http://jmcauley.ucsd.edu/data/amazon/)

<h3 id="4.2">Evaluation Metric</h3>
​        RMSE

<h3 id="4.3">Baseline Models</h3>
* RegSVD: RegSVD is the basic matrix factorization with L2 regularization, which uses only the user-item rating matrix. (https://www.librec.net/)
* FMR: FMR is the factorization machine with only the user-item rating matrix.(http://www.libfm.org/)
* HeteRec: HeteRec method is based on meta-path based similarity between users and items.
* SemRec: SemRec is a meta-path based recommendation on weighted HIN, which is built by connecting users and items with the same ratings.(https://github.com/zzqsmall/SemRec)

</body></html>]]></content>
      <tags>
        <tag>Recommendation</tag>
        <tag>Meta-graph</tag>
        <tag>Heterogeneous Information Networks</tag>
      </tags>
  </entry>
  <entry>
    <title>Personalized Recommendation Combining User Interest and Social Circle</title>
    <url>/2019/11/13/Personalized-Recommendation-Combining-User-Interest-and-Social-Circle/</url>
    <content><![CDATA[<blockquote>
<blockquote>
<p>Interpersonal influence and interest based on circles of friends can be used to solve cold start and sparsity problem of datasets.This paper used three social factors:<strong>personal interest</strong>,<strong>interpersonal interest similarity</strong> and <strong>interpersonal influence</strong> to make personalized recommendation.</p>
</blockquote>
</blockquote>
<h1 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h1><blockquote>
<ul>
<li><a href="#1">Introduction</a></li>
<li><a href="#2">Problem formulation</a></li>
<li><a href="#3">Related work</a><ul>
<li><a href="3.1">Basic matrix factorization</a></li>
<li><a href="3.2">CircleCon model</a></li>
<li><a href="3.3">ContextMF model</a></li>
</ul>
</li>
<li><a href="4">The approach</a><ul>
<li><a href="4.1">User interest factor</a></li>
<li><a href="4.2">Personalized recommendation model</a></li>
<li><a href="4.3">Model training</a></li>
</ul>
</li>
<li><a href="5">Experiments</a><ul>
<li><a href="5.1">Datasets</a></li>
<li><a href="5.2">Performance measures</a></li>
<li><a href="5.3">Evaluation</a></li>
</ul>
</li>
</ul>
</blockquote>
<h2 id='1'>Introduction</h2>
Recommender system(RS) has been successfully exploited to solve information overload.Many methods proposed to improve the performance of RS.And there are two main problems needed to solve:**1.the problems of cold start for users(new users into the RS with little historical behavior)2.the sparsity of datasets(the proportion of rated user-item pairs in all the user-item pairs of RS)**.

<p>In this paper, three social factors,<strong>personal interest,interpersonal interest similarity,and interpersonal influence</strong>, fuse into a unified personalized recommendation model based on probabilistic matrix factorization.</p>
<h2 id='2'>Problem formulation</h2>
]]></content>
      <tags>
        <tag>RS</tag>
      </tags>
  </entry>
  <entry>
    <title>The FacT: Taming Latent Factor Models for Explainability with Factorization Trees</title>
    <url>/2020/05/23/The-FacT-Taming-Latent-Factor-Models-for-Explainability-with-Factorization-Trees/</url>
    <content><![CDATA[]]></content>
      <tags>
        <tag>explainability</tag>
        <tag>regression tree</tag>
        <tag>recommendation</tag>
      </tags>
  </entry>
  <entry>
    <title>Semi-supervised Classification with Graph Convolutional Networks</title>
    <url>/2019/11/14/Semi-supervised-Classification-with-Graph-Convolutional-Networks/</url>
    <content><![CDATA[<html><head></head><body><blockquote>
<blockquote>
<p>The convolution is extended to the data of the graph structure.Motivate the choice of our convolutional architecture via a localized first-order approximation of <strong>spectral graph convolutions</strong>.The model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes.</p>
<p>In a number of experiments on <strong>citation networks</strong> and on a <strong>knowledge graph dataset</strong>,This model works well in semi-supervised tasks.</p>
</blockquote>
</blockquote>
<h1 id="Contents"><a href="#Contents" class="headerlink" title="Contents"></a>Contents</h1><blockquote>
<ul>
<li><a href="#1">The main content of the article</a><ul>
<li><a href="#1.1">abstract</a></li>
<li><a href="#1.2">spectral graph convolutions</a></li>
<li><a href="#1.3">layer-wise linear model</a></li>
<li><a href="#1.4">semi-supervised node classifacation</a></li>
</ul>
</li>
<li><a href="#2">My learing understanding of semi-GCN</a></li>
</ul>
</blockquote>
<h2 id="1">The main content of the article</h2>

<h3 id="1.1">abstract</h3>

<p>The authors motivate the choice of their convolutional architecture via a localized first-order approximation of <strong>spectral graph convolutions</strong>.</p>
<p>By training the structural model of convolution neural network with some tagged node data in the graph structural data ,the network model can further classify the remaining untagged data.</p>
<p>So,on the whole , the way which the convolution is extended to the data of the graph structural ,can get a better data representation ,and it works well in semi-supervised tasks.</p>
<blockquote>
<p>Datasets: citation network datasets(Citeseer ,Cora ,Pubmed) ,bipartite graph dataset(NELL) </p>
</blockquote>
<h3 id="1.2">spectral graph convolutions</h3>

<p><em>In this period ,I mainly refer to this website:<a href="https://blog.csdn.net/qq_41727666/article/details/84622965" target="_blank" rel="noopener">https://blog.csdn.net/qq_41727666/article/details/84622965</a></em></p>
<p>In fact ,there are two versions of GCN:</p>
<p>1.The first generation of GCN is this formula:<strong>$$g{\theta} * x = Ug{\theta}U^Tx$$</strong>.</p>
<p>In the formula ,$x$ is the eigenvector of the graph node ,$g{\theta}=diag(\theta)$ is the convolution kernel.$U$ is the matrix of eigenvectors of the normalized graph Laplacian $L$. And the $L = I_N - D^{-1/2}AD^{-1/2} = U{\Lambda}U^T$</p>
<p><strong>But ,this is too complicated to calculate</strong></p>
<p>2.To circumvent this problem ,$g{\theta}(\Lambda)$ can be well-approximated by a truncated expansion in terms of Chebyshev polynomial $T_k(x)$ up to $K^{th}$ order:</p>
<p>$$g{\theta}^<code>{\approx} {\sum_{k=0}^{K}{\theta}^</code>T_k(\Lambda^1)}$$</p>
<p>With a rescaled $\Lambda^1 = {\frac{2}{\lambda_{max}}}\Lambda - I_N$. $\lambda_{max}$ Denotes the largest eigenvalue of $L$. $\theta^`$ is now a vector of Chebyshev coefficients. The Chebyshev polynomials are recursively defined as $T_k(x) = 2xT_{k-1}(x) - T_{k-2}(x)$, with $T_0(x) = 1$ and $T_1(x) = x$.</p>
<p>So, now the formula is:</p>
<p>$$g{\theta}^<code>* x \approx {\sum_{k=0}^{K}{\theta}_k^</code>T_k(L^1)x}$$</p>
<p>With $L^1 = {\frac{2}{\lambda_{max}}L - I_N}$ .This expression is now K-localized since it is a <strong>$K^{th}$-order ploynomial in the Laplacian</strong>.</p>
<h3 id="1.3">layer-wise linear model</h3>

<p>The authors approximate $\lambda_{max} \approx 2$, so now the formula is simplified to:</p>
<p><img alt="semi_eq5.png" data-src="https://i.loli.net/2019/11/17/LdqIwO9NfxzXWJv.png" class="lazyload"></p>
<h3 id="1.4">semi-supervised node classification</h3>

<p>In this example, they consider a two-layer GCN for semi-supervised node classification on a graph with a symmetric adjacency matrix A(binary or weighted). </p>
<p>First, calculate $A^` = D1^{-1/2}A^1D1^{-1/2}$.</p>
<p>And the forward model then takes the simple form:$$Z = f(X,A) = softmax(A^<code>{ReLU}(A^</code>XW^{(0)})W^{(1)})$$</p>
<p>Here, $W^{(0)}$ is an input-to-hidden weight matrix for a hidden layer with H feature maps.</p>
<p>$W^{(1)}$ is a hidden-to-output weight matrix.</p>
<p>The softmax activation function defined as $softmax(x_i) = \frac{1}{\sum_i{exp(x_i)}}exp(x_i)$</p>
<p>For semi-supervised multiclass classification, they then evaluate the cross-entropy error over all labeled examples :$\xi = -\sum_{l\epsilon{y_l}}\sum_{f=1}^{F}Y_{lf}lnZ_{lf}$</p>
<p>The author perform <strong>batch gradient descent</strong>. Stochasticity in the training process is in produced via <strong>dropout</strong>. </p>
<h2 id="2">My learing understanding of semi-GCN</h2>

<p><em>mainly depend on <a href="https://zhuanlan.zhihu.com/p/58178060" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/58178060</a></em></p>
</body></html>]]></content>
      <tags>
        <tag>GCN</tag>
        <tag>Deeplearning</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2019/11/12/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>学习资源汇总</title>
    <url>/2020/04/17/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB/</url>
    <content><![CDATA[<blockquote>
<blockquote>
<p>Some links about graph network.</p>
<p>Learning resources about GNN \ graph embedding \ LSTM etc..</p>
</blockquote>
</blockquote>
<ol>
<li><a href="https://mp.weixin.qq.com/s/J6GUf-pAXmI9jhFGyVyU8w" target="_blank" rel="noopener">图系列|三篇图层次化表示学习(Hierarchical GNN)：图分类以及节点分类</a></li>
<li><a href="https://mp.weixin.qq.com/s/A64paLV_3Arhu1LKvKc63w" target="_blank" rel="noopener">PinSAGE | GCN 在工业级推荐系统中的应用</a><br>《Graph Convolutional Neural Networks for Web-Scale Recommender Systems》2018.</li>
<li><a href="https://mp.weixin.qq.com/s/ewzsURiU7bfG3gObzIP2Mw" target="_blank" rel="noopener">深度长文：图神经网络欺诈检测方法总结</a></li>
<li><a href="https://mp.weixin.qq.com/s/Yj_yP6LjxrG_UJX9W3cAHQ" target="_blank" rel="noopener">腾讯 at IJCAI 2020，基于内部-环境注意力网络的推荐多队列冷启动召回</a></li>
<li><a href="https://mp.weixin.qq.com/s/rjWOkwzX3IE59Kc9P9leAQ" target="_blank" rel="noopener">格“物”致知：多模态预训练再次入门</a></li>
<li><a href="https://mp.weixin.qq.com/s/FtUxxRWkUJ7346gWAJqQDQ" target="_blank" rel="noopener">【KDD2020-MSU】图结构学习的鲁棒图神经网络，克服对抗攻击提升GNN防御能力</a></li>
<li><a href="https://mp.weixin.qq.com/s/6OMMboBdbLVA-HsEjA3bSA" target="_blank" rel="noopener">Ctr 预估之 Calibration</a></li>
<li><a href="https://mp.weixin.qq.com/s/aNx9or0-eHHT1XI1BJyB7w" target="_blank" rel="noopener">综述|73页近百篇参考文献JMLR20动态图上的表示学习</a></li>
<li><a href="https://mp.weixin.qq.com/s/lbRymEKPcCrLsk9r9w1tlQ" target="_blank" rel="noopener">高性能涨点的动态卷积 DyNet 与 CondConv、DynamicConv 有什么区别联系？</a></li>
<li><a href="https://mp.weixin.qq.com/s/7_cZhszsnfBKyoUKYMkQhw" target="_blank" rel="noopener">近期必读的五篇计算机视觉顶会CVPR 2020【图神经网络 (GNN) 】相关论文-Part 3</a></li>
<li><a href="https://mp.weixin.qq.com/s/KHbCbzP1JMgoYMjY5cMOmA" target="_blank" rel="noopener">华为诺亚实验室开源Disout算法，直接对标谷歌申请专利的Dropout算法</a></li>
<li><a href="https://mp.weixin.qq.com/s/n3CSdTdsk5erpwRpZE9Qvw" target="_blank" rel="noopener">图专题|IJCAI2020两篇多层次/多视角相关的图神经网络GNN研究论文</a></li>
<li><a href="https://mp.weixin.qq.com/s/_VBO_9yiZ0qngq4yfwcxEg" target="_blank" rel="noopener">从EMD、WMD到WRD：文本向量序列的相似度计算</a></li>
<li><a href="https://mp.weixin.qq.com/s/Fjr2iCbeTy9AYNAwvJr7Og" target="_blank" rel="noopener">Mila唐建博士最新《图表示学习:算法与应用》2020研究进展，附59页ppt</a></li>
<li><a href="https://mp.weixin.qq.com/s/DKDlFDRwoedSlhL8cu99DA" target="_blank" rel="noopener">从 Triplet loss 看推荐系统中文章Embedding</a></li>
<li><a href="https://mp.weixin.qq.com/s/aCICOsif_jYNUkOTiB_-FQ" target="_blank" rel="noopener">【Code】GraphSAGE 源码解析</a></li>
<li><a href="https://mp.weixin.qq.com/s/upXLNJsyq1muoNn3uJCKJg" target="_blank" rel="noopener">入门推荐系统，这25篇综述文章足够了</a></li>
<li><a href="https://mp.weixin.qq.com/s/w0KhXwE3tkZtjzQcRApRow" target="_blank" rel="noopener">KDD19开源论文 Heterogeneous Graph Neural Network</a></li>
<li><a href="https://mp.weixin.qq.com/s/Xn1xMXu0V7nkolX5x_h2lw" target="_blank" rel="noopener">SDM(Sequential Deep Matching Model)的复现之路</a></li>
<li><a href="https://mp.weixin.qq.com/s/WnF-fqQyr2VNqr75Jzoqsw" target="_blank" rel="noopener">【GNN】Diff Pool：网络图的层次化表达</a></li>
<li><a href="https://mp.weixin.qq.com/s/jIVhXMmP95G7hJP_KtksxA" target="_blank" rel="noopener">Youtube推荐RL首弹，基于Top-K的Off-Policy矫正解决推荐中的信息茧房困境</a></li>
<li><a href="https://mp.weixin.qq.com/s/yyE0ki5rymOCyGa6CNfnzQ" target="_blank" rel="noopener">《深度学习推荐系统》读书笔记之Embedding技术在推荐中的应用</a></li>
<li><a href="https://mp.weixin.qq.com/s/FZoiDfM05rWIqWJQg8F-5w" target="_blank" rel="noopener">RecSys 2019最佳论文：基于深度学习的推荐系统是否真的优于传统经典方法？</a></li>
<li><a href="https://mp.weixin.qq.com/s/daIWHWMulVlqAsm04qyi6w" target="_blank" rel="noopener">2019年，异质图神经网络领域有哪些值得读的顶会论文？</a></li>
<li><a href="https://mp.weixin.qq.com/s/63lTwUB2wsvhikWnNKa0CQ" target="_blank" rel="noopener">「工业落地」基于异质图神经网络的异常账户检测</a></li>
<li><a href="https://mp.weixin.qq.com/s/Ry8R6FmiAGSq5RBC7UqcAQ" target="_blank" rel="noopener">深入理解图注意力机制（Graph Attention Network）</a></li>
<li><a href="https://mp.weixin.qq.com/s/1xVPRIVwQQJfEen0RiNYvg" target="_blank" rel="noopener">谈谈推荐系统中的用户行为序列建模最新进展</a></li>
<li><a href="https://mp.weixin.qq.com/s/Jmutb3fWuxxmbpX4CWS3HA" target="_blank" rel="noopener">《深度学习推荐系统》读书笔记之推荐系统的进化之路</a></li>
<li><a href="https://mp.weixin.qq.com/s/zLBq2VVSAr0AUtozK7qqVA" target="_blank" rel="noopener">浅析Faiss在推荐系统中的应用及原理</a></li>
<li><a href="https://mp.weixin.qq.com/s/Fg9GKw-QLlOskEqX7pl1dA" target="_blank" rel="noopener">推荐系统之FM算法原理及实现（附代码）</a></li>
<li><a href="https://mp.weixin.qq.com/s/dSa0-BydmzmqDfrICJoUBQ" target="_blank" rel="noopener">ECAI2020推荐系统论文聚焦</a></li>
<li><a href="https://mp.weixin.qq.com/s/wLjBcK9PQHz6lUIsiZCjgg" target="_blank" rel="noopener">长文|三大主题全方位梳理图论与图学习中的基本概念：图搜索，最短路径，聚类系数，中心度等</a></li>
<li><a href="https://mp.weixin.qq.com/s/jVG_JJN5OalJcbtTGti9AA" target="_blank" rel="noopener">「工业落地」阿里异质图神经网络推荐：19KDD IntentGC</a></li>
<li><a href="https://mp.weixin.qq.com/s/3gHKD8g3tB6z88DVq5AkIw" target="_blank" rel="noopener">「工业落地」阿里异构图表示学习：19KDD GATNE</a></li>
<li><a href="https://mp.weixin.qq.com/s/y94JACgjTxObvNDhmUfulA" target="_blank" rel="noopener">图神经网络入门（三）GAT图注意力网络</a></li>
<li><a href="https://mp.weixin.qq.com/s/3x6FtOMzGitTlwwJs-dN9w" target="_blank" rel="noopener">从ACL 2020和ICLR 2020看知识图谱嵌入的近期研究进展</a></li>
<li><a href="https://mp.weixin.qq.com/s/4CPfejPrAhJuYMgSj6aGsw" target="_blank" rel="noopener">【香港理工】生成式对抗网络(GANs)最新2020综述，41页pdf阐述GAN训练、 挑战、解决方案和未来方向</a></li>
<li><a href="https://mp.weixin.qq.com/s/53zPX7N7YLlUnvD23kT6pg" target="_blank" rel="noopener">SIGIR 20 | 腾讯看点首次在推荐中应用迁移学习提出PeterRec框架，从少量行为数据中学习用户表示</a></li>
<li><a href="https://mp.weixin.qq.com/s/lf9BjP9qs_cn8f9vGUECpw" target="_blank" rel="noopener">比CNN更强有力，港中文贾佳亚团队提出两类新型自注意力网络｜CVPR2020</a></li>
<li><a href="https://www.toutiao.com/a6825468794287161860/?tt_from=weixin&utm_campaign=client_share&wxshare_count=1&timestamp=1589325241&app=news_article&utm_source=weixin&utm_medium=toutiao_ios&req_id=202005130714000101941000345D3EC4DC&group_id=6825468794287161860" target="_blank" rel="noopener">33 个神经网络「炼丹」技巧</a></li>
<li><a href="https://mp.weixin.qq.com/s/azgdVJ3-Ge1HJC1mVTRVfA" target="_blank" rel="noopener">自动化神经网络理论进展缓慢，AutoML 算法的边界到底在哪</a></li>
<li><a href="https://mp.weixin.qq.com/s/SqBU40sfo3IEj_iHnb4cXQ" target="_blank" rel="noopener">高效利用无标注数据：自监督学习简述</a></li>
<li><a href="https://mp.weixin.qq.com/s/0Jn79XT9A70sQEMOImzFLg" target="_blank" rel="noopener">【斯坦福谷歌】最新《图机器学习》综述论文，38页pdf阐述最新图表示学习进展</a></li>
<li><a href="https://mp.weixin.qq.com/s/GCoyRPKe9ND7CnG9-zWTkA" target="_blank" rel="noopener">近期必读的5篇顶会CVPR 2020【场景图+图神经网络（SG+GNN）】相关论文</a></li>
<li><a href="https://mp.weixin.qq.com/s/SOaA9XNnymLgGgJ5JNSdBg" target="_blank" rel="noopener">对比学习（Contrastive Learning）相关进展梳理</a></li>
<li><a href="https://mp.weixin.qq.com/s/0jPOSvSqsTEtwjQ_nRgQ_g" target="_blank" rel="noopener">干货！2019五大顶会必读的Graph Embedding相关的论文</a></li>
<li><a href="https://mp.weixin.qq.com/s/W2UTCtoTqRW739EFxCFf1w" target="_blank" rel="noopener">在深度学习顶会ICLR 2020上，Transformer模型有什么新进展？</a></li>
<li><a href="https://mp.weixin.qq.com/s/z0UMPHLYFOaiTCxqt65uRg" target="_blank" rel="noopener">当深度学习遇上量化交易——图与知识图谱篇</a></li>
<li><a href="https://mp.weixin.qq.com/s/wLvTo_k67VwiwrfW9dfSWg" target="_blank" rel="noopener">「工业落地」阿里在图神经网络推荐的探</a></li>
<li><a href="https://mp.weixin.qq.com/s/RhnyQUnXxMDybv8B-dSj4Q" target="_blank" rel="noopener">图系列|WWW2020 图学习/图神经网络GNN相关论文速览</a></li>
<li><a href="https://mp.weixin.qq.com/s/byVdEPcCmVPJOk-uIyGsbw" target="_blank" rel="noopener">【重磅】GCN大佬Thomas Kipf博士论文《深度学习图结构表示》178页pdf阐述图卷积神经网络等机制与应用</a></li>
</ol>
]]></content>
      <tags>
        <tag>recommendation</tag>
        <tag>GNN</tag>
      </tags>
  </entry>
  <entry>
    <title>metapath2vec: Scalable Representation Learning for Heterogeneous Networks</title>
    <url>/2019/12/12/metapath2vec-Scalable-Representation-Learning-for-Heterogeneous-Networks/</url>
    <content><![CDATA[<blockquote>
<blockquote>
<p>This article proposed two scalable representation learning models: <strong>metapath2vec</strong> and <strong>metapath2vec++</strong>. </p>
<p>In metapath2vec: </p>
<ul>
<li>First, they propose <strong>meta-path</strong> based <strong>random walks</strong> in heterogeneous networks to generate <strong>hetetogeneous neighborhoods</strong> with network semantics for various types of nodes.</li>
<li>Second, they extend the <strong>skip-gram</strong> model to facilitate the modeling of geographically and semantically close nodes.</li>
</ul>
<p>In metapath2vec++:</p>
<ul>
<li>they develop a <strong>heterogeneous negative sampling-based</strong> method that enables the accurate and efficient prediction of a node’s heterogeneous neighborhood.</li>
</ul>
</blockquote>
</blockquote>
]]></content>
      <tags>
        <tag>meta path</tag>
        <tag>heterogeneous</tag>
        <tag>skip-gram</tag>
        <tag>negative sampling-based method</tag>
      </tags>
  </entry>
  <entry>
    <title>Heterogeneous Graph Neural Network</title>
    <url>/2020/05/20/Heterogeneous%20Graph%20Neural%20Network/</url>
    <content><![CDATA[<html><head></head><body><blockquote>
<blockquote>
<p>Zhang C , Song D , Huang C , et al. Heterogeneous Graph Neural Network[C]// the 25th ACM SIGKDD International Conference. ACM, 2019.</p>
</blockquote>
</blockquote>
<h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><h5 id="idea-of-the-article-HetGNN"><a href="#idea-of-the-article-HetGNN" class="headerlink" title="idea of the article(HetGNN)"></a>idea of the article(HetGNN)</h5><blockquote>
<p>This paper models the heterogeneous graph network and gets each nodes’ vector representation.<br>purpose: learn how to represent the vectors of each node in a heterogeneous graph.(embedding)</p>
</blockquote>
<p>First, <font color="red">a reboot-based random walk strategy is used to select neighbors for each node according to the node type</font>.</p>
<p>Then, two modules are used to aggregate the characteristics of neighbor nodes:</p>
<ul>
<li>Generate feature vectors by modeling <font color="red">the features of different types of nodes.</font></li>
<li><font color="red">Aggregate different types of neighbor nodes</font>, and assign different weights to different types of nodes by fusing attention mechanism to obtain the final vector representation.</li>
</ul>
<p>Finally, establish loss function, use mini-batch gradient descent. </p>
<p><em>the learnted vector can be used to <b>link prediction, recommendation, nodes classification, cluster, etc..</b></em></p>
<hr>
<p><img alt="HGNN_challenge.jpeg" data-src="https://i.loli.net/2020/05/21/TFq79ogQsh1xCm3.jpg" class="lazyload"></p>
<hr>
<h3>HetGNN模型结构</h3>



<p>针对上文提到的异构网络面临的三个挑战，HetGNN分成了三个部分：</p>
<ul>
<li><p>邻居采样策略：Sampling Heterogeneous Neighbors（挑战一）</p>
</li>
<li><p>特征编码：Encoding Heterogeneous Contents（挑战二）</p>
</li>
<li><p>聚合邻居：Aggregating Heterogeneous Neighbors（挑战三）</p>
</li>
</ul>
<p>最后，根据目标函数进行优化，进行训练和预测</p>
<h5>Sampling Heterogeneous Neighbors(挑战一)</h5>

<p><em>(挑战一：对异构图如何采样？)</em></p>
<p>在异构图中，直接采样邻居面临的问题：</p>
<ol>
<li><p><font color="red">不能捕捉到不同类型邻居的信息。</font>比如说，在下图的作者-论文-会议的图中，作者之间并不相连，作者会议也不相连，但他们之间的关系不可被忽视。</p>
<div align="center"><img alt="HetG_example.png" style="zoom: 50%;" data-src="https://i.loli.net/2020/05/21/kLxdo3AM9jJPlVc.png" class="lazyload">
</div></li>
<li><p><font color="red">邻居数量的影响</font>。有的作者写了很多篇论文，有的作者写的少。有的商品被很多人访问，有的商品无人问津，冷启动问题不能很好的表示。</p>
</li>
<li><p><font color="red">节点的特征类型不同（如图像，文字等），不能直接聚合。</font></p>
</li>
</ol>
<p>针对上述问题，本文采用一种<font color="red">random walk with restart(RWR)</font>方法进行采样，主要有两步：</p>
<ol>
<li><p>从节点v<span style="border-bottom:2px dashed red;">随机游走采样</span>，采样固定长度，每次以概率p访问邻居节点或返回初始节点，每种类型节点采样数固定，确保每类节点都会被采样到。</p>
</li>
<li><p>对不同类型的邻居分组，不同类型的邻居，根据采样频率返回前k个</p>
</li>
</ol>
<p>上述采样方法中：</p>
<ol>
<li><p>对于每种类型的节点都采样到了</p>
</li>
<li><p>每种类型节点数量相同，并且高频邻居被选择</p>
</li>
<li><p>同种类型的邻居放在了一起，邻居信息可以聚合</p>
</li>
</ol>
<h5>Encoding Heterogeneous Contents(挑战二)</h5>

<p><em>(挑战二：对不同类型的特征怎样进行编码？)</em></p>
<p>同一个节点，也往往有多种类型的特征，如图像，文字等，<span style="border-bottom:2px dashed red;">文章提出先对这一类特征进行预训练，如类别特征直接利用one-hot，文本特征利用par2vec，图像特征利用CNN，训练得到每类特征的向量表示后，利用Bi-LSTM进行编码后聚合</span>。模型架构如图所示：</p>
<img alt="embedding.png" style="zoom:50%;" data-src="https://i.loli.net/2020/05/22/u9vmF5WgNJ7PUjy.jpg" class="lazyload">

<blockquote>
<p>类别型特征（categorical feature）主要是指年龄，职业，血型等在有限类别内取值的特征。它的原始输入通常是字符串形式，除了决策树族的算法能直接接受类别型特征作为输入，对于支持向量机，逻辑回归等模型来说，必须对其做一定的处理，转换成可靠的数值特征才能正确运行。一般的处理方式就是<a href="https://zhuanlan.zhihu.com/p/88921408" target="_blank" rel="noopener">one-hot encoding</a></p>
</blockquote>
<blockquote>
<p><a href="https://blog.csdn.net/hero_fantao/article/details/69659457?locationNum=10&fps=1" target="_blank" rel="noopener">par2vec</a>: 是根据word2vec产生的，专门针对paragraph vector。</p>
</blockquote>
<p>在数学表达式上，节点v的向量表示$f{_1}(v)$为：</p>
<img alt="f1_v_.jpeg" style="zoom:50%;" data-src="https://i.loli.net/2020/05/22/TcMxmGuFK9ZkR2r.jpg" class="lazyload">

<h5>Aggregating Heterogeneous Neighbors(挑战三)</h5>

<p><em>(挑战三：对不同类型的节点如何聚合？)</em></p>
<p>在上一部分，得到了每个节点的特征表示，在聚合上面临两个问题：对同样类型的不同节点怎样聚合？对不同类型的节点怎样聚合？分两步解决这两个问题：</p>
<p>Same type neighbors aggregation:</p>
<p>在采样中，我们对不同类型的节点进行采样，<font color="red">通过上一步得到了每个节点的特征，这里，需要对同一类型的节点特征进行聚合，此处仍然采用Bi-LSTM方法</font>：</p>
<img alt="f2_v_.png" style="zoom:50%;" data-src="https://i.loli.net/2020/05/22/PAKxtBSkiIfaoJj.jpg" class="lazyload">

<p><span style="border-bottom:2px dashed red;">输入为采样得到的相同类型邻居的特征表示，输出为这一类型邻居的向量表示</span>，有点类似与GraphSAGE的思想，<span style="border-bottom:2px dashed red;">利用Bi-LSTM对相同类型的邻居节点进行聚合</span>，模型结构如图所示：</p>
<img alt="NN-2.png" style="zoom:50%;" data-src="https://i.loli.net/2020/05/22/j768MorHRWbhtUd.jpg" class="lazyload">

<p>Type Combination:</p>
<p>上述得到了每个类型节点的向量表示，这里，希望对这些类型的节点进行聚合，考虑到不同类型节点的邻居贡献不同，因此引入注意力机制l联合学习不同类型的邻居：</p>
<img alt="FC8680C6-BD1C-4ADE-9DBF-A0B3C255DF27.png" style="zoom:50%;" data-src="https://i.loli.net/2020/05/22/TLhV1KUcCMB6pYP.jpg" class="lazyload">



<p>模型结构如图所示：</p>
<img alt="NN-3.png" style="zoom:50%;" data-src="https://i.loli.net/2020/05/22/wVPgWO1y8K3UJ2h.jpg" class="lazyload">

<hr>
<h3>Objective function and Framework</h3>

<p>根据目标函数学习模型参数：</p>
<img alt="fn.png" style="zoom:50%;" data-src="https://i.loli.net/2020/05/22/IAXLDbB9mT82onk.jpg" class="lazyload">

<p>Framework:</p>
<img alt="Framework.png" style="zoom:50%;" data-src="https://i.loli.net/2020/05/22/E3RnvjMmPidVucN.jpg" class="lazyload">

<p><strong>分成5步：</strong></p>
<ol>
<li><p>对邻居节点进行采样，按照节点类型进行分类</p>
</li>
<li><p>NN-1：对节点不同类型特征学习</p>
</li>
<li><p>NN-2：对相同类型节点各个特征的聚合</p>
</li>
<li><p>NN-3：对不同类型节点的聚合</p>
</li>
<li><p>根据Graph Context Loss损失函数进行优化</p>
</li>
</ol>
<p>最终得到每个节点的向量表示用于下游任务</p>
<hr>
<h3>Contribution:</h3>

<ul>
<li>Define heterogeneous graph: heterogeneous of graph structure and nodes information.</li>
<li>Propose HetGNN, which can capture the heterogeneous of stucture and content at the same time. It can be applied to both direct and inductive tasks.</li>
<li>Good performance in multi-data set experiments, link prediction, node classification, clustering and other tasks.</li>
</ul>
<hr>
<p>Reference :</p>
<ul>
<li><a href="https://mp.weixin.qq.com/s/w0KhXwE3tkZtjzQcRApRow" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/w0KhXwE3tkZtjzQcRApRow</a></li>
<li><a href="https://www.jianshu.com/p/fd8355e3d5d5" target="_blank" rel="noopener">https://www.jianshu.com/p/fd8355e3d5d5</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/88921408" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/88921408</a></li>
<li><a href="https://blog.csdn.net/hero_fantao/article/details/69659457?locationNum=10&fps=1" target="_blank" rel="noopener">https://blog.csdn.net/hero_fantao/article/details/69659457?locationNum=10&fps=1</a></li>
</ul>
<p>Paper:</p>
<ul>
<li><a href="https://doi.org/10.1145/3292500.3330961" target="_blank" rel="noopener">https://doi.org/10.1145/3292500.3330961</a></li>
</ul>
<p>Code:</p>
<ul>
<li><a href="https://github.com/chuxuzhang/KDD2019_HetGNN" target="_blank" rel="noopener">https://github.com/chuxuzhang/KDD2019_HetGNN</a></li>
</ul>
</body></html>]]></content>
      <tags>
        <tag>heterogeneous GNN</tag>
      </tags>
  </entry>
</search>
